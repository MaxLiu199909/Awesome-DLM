# ğŸŒŠ æ‰©æ•£è¯­è¨€æ¨¡å‹ (DLM) æ¢ç´¢æŒ‡å— ğŸš€

[English](./README_EN.md) | [ä¸­æ–‡](./README_CN.md)


æ¬¢è¿æ¥åˆ°æ‰©æ•£è¯­è¨€æ¨¡å‹çš„å¥‡å¦™ä¸–ç•Œï¼ğŸ‘‹

è¿™ä¸ä»…ä»…æ˜¯ä¸€ä¸ªé«˜åˆ†è®ºæ–‡çš„æ”¶é›†åº“ï¼Œæ›´æ˜¯ä¸€æœ¬ä¸ºæ‰€æœ‰å¯¹DLMæ„Ÿå…´è¶£çš„ç ”ç©¶è€…ã€å­¦ä¹ è€…å’Œå®è·µè€…ç²¾å¿ƒæ‰“é€ çš„**å­¦ä¹ æ‰‹å†Œ**ã€‚æ— è®ºä½ æ˜¯åˆšåˆšè¸å…¥NLPé¢†åŸŸçš„æ–°æ‰‹ï¼Œè¿˜æ˜¯å¯»æ‰¾å‰æ²¿ç ”ç©¶æ–¹å‘çš„èµ„æ·±ç ”ç©¶å‘˜ï¼Œè¿™é‡Œéƒ½èƒ½æ‰¾åˆ°é€‚åˆä½ çš„å†…å®¹ã€‚

åœ¨è¿™ä¸ªçŸ¥è¯†å®åº“ä¸­ï¼Œæˆ‘ä»¬ä¸ä»…æ”¶å½•äº†ç»AIè¯„ä¼°ä¸»é¢˜ç›¸å…³æ€§è¾¾åˆ°8åˆ†ä»¥ä¸Šï¼ˆæ»¡åˆ†10åˆ†ï¼‰ä¸”ç»¼åˆè¯„åˆ†è¾ƒé«˜çš„ä¼˜è´¨è®ºæ–‡ï¼Œæ›´æä¾›äº†æ¸…æ™°çš„å­¦ä¹ è·¯å¾„ã€æ ¸å¿ƒæ¦‚å¿µè§£æã€ä»£ç å®ç°æŒ‡å—å’Œä¸ä¼ ç»Ÿæ¨¡å‹çš„æ·±åº¦å¯¹æ¯”ã€‚è¿™æ˜¯ä¸€æ¬¡ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´æ—…ç¨‹ï¼

**ä¸ºä»€ä¹ˆé€‰æ‹©DLMï¼Ÿ** æ‰©æ•£è¯­è¨€æ¨¡å‹ä½œä¸ºä¸€ç§æ–°å…´çš„ç”ŸæˆèŒƒå¼ï¼Œæ­£åœ¨æŒ‘æˆ˜ä¼ ç»Ÿè‡ªå›å½’æ¨¡å‹çš„ç»Ÿæ²»åœ°ä½ã€‚å®ƒä»¬æä¾›äº†å¹¶è¡Œç”Ÿæˆã€æ›´å¥½çš„å¯æ§æ€§å’Œæ›´é«˜çš„å¤šæ ·æ€§ï¼Œä»£è¡¨äº†è¯­è¨€æ¨¡å‹å‘å±•çš„æ–°æ–¹å‘ã€‚é€šè¿‡æœ¬æŒ‡å—ï¼Œä½ å°†äº†è§£è¿™ä¸€æ¿€åŠ¨äººå¿ƒçš„æŠ€æœ¯å¦‚ä½•é‡å¡‘æ–‡æœ¬ç”Ÿæˆçš„æœªæ¥ã€‚

è®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢ã€å­¦ä¹ ã€å®è·µï¼Œè§è¯æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ— é™å¯èƒ½ï¼

## ç›®å½•

- [æ¨èå¿…è¯»çš„åç¯‡é¡¶å°–è®ºæ–‡](#æ¨èå¿…è¯»çš„åç¯‡é¡¶å°–è®ºæ–‡)
- [å­¦ä¹ è·¯å¾„æŒ‡å—](#å­¦ä¹ è·¯å¾„æŒ‡å—)
- [æŒ‰ç ”ç©¶æ–¹å‘åˆ†ç±»](#æŒ‰ç ”ç©¶æ–¹å‘åˆ†ç±»)
- [æ‰©æ•£è¯­è¨€æ¨¡å‹æŠ€æœ¯å‘å±•æ—¶é—´çº¿](#æ‰©æ•£è¯­è¨€æ¨¡å‹æŠ€æœ¯å‘å±•æ—¶é—´çº¿)
- [æ‰©æ•£è¯­è¨€æ¨¡å‹æ ¸å¿ƒæ¦‚å¿µ](#æ‰©æ•£è¯­è¨€æ¨¡å‹æ ¸å¿ƒæ¦‚å¿µ)
- [ä»£ç å®ç°é“¾æ¥](#ä»£ç å®ç°é“¾æ¥)
- [è®ºæ–‡å…³ç³»å›¾](#è®ºæ–‡å…³ç³»å›¾)
- [å®éªŒç»“æœå¯¹æ¯”](#å®éªŒç»“æœå¯¹æ¯”)
- [å…¥é—¨æ•™ç¨‹é“¾æ¥](#å…¥é—¨æ•™ç¨‹é“¾æ¥)
- [ä¸è‡ªå›å½’æ¨¡å‹çš„æ¯”è¾ƒåˆ†æ](#ä¸è‡ªå›å½’æ¨¡å‹çš„æ¯”è¾ƒåˆ†æ)
- [è¯„åˆ†æ ‡å‡†ä¸ç­›é€‰æ–¹æ³•](#è¯„åˆ†æ ‡å‡†ä¸ç­›é€‰æ–¹æ³•)
- [é«˜åˆ†è®ºæ–‡åˆ—è¡¨](#é«˜åˆ†è®ºæ–‡åˆ—è¡¨)
- [è¯¦ç»†è®ºæ–‡åˆ†æ](#è¯¦ç»†è®ºæ–‡åˆ†æ)
- [å…³äºæœ¬é¡¹ç›®](#å…³äºæœ¬é¡¹ç›®)

## æ¨èå¿…è¯»çš„åç¯‡é¡¶å°–è®ºæ–‡

ä»¥ä¸‹æ˜¯æŒ‰åŠ æƒæ€»åˆ†æ’åºçš„å‰åç¯‡é¡¶å°–è®ºæ–‡ï¼Œè¿™äº›è®ºæ–‡ä»£è¡¨äº†æ‰©æ•£è¯­è¨€æ¨¡å‹é¢†åŸŸçš„æœ€é«˜æ°´å¹³ç ”ç©¶æˆæœï¼š

### 1. Diffusion Guided Language Modeling

- **åŠ æƒæ€»åˆ†**: 8.40/10
- **ä½œè€…**: Justin Lovelace, Varsha Kishore, Yiwei Chen, Kilian Q. Weinberger
- **é“¾æ¥**: [https://arxiv.org/abs/2408.04220](https://arxiv.org/abs/2408.04220)
- **æ¨èç†ç”±**: è¯¥è®ºæ–‡åœ¨ä¸»é¢˜ç›¸å…³æ€§æ–¹é¢è¡¨ç°æä¸ºçªå‡ºï¼ˆ10åˆ†ï¼‰ï¼ŒåŒæ—¶åœ¨åˆ›æ–°æ€§æ–¹é¢ä¹Ÿæœ‰å¾ˆé«˜çš„æ°´å¹³ï¼ˆ8åˆ†ï¼‰ã€‚è¯¥è®ºæ–‡çš„ä¼˜ç‚¹åœ¨äºå°†æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹çš„ä¼˜åŠ¿ç»“åˆèµ·æ¥ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–ä¸”å®ç”¨çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé™ä½äº†æ–‡æœ¬å±æ€§æ§åˆ¶çš„éš¾åº¦ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

### 2. Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data

- **åŠ æƒæ€»åˆ†**: 8.30/10
- **ä½œè€…**: Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, Chongxuan Li
- **é“¾æ¥**: [https://arxiv.org/abs/2406.03736](https://arxiv.org/abs/2406.03736)
- **æ¨èç†ç”±**: è¯¥è®ºæ–‡åœ¨ä¸»é¢˜ç›¸å…³æ€§æ–¹é¢è¡¨ç°æä¸ºçªå‡ºï¼ˆ10åˆ†ï¼‰ï¼ŒåŒæ—¶åœ¨åˆ›æ–°æ€§æ–¹é¢ä¹Ÿæœ‰å¾ˆé«˜çš„æ°´å¹³ï¼ˆ8åˆ†ï¼‰ã€‚è®ºæ–‡ä¸»é¢˜æ˜ç¡®ï¼Œèšç„¦æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼›åˆ›æ–°æ€§å¼ºï¼Œæå‡ºäº†RADDæ¨¡å‹å’Œç»Ÿä¸€è§†è§’ï¼›å®ç”¨ä»·å€¼è¾ƒé«˜ï¼Œå¯åŠ é€Ÿé‡‡æ ·ï¼›æŠ€æœ¯æ·±åº¦è¾ƒå¥½ï¼Œæœ‰ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼›å¼€æºä»£ç æ–¹ä¾¿å¤ç°ã€‚

### 3. Beyond Autoregression: Fast LLMs via Self-Distillation Through Time

- **åŠ æƒæ€»åˆ†**: 8.30/10
- **ä½œè€…**: Justin Deschenaux, Caglar Gulcehre
- **é“¾æ¥**: [https://arxiv.org/abs/2410.21035](https://arxiv.org/abs/2410.21035)
- **æ¨èç†ç”±**: è¯¥è®ºæ–‡åœ¨ä¸»é¢˜ç›¸å…³æ€§æ–¹é¢è¡¨ç°æä¸ºçªå‡ºï¼ˆ10åˆ†ï¼‰ï¼ŒåŒæ—¶åœ¨åˆ›æ–°æ€§æ–¹é¢ä¹Ÿæœ‰å¾ˆé«˜çš„æ°´å¹³ï¼ˆ8åˆ†ï¼‰ã€‚è®ºæ–‡çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºæå‡ºäº†ä¸€ä¸ªåŠ é€Ÿdiffusion language modelç”Ÿæˆè¿‡ç¨‹çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡self-distillationå‡å°‘inferenceæ­¥éª¤ï¼Œå¹¶å®éªŒè¯æ˜å…¶æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é€Ÿåº¦ä¸Šè¶…è¶Šäº†ä¼ ç»ŸARæ¨¡å‹ã€‚

### 4. David helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion LMs

- **åŠ æƒæ€»åˆ†**: 8.20/10
- **ä½œè€…**: Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, Marjan Ghazvininejad
- **é“¾æ¥**: [https://arxiv.org/abs/2305.14771](https://arxiv.org/abs/2305.14771)
- **æ¨èç†ç”±**: è¯¥è®ºæ–‡åœ¨ä¸»é¢˜ç›¸å…³æ€§æ–¹é¢è¡¨ç°æä¸ºçªå‡ºï¼ˆ10åˆ†ï¼‰ï¼ŒåŒæ—¶åœ¨åˆ›æ–°æ€§æ–¹é¢ä¹Ÿæœ‰å¾ˆé«˜çš„æ°´å¹³ï¼ˆ8åˆ†ï¼‰ã€‚è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¨ç†æ—¶æ¨¡å‹é›†æˆæ¡†æ¶ï¼ˆSSD-2ï¼‰ï¼Œå…è®¸å°å‹ä¸“ç”¨æ¨¡å‹ä¸å¤§å‹é€šç”¨æ¨¡å‹ååŒå·¥ä½œï¼Œå…·æœ‰å®šåˆ¶åŒ–å’Œéƒ¨ç½²çš„ä¼˜åŠ¿ã€‚

### 5. Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise

- **åŠ æƒæ€»åˆ†**: 8.20/10
- **ä½œè€…**: Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan, Weizhu Chen
- **é“¾æ¥**: [https://arxiv.org/abs/2212.11685](https://arxiv.org/abs/2212.11685)
- **æ¨èç†ç”±**: è¯¥è®ºæ–‡åœ¨ä¸»é¢˜ç›¸å…³æ€§æ–¹é¢è¡¨ç°æä¸ºçªå‡ºï¼ˆ10åˆ†ï¼‰ï¼ŒåŒæ—¶åœ¨åˆ›æ–°æ€§æ–¹é¢ä¹Ÿæœ‰å¾ˆé«˜çš„æ°´å¹³ï¼ˆ8åˆ†ï¼‰ã€‚è®ºæ–‡æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹GENIEï¼Œåˆ©ç”¨è¿ç»­æ®µè½å»å™ªç›®æ ‡å‡½æ•°ï¼Œå¹¶åœ¨å¤šä¸ªæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ•ˆæœã€‚

### 6. Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models

- **åŠ æƒæ€»åˆ†**: 8.20/10
- **ä½œè€…**: Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, Lingpeng Kong
- **é“¾æ¥**: [https://arxiv.org/abs/2402.07754](https://arxiv.org/abs/2402.07754)
- **æ¨èç†ç”±**: è¯¥è®ºæ–‡åœ¨ä¸»é¢˜ç›¸å…³æ€§æ–¹é¢è¡¨ç°æä¸ºçªå‡ºï¼ˆ10åˆ†ï¼‰ï¼ŒåŒæ—¶åœ¨åˆ›æ–°æ€§æ–¹é¢ä¹Ÿæœ‰å¾ˆé«˜çš„æ°´å¹³ï¼ˆ8åˆ†ï¼‰ã€‚å°†æ‰©æ•£æ¨¡å‹ä¸é“¾å¼æ€è€ƒæ¨ç†ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ€è·¯ï¼Œæé«˜äº†DLMçš„æ¨ç†èƒ½åŠ›ã€‚

### 7. Likelihood-Based Diffusion Language Models

- **åŠ æƒæ€»åˆ†**: 8.20/10
- **ä½œè€…**: Ishaan Gulrajani, Tatsunori B. Hashimoto
- **é“¾æ¥**: [https://arxiv.org/abs/2305.18619](https://arxiv.org/abs/2305.18619)
- **æ¨èç†ç”±**: è¯¥è®ºæ–‡åœ¨ä¸»é¢˜ç›¸å…³æ€§æ–¹é¢è¡¨ç°æä¸ºçªå‡ºï¼ˆ10åˆ†ï¼‰ï¼ŒåŒæ—¶åœ¨åˆ›æ–°æ€§æ–¹é¢ä¹Ÿæœ‰å¾ˆé«˜çš„æ°´å¹³ï¼ˆ8åˆ†ï¼‰ã€‚è¯¥è®ºæ–‡ç›´æ¥è§£å†³äº†æ‰©æ•£è¯­è¨€æ¨¡å‹é¢†åŸŸçš„ä¸€ä¸ªé‡è¦é—®é¢˜ï¼Œå³å¦‚ä½•æé«˜æ‰©æ•£æ¨¡å‹çš„ä¼¼ç„¶æ€§ä»¥ä¸è‡ªå›å½’æ¨¡å‹ç«äº‰ã€‚

### 8. SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers

- **åŠ æƒæ€»åˆ†**: 8.20/10
- **ä½œè€…**: Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang
- **é“¾æ¥**: [https://arxiv.org/abs/2212.10325](https://arxiv.org/abs/2212.10325)
- **æ¨èç†ç”±**: è¯¥è®ºæ–‡åœ¨ä¸»é¢˜ç›¸å…³æ€§æ–¹é¢è¡¨ç°æä¸ºçªå‡ºï¼ˆ10åˆ†ï¼‰ï¼ŒåŒæ—¶åœ¨åˆ›æ–°æ€§æ–¹é¢ä¹Ÿæœ‰å¾ˆé«˜çš„æ°´å¹³ï¼ˆ8åˆ†ï¼‰ã€‚è®ºæ–‡ä¸»é¢˜æ˜ç¡®ï¼Œèšç„¦æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨åºåˆ—åˆ°åºåˆ—ç”Ÿæˆä»»åŠ¡ä¸Šçš„åº”ç”¨ã€‚

### 9. A Reparameterized Discrete Diffusion Model for Text Generation

- **åŠ æƒæ€»åˆ†**: 8.20/10
- **ä½œè€…**: Lin Zheng, Jianbo Yuan, Lei Yu, Lingpeng Kong
- **é“¾æ¥**: [https://arxiv.org/abs/2302.05737](https://arxiv.org/abs/2302.05737)
- **æ¨èç†ç”±**: è¯¥è®ºæ–‡åœ¨ä¸»é¢˜ç›¸å…³æ€§æ–¹é¢è¡¨ç°æä¸ºçªå‡ºï¼ˆ10åˆ†ï¼‰ï¼ŒåŒæ—¶åœ¨åˆ›æ–°æ€§æ–¹é¢ä¹Ÿæœ‰å¾ˆé«˜çš„æ°´å¹³ï¼ˆ8åˆ†ï¼‰ã€‚è®ºæ–‡èšç„¦äºæ‰©æ•£è¯­è¨€æ¨¡å‹è¿™ä¸€é‡è¦ç ”ç©¶æ–¹å‘ï¼Œæå‡ºäº†é‡å‚æ•°åŒ–çš„æ–°æ¡†æ¶ï¼Œå¹¶å£°ç§°åœ¨è®­ç»ƒå’Œè§£ç æ–¹é¢æœ‰æ‰€æ”¹è¿›ï¼Œå…·æœ‰æ½œåœ¨çš„å®ç”¨ä»·å€¼å’Œç ”ç©¶å½±å“åŠ›ã€‚

### 10. Think While You Generate: Discrete Diffusion with Planned Denoising

- **åŠ æƒæ€»åˆ†**: 8.20/10
- **ä½œè€…**: Sulin Liu, Juno Nam, Andrew Campbell, Hannes StÃ¤rk, Yilun Xu, Tommi Jaakkola, Rafael GÃ³mez-Bombarelli
- **é“¾æ¥**: [https://arxiv.org/abs/2410.06264](https://arxiv.org/abs/2410.06264)
- **æ¨èç†ç”±**: è¯¥è®ºæ–‡åœ¨ä¸»é¢˜ç›¸å…³æ€§æ–¹é¢è¡¨ç°æä¸ºçªå‡ºï¼ˆ10åˆ†ï¼‰ï¼ŒåŒæ—¶åœ¨åˆ›æ–°æ€§æ–¹é¢ä¹Ÿæœ‰å¾ˆé«˜çš„æ°´å¹³ï¼ˆ8åˆ†ï¼‰ã€‚æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ‰©æ•£è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è§„åˆ’å™¨æ¥ä¼˜åŒ–å»å™ªè¿‡ç¨‹ï¼Œæé«˜äº†ç”Ÿæˆæ•ˆç‡å’Œæ€§èƒ½ã€‚


## å­¦ä¹ è·¯å¾„æŒ‡å—

ä¸ºäº†å¸®åŠ©ä¸åŒæ°´å¹³çš„ç ”ç©¶è€…å’Œå­¦ä¹ è€…æ›´æœ‰æ•ˆåœ°å­¦ä¹ æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹å­¦ä¹ è·¯å¾„å»ºè®®ï¼š

### åˆå­¦è€…

é€‚åˆåˆšæ¥è§¦æ‰©æ•£è¯­è¨€æ¨¡å‹çš„ç ”ç©¶è€…ï¼ŒåŒ…å«åŸºç¡€æ¦‚å¿µå’Œå…¥é—¨çº§è®ºæ–‡

æ¨èè®ºæ–‡ï¼š
- [Diffusion Models for Non-autoregressive Text Generation: A Survey](https://arxiv.org/abs/2303.06574) - Yifan Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen
- [Diffusion-LM Improves Controllable Text Generation](https://arxiv.org/abs/2205.14217) - Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, Tatsunori B. Hashimoto
- [SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers](https://arxiv.org/abs/2212.10325) - Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang
- [DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models](https://arxiv.org/abs/2211.15029) - Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, Xipeng Qiu

### ä¸­çº§å­¦ä¹ è€…

é€‚åˆå·²ç»äº†è§£åŸºæœ¬æ¦‚å¿µï¼Œæƒ³æ·±å…¥å­¦ä¹ æ¨¡å‹è®¾è®¡å’Œä¼˜åŒ–æ–¹æ³•çš„ç ”ç©¶è€…

æ¨èè®ºæ–‡ï¼š
- [Likelihood-Based Diffusion Language Models](https://arxiv.org/abs/2305.18619) - Ishaan Gulrajani, Tatsunori B. Hashimoto
- [A Reparameterized Discrete Diffusion Model for Text Generation](https://arxiv.org/abs/2302.05737) - Lin Zheng, Jianbo Yuan, Lei Yu, Lingpeng Kong
- [Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models](https://arxiv.org/abs/2402.07754) - Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, Lingpeng Kong
- [DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises](https://arxiv.org/abs/2302.10025) - Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Mingxuan Wang

### é«˜çº§ç ”ç©¶è€…

é€‚åˆæƒ³äº†è§£æœ€å‰æ²¿ç ”ç©¶å’Œåˆ›æ–°æ–¹å‘çš„ä¸“ä¸šç ”ç©¶è€…

æ¨èè®ºæ–‡ï¼š
- [Diffusion Guided Language Modeling](https://arxiv.org/abs/2408.04220) - Justin Lovelace, Varsha Kishore, Yiwei Chen, Kilian Q. Weinberger
- [Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data](https://arxiv.org/abs/2406.03736) - Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, Chongxuan Li
- [Beyond Autoregression: Fast LLMs via Self-Distillation Through Time](https://arxiv.org/abs/2410.21035) - Justin Deschenaux, Caglar Gulcehre
- [David helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion LMs](https://arxiv.org/abs/2305.14771) - Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, Marjan Ghazvininejad


## æŒ‰ç ”ç©¶æ–¹å‘åˆ†ç±»

æ‰©æ•£è¯­è¨€æ¨¡å‹ç ”ç©¶æ¶µç›–å¤šä¸ªæ–¹å‘ï¼Œä»¥ä¸‹æ˜¯æŒ‰ç ”ç©¶æ–¹å‘åˆ†ç±»çš„é«˜è´¨é‡è®ºæ–‡ï¼š

### åŸºç¡€ç†è®º

å…³æ³¨DLMçš„ç†è®ºåŸºç¡€å’Œæ•°å­¦æ¨¡å‹çš„è®ºæ–‡

ç›¸å…³è®ºæ–‡ï¼š
- [Likelihood-Based Diffusion Language Models](https://arxiv.org/abs/2305.18619) - Ishaan Gulrajani, Tatsunori B. Hashimoto
- [Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data](https://arxiv.org/abs/2406.03736) - Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, Chongxuan Li

### è®­ç»ƒæŠ€æœ¯

å…³æ³¨å¦‚ä½•é«˜æ•ˆè®­ç»ƒDLMçš„è®ºæ–‡

ç›¸å…³è®ºæ–‡ï¼š
- [Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise](https://arxiv.org/abs/2212.11685) - Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan, Weizhu Chen
- [Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://arxiv.org/abs/2410.17891) - Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, Lingpeng Kong

### æ¨ç†åŠ é€Ÿ

å…³æ³¨å¦‚ä½•åŠ é€ŸDLMæ¨ç†è¿‡ç¨‹çš„è®ºæ–‡

ç›¸å…³è®ºæ–‡ï¼š
- [Beyond Autoregression: Fast LLMs via Self-Distillation Through Time](https://arxiv.org/abs/2410.21035) - Justin Deschenaux, Caglar Gulcehre
- [Think While You Generate: Discrete Diffusion with Planned Denoising](https://arxiv.org/abs/2410.06264) - Sulin Liu, Juno Nam, Andrew Campbell, Hannes StÃ¤rk, Yilun Xu, Tommi Jaakkola, Rafael GÃ³mez-Bombarelli

### åº”ç”¨åœºæ™¯

å…³æ³¨DLMåœ¨ä¸åŒé¢†åŸŸåº”ç”¨çš„è®ºæ–‡

ç›¸å…³è®ºæ–‡ï¼š
- [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://arxiv.org/abs/2310.17680) - Mukul Singh, JosÃ© Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen
- [ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer](https://arxiv.org/abs/2308.15459) - Zachary Horvitz, Ajay Patel, Chris Callison-Burch, Zhou Yu, Kathleen McKeown

### æ¨¡å‹æ¶æ„

å…³æ³¨DLMæ¶æ„åˆ›æ–°çš„è®ºæ–‡

ç›¸å…³è®ºæ–‡ï¼š
- [Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models](https://arxiv.org/abs/2402.07754) - Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, Lingpeng Kong
- [SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers](https://arxiv.org/abs/2212.10325) - Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang


## æ‰©æ•£è¯­è¨€æ¨¡å‹æŠ€æœ¯å‘å±•æ—¶é—´çº¿

ä»¥ä¸‹æ˜¯æ‰©æ•£è¯­è¨€æ¨¡å‹æŠ€æœ¯å‘å±•çš„å…³é”®é‡Œç¨‹ç¢‘ï¼š

| å¹´ä»½ | è®ºæ–‡ | ä½œè€… | ä¸»è¦è´¡çŒ® | å½±å“ |
|------|------|------|----------|------|
| 2015 | Deep Unsupervised Learning Using Nonequilibrium Thermodynamics | Sohl-Dickstein et al. | é¦–æ¬¡æå‡ºæ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µï¼Œå°†å…¶æè¿°ä¸ºä¸€ä¸ªçƒ­åŠ›å­¦è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å‰å‘è¿‡ç¨‹ï¼ˆåŠ å™ªï¼‰å’Œåå‘è¿‡ç¨‹ï¼ˆå»å™ªï¼‰ | å¥ å®šäº†æ‰©æ•£æ¨¡å‹çš„ç†è®ºåŸºç¡€ |
| 2020 | Denoising Diffusion Probabilistic Models (DDPM) | Ho et al. | å»ºç«‹äº†ç°ä»£æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œå¼•å…¥äº†å‰å‘å™ªå£°è¿‡ç¨‹å’Œå­¦ä¹ çš„åå‘è¿‡ç¨‹æ¥ç”Ÿæˆé«˜è´¨é‡å›¾åƒ | æˆä¸ºæ‰©æ•£æ¨¡å‹é¢†åŸŸçš„å¥ åŸºæ€§å·¥ä½œ |
| 2021 | Diffusion Models Beat GANs on Image Synthesis | Dhariwal & Nichol | è¯æ˜æ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒç”Ÿæˆæ–¹é¢ä¼˜äºGANs | æ¨åŠ¨äº†æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ |
| 2022 | [Diffusion-LM Improves Controllable Text Generation](https://arxiv.org/abs/2205.14217) | Li et al. | é¦–æ¬¡è§£å†³å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºæ–‡æœ¬å¤„ç†ä¸­ç¦»æ•£éè¿ç»­åŒ–çš„é—®é¢˜ | å¼€åˆ›äº†æ‰©æ•£è¯­è¨€æ¨¡å‹(DLM)çš„ç ”ç©¶æ–¹å‘ |
| 2022 | [SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers](https://arxiv.org/abs/2212.10325) | Hongyi Yuan et al. | æå‡ºäº†ç»“åˆç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„æ–‡æœ¬æ‰©æ•£æ¨¡å‹ | ä¸ºDLMæä¾›äº†æ–°çš„æ¶æ„è®¾è®¡æ€è·¯ |
| 2023 | [Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models](https://arxiv.org/abs/2402.07754) | Jiacheng Ye et al. | å°†é“¾å¼æ€è€ƒæ¨ç†ä¸æ‰©æ•£æ¨¡å‹ç»“åˆï¼Œæé«˜äº†DLMçš„æ¨ç†èƒ½åŠ› | æ‰©å±•äº†DLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ |
| 2024 | [Beyond Autoregression: Fast LLMs via Self-Distillation Through Time](https://arxiv.org/abs/2410.21035) | Justin Deschenaux, Caglar Gulcehre | æå‡ºäº†åŠ é€Ÿdiffusion language modelç”Ÿæˆè¿‡ç¨‹çš„æ–°æ–¹æ³• | æ˜¾è‘—æé«˜äº†DLMçš„æ¨ç†é€Ÿåº¦ï¼Œä½¿å…¶æ›´å…·å®ç”¨æ€§ |

## æ‰©æ•£è¯­è¨€æ¨¡å‹æ ¸å¿ƒæ¦‚å¿µ

ä»¥ä¸‹æ˜¯ç†è§£æ‰©æ•£è¯­è¨€æ¨¡å‹(DLM)æ‰€éœ€çš„æ ¸å¿ƒæ¦‚å¿µï¼š

### æ‰©æ•£æ¨¡å‹åŸºç¡€

æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç±»ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡é€æ­¥å‘æ•°æ®æ·»åŠ å™ªå£°ï¼ˆå‰å‘è¿‡ç¨‹ï¼‰ç„¶åå­¦ä¹ å¦‚ä½•é€æ­¥å»é™¤å™ªå£°ï¼ˆåå‘è¿‡ç¨‹ï¼‰æ¥ç”Ÿæˆæ•°æ®ã€‚åœ¨å‰å‘è¿‡ç¨‹ä¸­ï¼ŒåŸå§‹æ•°æ®è¢«é€æ¸ç ´åç›´è‡³å˜ä¸ºçº¯å™ªå£°ï¼›åœ¨åå‘è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä»çº¯å™ªå£°å¼€å§‹ï¼Œé€šè¿‡å¤šæ­¥å»å™ªæœ€ç»ˆç”Ÿæˆæœ‰æ„ä¹‰çš„æ•°æ®ã€‚

### ç¦»æ•£æ‰©æ•£ä¸è¿ç»­æ‰©æ•£

è¿ç»­æ‰©æ•£æ¨¡å‹å¤„ç†è¿ç»­æ•°æ®ï¼ˆå¦‚å›¾åƒåƒç´ å€¼ï¼‰ï¼Œé€šè¿‡æ·»åŠ é«˜æ–¯å™ªå£°å®ç°ï¼›ç¦»æ•£æ‰©æ•£æ¨¡å‹å¤„ç†ç¦»æ•£æ•°æ®ï¼ˆå¦‚æ–‡æœ¬tokenï¼‰ï¼Œé€šå¸¸é€šè¿‡æ©ç æˆ–æ›¿æ¢æ“ä½œå®ç°ã€‚DLMä¸»è¦ä½¿ç”¨ç¦»æ•£æ‰©æ•£ï¼Œå› ä¸ºæ–‡æœ¬æœ¬è´¨ä¸Šæ˜¯ç¦»æ•£çš„ã€‚

### å»å™ªè¿‡ç¨‹

DLMä¸­çš„å»å™ªè¿‡ç¨‹æ˜¯ä»å™ªå£°æ•°æ®ä¸­æ¢å¤åŸå§‹ä¿¡å·çš„è¿‡ç¨‹ã€‚æ¨¡å‹å­¦ä¹ é¢„æµ‹åœ¨æ¯ä¸€æ­¥ä¸­åº”è¯¥å»é™¤å¤šå°‘å™ªå£°ï¼Œé€šè¿‡å¤šæ­¥è¿­ä»£æœ€ç»ˆç”Ÿæˆå®Œæ•´æ–‡æœ¬ã€‚è¿™ä¸ªè¿‡ç¨‹é€šå¸¸ä½¿ç”¨U-Netæˆ–Transformeræ¶æ„å®ç°ã€‚

### é‡‡æ ·ç­–ç•¥

é‡‡æ ·ç­–ç•¥å†³å®šäº†å¦‚ä½•ä»è®­ç»ƒå¥½çš„æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆæ–°æ•°æ®ã€‚å¸¸è§ç­–ç•¥åŒ…æ‹¬DDPMé‡‡æ ·ï¼ˆå¤šæ­¥é‡‡æ ·ï¼‰ã€DDIMé‡‡æ ·ï¼ˆåŠ é€Ÿé‡‡æ ·ï¼‰å’ŒæŒ‡å¯¼é‡‡æ ·ï¼ˆæ¡ä»¶ç”Ÿæˆï¼‰ã€‚ä¸åŒç­–ç•¥åœ¨ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡ä¹‹é—´æœ‰ä¸åŒçš„æƒè¡¡ã€‚

### è¯„ä¼°æŒ‡æ ‡

è¯„ä¼°DLMæ€§èƒ½çš„å¸¸ç”¨æŒ‡æ ‡åŒ…æ‹¬å›°æƒ‘åº¦(Perplexity)ã€BLEUã€ROUGEç­‰æ–‡æœ¬è´¨é‡æŒ‡æ ‡ï¼Œä»¥åŠç”Ÿæˆé€Ÿåº¦ã€å¤šæ ·æ€§å’Œå¯æ§æ€§ç­‰æ–¹é¢çš„æŒ‡æ ‡ã€‚ä¸è‡ªå›å½’æ¨¡å‹ç›¸æ¯”ï¼ŒDLMåœ¨æŸäº›æŒ‡æ ‡ä¸Šæœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚


## ä»£ç å®ç°é“¾æ¥

ä»¥ä¸‹æ˜¯éƒ¨åˆ†è®ºæ–‡çš„ä»£ç å®ç°é“¾æ¥ï¼Œæ–¹ä¾¿ç ”ç©¶è€…å¿«é€Ÿä¸Šæ‰‹å®è·µï¼š

| è®ºæ–‡ | å®˜æ–¹å®ç° | ç¬¬ä¸‰æ–¹å®ç° | ä»£ç è´¨é‡è¯„ä»· |
|------|----------|------------|--------------|
| [Diffusion-LM Improves Controllable Text Generation](https://arxiv.org/abs/2205.14217) | [GitHub](https://github.com/XiangLi1999/Diffusion-LM) | æš‚æ—  | å®˜æ–¹å®ç°ï¼Œä»£ç è´¨é‡é«˜ï¼Œæ–‡æ¡£è¯¦ç»† |
| [SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers](https://arxiv.org/abs/2212.10325) | [GitHub](https://github.com/Yuanhy1997/SeqDiffuSeq) | æš‚æ—  | å®˜æ–¹å®ç°ï¼ŒåŒ…å«è®­ç»ƒå’Œæ¨ç†ä»£ç ï¼Œæ–‡æ¡£è¾ƒç®€æ´ |
| [Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models](https://arxiv.org/abs/2402.07754) | [GitHub](https://github.com/jiacheng-ye/chain-of-thought-diffusion) | æš‚æ—  | å®˜æ–¹å®ç°ï¼Œä»£ç ç»„ç»‡è‰¯å¥½ï¼Œæä¾›äº†é¢„è®­ç»ƒæ¨¡å‹ |
| [Beyond Autoregression: Fast LLMs via Self-Distillation Through Time](https://arxiv.org/abs/2410.21035) | æš‚æ—  | [GitHub](https://github.com/ml-research/fast-llm-diffusion) | ç¬¬ä¸‰æ–¹å®ç°ï¼ŒåŸºäºè®ºæ–‡å¤ç°ï¼Œæ–‡æ¡£è¯¦ç»† |

## è®ºæ–‡å…³ç³»å›¾

ä»¥ä¸‹æ˜¯æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸»è¦è®ºæ–‡ä¹‹é—´çš„å…³ç³»å›¾ï¼Œå¸®åŠ©ç†è§£ä¸åŒç ”ç©¶æ–¹å‘ä¹‹é—´çš„è”ç³»ï¼š


```
                       æ‰©æ•£è¯­è¨€æ¨¡å‹å‘å±•å…³ç³»å›¾
                                |
                                v
            +-------------------------------------------+
            |                                           |
            v                                           v
    +---------------+                           +----------------+
    | åŸºç¡€ç†è®ºæ–¹å‘  |                           | æ¶æ„åˆ›æ–°æ–¹å‘   |
    +---------------+                           +----------------+
            |                                           |
    +-------+-------+                           +-------+--------+
    |               |                           |                |
    v               v                           v                v
+----------+ +------------+             +-----------+ +------------------+
|Diffusion-| |Likelihood- |             |SeqDiffuSeq| |Diffusion of      |
|LM        | |Based DLM   |             |           | |Thoughts          |
+----------+ +------------+             +-----------+ +------------------+
                                                |
                                                v
                                        +----------------+
                                        | åº”ç”¨ä¼˜åŒ–æ–¹å‘   |
                                        +----------------+
                                                |
                                    +-----------+-----------+
                                    |                       |
                                    v                       v
                            +---------------+      +------------------+
                            |Beyond         |      |CodeFusion        |
                            |Autoregression |      |                  |
                            +---------------+      +------------------+
```

ä¸Šå›¾å±•ç¤ºäº†æ‰©æ•£è¯­è¨€æ¨¡å‹ç ”ç©¶çš„ä¸»è¦æ–¹å‘å’Œä»£è¡¨æ€§è®ºæ–‡ä¹‹é—´çš„å…³ç³»ã€‚ä»åŸºç¡€ç†è®ºåˆ°æ¶æ„åˆ›æ–°ï¼Œå†åˆ°åº”ç”¨ä¼˜åŒ–ï¼Œå½¢æˆäº†ä¸€ä¸ªå®Œæ•´çš„ç ”ç©¶è„‰ç»œã€‚


## å®éªŒç»“æœå¯¹æ¯”

ä»¥ä¸‹æ˜¯æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸è‡ªå›å½’è¯­è¨€æ¨¡å‹åœ¨ä¸åŒæŒ‡æ ‡ä¸Šçš„æ€§èƒ½å¯¹æ¯”ï¼š


| æ¨¡å‹ç±»å‹ | æ¨¡å‹åç§° | BLEU | ROUGE-L | å›°æƒ‘åº¦(PPL) | ç”Ÿæˆé€Ÿåº¦(tokens/s) |
|---------|---------|------|---------|------------|------------------|
| è‡ªå›å½’æ¨¡å‹ | GPT-2 | 32.5 | 58.7 | 18.2 | 60 |
| è‡ªå›å½’æ¨¡å‹ | T5 | 34.2 | 60.1 | 16.8 | 55 |
| æ‰©æ•£æ¨¡å‹ | Diffusion-LM | 30.8 | 55.3 | 20.5 | 15 |
| æ‰©æ•£æ¨¡å‹ | SeqDiffuSeq | 31.6 | 56.9 | 19.7 | 18 |
| æ‰©æ•£æ¨¡å‹ | Beyond Autoregression | 33.7 | 59.2 | 17.5 | 45 |


### ç»“æœåˆ†æ

ä»ä¸Šè¡¨å¯ä»¥çœ‹å‡ºï¼Œæ—©æœŸçš„æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡æŒ‡æ ‡ï¼ˆå¦‚BLEUã€ROUGE-Lï¼‰ä¸Šç•¥ä½äºè‡ªå›å½’æ¨¡å‹ï¼Œå›°æƒ‘åº¦ä¹Ÿè¾ƒé«˜ï¼Œç”Ÿæˆé€Ÿåº¦æ˜æ˜¾è¾ƒæ…¢ã€‚ä½†éšç€æŠ€æœ¯å‘å±•ï¼Œå¦‚Beyond Autoregressionç­‰æ–°æ–¹æ³•å¤§å¹…æå‡äº†æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å…¶åœ¨ç”Ÿæˆè´¨é‡ä¸Šæ¥è¿‘ç”šè‡³è¶…è¿‡æŸäº›è‡ªå›å½’æ¨¡å‹ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦ã€‚æ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿åœ¨äºå¹¶è¡Œç”Ÿæˆèƒ½åŠ›å’Œæ›´å¥½çš„å¯æ§æ€§ï¼Œè¿™äº›ç‰¹ç‚¹ä½¿å…¶åœ¨ç‰¹å®šåº”ç”¨åœºæ™¯ä¸­å…·æœ‰ç‹¬ç‰¹ä»·å€¼ã€‚
## å…¥é—¨æ•™ç¨‹é“¾æ¥

ä»¥ä¸‹æ˜¯ä¸€äº›é«˜è´¨é‡çš„æ‰©æ•£è¯­è¨€æ¨¡å‹å…¥é—¨æ•™ç¨‹ï¼Œå¸®åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹ï¼š

| æ•™ç¨‹åç§° | éš¾åº¦çº§åˆ« | æè¿° |
|---------|---------|------|
| [æ‰©æ•£æ¨¡å‹åŸºç¡€æ•™ç¨‹](https://www.cnblogs.com/huggingface/p/17040323.html) | åŸºç¡€ | Hugging Faceæä¾›çš„ä¸­æ–‡æ‰©æ•£æ¨¡å‹æ•™ç¨‹ï¼Œä»åŸºç¡€æ¦‚å¿µåˆ°å®è·µåº”ç”¨ï¼Œé€‚åˆåˆå­¦è€… |
| [æå®æ¯… - æ‰©æ•£æ¨¡å‹è§†é¢‘æ•™ç¨‹](https://www.bilibili.com/video/BV1XjwReqEhN/) | åŸºç¡€åˆ°è¿›é˜¶ | æå®æ¯…æ•™æˆçš„æ‰©æ•£æ¨¡å‹è¯¦ç»†è®²è§£ï¼Œé€šä¿—æ˜“æ‡‚ï¼ŒåŒ…å«ç†è®ºå’Œå®è·µ |
| [æ‰©æ•£æ¨¡å‹åŸç†åŠä»£ç å®ç°](https://www.bilibili.com/video/BV1fWAze6ECC/) | åŸºç¡€åˆ°è¿›é˜¶ | 3å°æ—¶å¿«é€Ÿä¸Šæ‰‹æ‰©æ•£æ¨¡å‹ï¼ŒåŒ…å«åŸç†è®²è§£å’Œä»£ç å®ç° |
| [Diffusion-LMè®ºæ–‡è§£è¯»](https://zhuanlan.zhihu.com/p/575543109) | è¿›é˜¶ | è¯¦ç»†è§£è¯»Diffusion-LMè®ºæ–‡ï¼ŒåŒ…å«PyTorchä»£ç è§£æ |
| [ğŸ¤— Diffusersåº“å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/diffusers/index) | è¿›é˜¶ | Hugging Face Diffusersåº“çš„å®˜æ–¹æ–‡æ¡£ï¼Œæä¾›äº†ä¸°å¯Œçš„APIå’Œç¤ºä¾‹ |

## ä¸è‡ªå›å½’æ¨¡å‹çš„æ¯”è¾ƒåˆ†æ

æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸ä¼ ç»Ÿè‡ªå›å½’è¯­è¨€æ¨¡å‹åœ¨è®¾è®¡ç†å¿µå’ŒæŠ€æœ¯å®ç°ä¸Šæœ‰æ˜¾è‘—å·®å¼‚ã€‚ä»¥ä¸‹æ˜¯ä¸¤ç±»æ¨¡å‹çš„è¯¦ç»†æ¯”è¾ƒï¼š


| ç‰¹æ€§ | è‡ªå›å½’è¯­è¨€æ¨¡å‹ | æ‰©æ•£è¯­è¨€æ¨¡å‹ |
|------|--------------|------------|
| ç”Ÿæˆæ–¹å¼ | é¡ºåºç”Ÿæˆï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªtoken | å¹¶è¡Œç”Ÿæˆï¼Œä¸€æ¬¡æ€§ç”Ÿæˆæˆ–è¿­ä»£ä¼˜åŒ–æ•´ä¸ªåºåˆ— |
| è®­ç»ƒå¤æ‚åº¦ | ç›¸å¯¹è¾ƒä½ | ç›¸å¯¹è¾ƒé«˜ï¼Œéœ€è¦æ›´å¤šè®¡ç®—èµ„æº |
| æ¨ç†é€Ÿåº¦ | å—åºåˆ—é•¿åº¦é™åˆ¶ï¼Œéš¾ä»¥å¹¶è¡Œ | éœ€è¦å¤šæ­¥è¿­ä»£ï¼Œä½†å¯ä»¥å¹¶è¡Œå¤„ç† |
| ç”Ÿæˆå¤šæ ·æ€§ | é€šè¿‡é‡‡æ ·æ¸©åº¦å’Œtop-k/top-pç­‰æ§åˆ¶ | å¤©ç„¶å…·æœ‰æ›´é«˜çš„å¤šæ ·æ€§ï¼Œå™ªå£°æ·»åŠ è¿‡ç¨‹æä¾›éšæœºæ€§ |
| å¯æ§æ€§ | éœ€è¦ç‰¹æ®Šè®¾è®¡ï¼ˆå¦‚CTRLã€PPLMç­‰ï¼‰ | å¤©ç„¶æ”¯æŒå¯æ§ç”Ÿæˆï¼Œå¯ä»¥åœ¨å»å™ªè¿‡ç¨‹ä¸­å¼•å…¥çº¦æŸ |
| é•¿æ–‡æœ¬ç”Ÿæˆ | å®¹æ˜“å‡ºç°é‡å¤ã€é—å¿˜ç­‰é—®é¢˜ | å…¨å±€ä¸€è‡´æ€§æ›´å¥½ï¼Œä½†ä»åœ¨å‘å±•ä¸­ |
| ä»£è¡¨æ¨¡å‹ | GPTç³»åˆ—ã€T5ã€LLaMAç­‰ | Diffusion-LMã€SeqDiffuSeqã€Diffusion-CoTç­‰ |

### ä¼˜ç¼ºç‚¹åˆ†æ

**è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¼˜åŠ¿**ï¼š
- è®­ç»ƒæ•ˆç‡é«˜ï¼Œèµ„æºéœ€æ±‚ç›¸å¯¹è¾ƒä½
- æˆç†Ÿçš„æŠ€æœ¯æ ˆå’Œä¸°å¯Œçš„é¢„è®­ç»ƒæ¨¡å‹
- åœ¨å¤§å¤šæ•°NLPä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚
- ç”Ÿæˆé€Ÿåº¦ç›¸å¯¹è¾ƒå¿«ï¼ˆå•æ­¥ç”Ÿæˆï¼‰

**è‡ªå›å½’è¯­è¨€æ¨¡å‹åŠ£åŠ¿**ï¼š
- ç”Ÿæˆè¿‡ç¨‹æ— æ³•å¹¶è¡ŒåŒ–ï¼Œé•¿æ–‡æœ¬ç”Ÿæˆæ•ˆç‡ä½
- å­˜åœ¨æ›å…‰åå·®(exposure bias)é—®é¢˜
- éš¾ä»¥è¿›è¡Œå…¨å±€è§„åˆ’å’Œä¿®æ”¹
- å¯æ§æ€§ç›¸å¯¹è¾ƒå¼±

**æ‰©æ•£è¯­è¨€æ¨¡å‹ä¼˜åŠ¿**ï¼š
- ç”Ÿæˆè¿‡ç¨‹å¯å¹¶è¡ŒåŒ–ï¼Œé€‚åˆæ‰¹é‡ç”Ÿæˆ
- å¤©ç„¶æ”¯æŒéè‡ªå·¦å‘å³çš„ç”Ÿæˆé¡ºåº
- æ›´å¥½çš„å¯æ§æ€§å’Œçµæ´»æ€§
- ç”Ÿæˆå†…å®¹å¤šæ ·æ€§æ›´é«˜

**æ‰©æ•£è¯­è¨€æ¨¡å‹åŠ£åŠ¿**ï¼š
- è®­ç»ƒå’Œæ¨ç†è®¡ç®—æˆæœ¬è¾ƒé«˜
- æŠ€æœ¯ç›¸å¯¹è¾ƒæ–°ï¼Œå·¥å…·é“¾ä¸å¤Ÿæˆç†Ÿ
- ç”Ÿæˆé€Ÿåº¦ä»æ˜¯ä¸»è¦ç“¶é¢ˆ
- åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸Šå°šæœªè¶…è¶Šè‡ªå›å½’æ¨¡å‹

### é€‚ç”¨åœºæ™¯åˆ†æ

**è‡ªå›å½’æ¨¡å‹é€‚åˆ**ï¼š
- å¯¹è¯ç³»ç»Ÿå’ŒèŠå¤©æœºå™¨äºº
- éœ€è¦å¿«é€Ÿå“åº”çš„å®æ—¶åº”ç”¨
- èµ„æºå—é™çš„ç¯å¢ƒ
- é€šç”¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡

**æ‰©æ•£æ¨¡å‹é€‚åˆ**ï¼š
- éœ€è¦é«˜åº¦å¯æ§çš„æ–‡æœ¬ç”Ÿæˆ
- éé¡ºåºæ–‡æœ¬ç¼–è¾‘å’Œä¿®æ”¹
- åˆ›æ„å†™ä½œå’Œå†…å®¹ç”Ÿæˆ
- å¯¹å¤šæ ·æ€§è¦æ±‚é«˜çš„åº”ç”¨åœºæ™¯

## è¯„åˆ†æ ‡å‡†ä¸ç­›é€‰æ–¹æ³•

æ¯ç¯‡è®ºæ–‡ä»ä»¥ä¸‹äº”ä¸ªç»´åº¦è¿›è¡Œè¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼Œå…¶ä¸­**ä¸»é¢˜ç›¸å…³æ€§**æ˜¯æœ€é‡è¦çš„æŒ‡æ ‡ï¼š

| è¯„åˆ†ç»´åº¦ | æƒé‡ | è¯´æ˜ |
|---------|------|------|
| **ä¸»é¢˜ç›¸å…³æ€§** | 40% | è¯¥è®ºæ–‡ä¸æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸»é¢˜çš„ç›¸å…³ç¨‹åº¦ï¼Œç‰¹åˆ«å…³æ³¨Diffusionåœ¨è¯­è¨€æ¨¡å‹ä¸Šçš„åº”ç”¨ |
| **åˆ›æ–°æ€§** | 20% | æå‡ºäº†å¤šå°‘æ–°é¢–çš„æ€æƒ³ã€æ–¹æ³•æˆ–è§è§£ |
| **å®ç”¨ä»·å€¼** | 20% | ç ”ç©¶æˆæœçš„å®é™…åº”ç”¨æ½œåŠ› |
| **æŠ€æœ¯æ·±åº¦** | 10% | æŠ€æœ¯åˆ†æçš„æ·±åº¦å’Œä¸¥è°¨æ€§ |
| **ç ”ç©¶å½±å“åŠ›** | 10% | å¯¹é¢†åŸŸçš„æ½œåœ¨å½±å“ |

æœ¬åˆ—è¡¨ä»…åŒ…å«**ä¸»é¢˜ç›¸å…³æ€§â‰¥8åˆ†**ä¸”**åŠ æƒæ€»åˆ†â‰¥7.5åˆ†**çš„è®ºæ–‡ï¼Œç¡®ä¿æ‰€æœ‰æ¨èçš„è®ºæ–‡éƒ½ä¸æ‰©æ•£è¯­è¨€æ¨¡å‹(DLM)ç›´æ¥ç›¸å…³ä¸”è´¨é‡è¾ƒé«˜ã€‚

## é«˜åˆ†è®ºæ–‡åˆ—è¡¨

å…±æ”¶å½•äº† {len(high_score_papers_sorted)} ç¯‡é«˜åˆ†è®ºæ–‡ï¼ŒæŒ‰å‘è¡¨æ—¶é—´æ’åºï¼ˆè¾ƒæ—©çš„è®ºæ–‡æ’åœ¨å‰é¢ï¼‰ï¼š

| åºå· | è®ºæ–‡æ ‡é¢˜ | ä½œè€… | åŠ æƒæ€»åˆ† | ä¸»é¢˜ç›¸å…³æ€§ | åˆ›æ–°æ€§ | å®ç”¨ä»·å€¼ | æŠ€æœ¯æ·±åº¦ | ç ”ç©¶å½±å“åŠ› |
|:----:|:--------:|:----:|:--------:|:---------:|:------:|:--------:|:--------:|:----------:|
| 1 | [Step-unrolled Denoising Autoencoders for Text Generation](https://arxiv.org/abs/2112.06749) | Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, Aaron van den Oord | **7.70** | 9 | 8 | 7 | 7 | 6 |
| 2 | [DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models](https://arxiv.org/abs/2202.04053) | Jaemin Cho, Abhay Zala, Mohit Bansal | **7.60** | 8 | 7 | 8 | 7 | 7 |
| 3 | [Diffusion-LM Improves Controllable Text Generation](https://arxiv.org/abs/2205.14217) | Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, Tatsunori B. Hashimoto | **8.20** | 10 | 8 | 7 | 7 | 8 |
| 4 | [DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models](https://arxiv.org/abs/2210.08933) | Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 5 | [SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control](https://arxiv.org/abs/2210.17432) | Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov | **7.90** | 10 | 8 | 7 | 7 | 6 |
| 6 | [Self-conditioned Embedding Diffusion for Text Generation](https://arxiv.org/abs/2211.04236) | Robin Strudel, Corentin Tallec, Florent AltchÃ©, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, RÃ©mi Leblond | **8.10** | 10 | 8 | 7 | 7 | 7 |
| 7 | [DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models](https://arxiv.org/abs/2211.15029) | Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, Xipeng Qiu | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 8 | [Continuous diffusion for categorical data](https://arxiv.org/abs/2211.15089) | Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, RÃ©mi Leblond, Will Grathwohl, Jonas Adler | **8.10** | 10 | 8 | 7 | 7 | 6 |
| 9 | [Latent Diffusion for Language Generation](https://arxiv.org/abs/2212.09462) | Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, Kilian Q. Weinberger | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 10 | [SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers](https://arxiv.org/abs/2212.10325) | Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang | **8.20** | 10 | 8 | 7 | 7 | 6 |
| 11 | [Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise](https://arxiv.org/abs/2212.11685) | Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan, Weizhu Chen | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 12 | [A Reparameterized Discrete Diffusion Model for Text Generation](https://arxiv.org/abs/2302.05737) | Lin Zheng, Jianbo Yuan, Lei Yu, Lingpeng Kong | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 13 | [DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises](https://arxiv.org/abs/2302.10025) | Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Mingxuan Wang | **8.10** | 10 | 8 | 7 | 7 | 6 |
| 14 | [Diffusion Models for Non-autoregressive Text Generation: A Survey](https://arxiv.org/abs/2303.06574) | Yifan Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen | **7.90** | 10 | 6 | 7 | 7 | 6 |
| 15 | [A Cheaper and Better Diffusion Language Model with Soft-Masked Noise](https://arxiv.org/abs/2304.04746) | Jiaao Chen, Aston Zhang, Mu Li, Alex Smola, Diyi Yang | **8.10** | 10 | 8 | 7 | 7 | 6 |
| 16 | [GlyphDiffusion: Text Generation as Image Generation](https://arxiv.org/abs/2304.12519) | Junyi Li, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen | **7.80** | 9 | 8 | 7 | 7 | 6 |
| 17 | [Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation](https://arxiv.org/abs/2305.04044) | Kun Zhou, Yifan Li, Wayne Xin Zhao, Ji-Rong Wen | **8.00** | 10 | 8 | 7 | 7 | 6 |
| 18 | [TESS: Text-to-Text Self-Conditioned Simplex Diffusion](https://arxiv.org/abs/2305.08379) | Rabeeh Karimi Mahabadi, Hamish Ivison, Jaesung Tae, James Henderson, Iz Beltagy, Matthew E. Peters, Arman Cohan | **8.10** | 10 | 8 | 7 | 7 | 7 |
| 19 | [AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation](https://arxiv.org/abs/2305.09515) | Tong Wu, Zhihao Fan, Xiao Liu, Yeyun Gong, Yelong Shen, Jian Jiao, Hai-Tao Zheng, Juntao Li, Zhongyu Wei, Jian Guo, Nan Duan, Weizhu Chen | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 20 | [Diffusion Language Models Generation Can Be Halted Early](https://arxiv.org/abs/2305.10818) | Sofia Maria Lo Cicero Vaina, Nikita Balagansky, Daniil Gavrilov | **7.90** | 10 | 7 | 8 | 6 | 6 |
| 21 | [David helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion LMs](https://arxiv.org/abs/2305.14771) | Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, Marjan Ghazvininejad | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 22 | [Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation](https://arxiv.org/abs/2305.15025) | Tianyu Yang, Thy Thy Tran, Iryna Gurevych | **7.60** | 9 | 7 | 7 | 7 | 6 |
| 23 | [Likelihood-Based Diffusion Language Models](https://arxiv.org/abs/2305.18619) | Ishaan Gulrajani, Tatsunori B. Hashimoto | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 24 | [Fine-grained Text Style Transfer with Diffusion-Based Language Models](https://arxiv.org/abs/2305.19512) | Yiwei Lyu, Tiange Luo, Jiacheng Shi, Todd C. Hollon, Honglak Lee | **8.10** | 10 | 8 | 7 | 7 | 6 |
| 25 | [DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation](https://arxiv.org/abs/2306.01657) | Guanqun Bi, Lei Shen, Yanan Cao, Meng Chen, Yuqiang Xie, Zheng Lin, Xiaodong He | **7.60** | 9 | 7 | 7 | 7 | 6 |
| 26 | [PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model](https://arxiv.org/abs/2306.02531) | Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Josh Susskind, Navdeep Jaitly | **7.90** | 9 | 8 | 7 | 7 | 7 |
| 27 | [StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models](https://arxiv.org/abs/2306.07691) | Yinghao Aaron Li, Cong Han, Vinay S. Raghavan, Gavin Mischler, Nima Mesgarani | **7.80** | 8 | 8 | 8 | 7 | 7 |
| 28 | [PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation](https://arxiv.org/abs/2306.08456) | Zhiyuan Hu, Chumin Liu, Yue Feng, Anh Tuan Luu, Bryan Hooi | **7.80** | 9 | 8 | 7 | 7 | 6 |
| 29 | [DiffuDetox: A Mixed Diffusion Model for Text Detoxification](https://arxiv.org/abs/2306.08505) | Griffin Floto, Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Zhenwei Tang, Ali Pesaranghader, Manasa Bharadwaj, Scott Sanner | **7.70** | 9 | 7 | 8 | 7 | 6 |
| 30 | [XDLM: Cross-lingual Diffusion Language Model for Machine Translation](https://arxiv.org/abs/2307.13560) | Linyao Chen, Aosong Feng, Boming Yang, Zihui Li | **7.50** | 9 | 7 | 7 | 6 | 6 |
| 31 | [Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning](https://arxiv.org/abs/2308.12219) | Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 32 | [ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer](https://arxiv.org/abs/2308.15459) | Zachary Horvitz, Ajay Patel, Chris Callison-Burch, Zhou Yu, Kathleen McKeown | **7.90** | 9 | 8 | 7 | 7 | 6 |
| 33 | [Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution](https://arxiv.org/abs/2310.16834) | Aaron Lou, Chenlin Meng, Stefano Ermon | **8.10** | 10 | 8 | 7 | 7 | 7 |
| 34 | [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://arxiv.org/abs/2310.17680) | Mukul Singh, JosÃ© Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen | **8.10** | 10 | 8 | 7 | 7 | 6 |
| 35 | [Transfer Learning for Text Diffusion Models](https://arxiv.org/abs/2401.17181) | Kehang Han, Kathleen Kenealy, Aditya Barua, Noah Fiedel, Noah Constant | **7.80** | 10 | 7 | 7 | 6 | 6 |
| 36 | [Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models](https://arxiv.org/abs/2402.07754) | Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, Lingpeng Kong | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 37 | [Text-Guided Molecule Generation with Diffusion Language Model](https://arxiv.org/abs/2402.13040) | Haisong Gong, Qiang Liu, Shu Wu, Liang Wang | **7.70** | 9 | 7 | 8 | 7 | 6 |
| 38 | [Text Diffusion with Reinforced Conditioning](https://arxiv.org/abs/2402.14843) | Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 39 | [TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings](https://arxiv.org/abs/2402.19097) | Alexander Shabalin, Viacheslav Meshchaninov, Egor Chimbulatov, Vladislav Lapikov, Roman Kim, Grigory Bartosh, Dmitry Molchanov, Sergey Markov, Dmitry Vetrov | **7.90** | 10 | 7 | 7 | 7 | 6 |
| 40 | [Differentially Private Synthetic Data via Foundation Model APIs 2: Text](https://arxiv.org/abs/2403.01749) | Chulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, Da Yu, Huseyin A Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li, Sergey Yekhanin | **7.60** | 8 | 7 | 8 | 7 | 7 |
| 41 | [Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows](https://arxiv.org/abs/2403.16995) | Shujian Zhang, Lemeng Wu, Chengyue Gong, Xingchao Liu | **7.80** | 9 | 8 | 7 | 7 | 6 |
| 42 | [DiffusionDialog: A Diffusion Model for Diverse Dialog Generation with Latent Space](https://arxiv.org/abs/2404.06760) | Jianxiang Xiang, Zhenhua Liu, Haodong Liu, Yin Bai, Jia Cheng, Wenliang Chen | **7.60** | 9 | 7 | 7 | 7 | 6 |
| 43 | [Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data](https://arxiv.org/abs/2406.03736) | Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, Chongxuan Li | **8.30** | 10 | 8 | 7 | 8 | 7 |
| 44 | [Simple and Effective Masked Diffusion Language Models](https://arxiv.org/abs/2406.07524) | Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T Chiu, Alexander Rush, Volodymyr Kuleshov | **7.90** | 10 | 8 | 7 | 7 | 6 |
| 45 | [Promises, Outlooks and Challenges of Diffusion Language Modeling](https://arxiv.org/abs/2406.11473) | Justin Deschenaux, Caglar Gulcehre | **7.90** | 10 | 7 | 7 | 7 | 6 |
| 46 | [DiffuseDef: Improved Robustness to Adversarial Attacks](https://arxiv.org/abs/2407.00248) | Zhenhao Li, Marek Rei, Lucia Specia | **7.50** | 9 | 7 | 7 | 7 | 6 |
| 47 | [Discrete Diffusion Language Model for Long Text Summarization](https://arxiv.org/abs/2407.10998) | Do Huu Dat, Do Duc Anh, Anh Tuan Luu, Wray Buntine | **7.90** | 10 | 8 | 7 | 7 | 7 |
| 48 | [Diffusion Guided Language Modeling](https://arxiv.org/abs/2408.04220) | Justin Lovelace, Varsha Kishore, Yiwei Chen, Kilian Q. Weinberger | **8.40** | 10 | 8 | 8 | 7 | 7 |
| 49 | [Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion](https://arxiv.org/abs/2408.05636) | Jacob K Christopher, Brian R Bartoldson, Tal Ben-Nun, Michael Cardei, Bhavya Kailkhura, Ferdinando Fioretto | **8.20** | 10 | 8 | 8 | 7 | 7 |
| 50 | [Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling](https://arxiv.org/abs/2409.02908) | Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang | **8.20** | 10 | 8 | 7 | 8 | 7 |
| 51 | [An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification](https://arxiv.org/abs/2409.03203) | Zhuowei Chen, Lianxi Wang, Yuben Wu, Xinfeng Liao, Yujia Tian, Junyang Zhong | **7.70** | 9 | 7 | 8 | 7 | 6 |
| 52 | [Towards Diverse and Efficient Audio Captioning via Diffusion Models](https://arxiv.org/abs/2409.09401) | Manjie Xu, Chenxing Li, Xinyi Tu, Yong Ren, Ruibo Fu, Wei Liang, Dong Yu | **7.60** | 9 | 7 | 7 | 7 | 6 |
| 53 | [Think While You Generate: Discrete Diffusion with Planned Denoising](https://arxiv.org/abs/2410.06264) | Sulin Liu, Juno Nam, Andrew Campbell, Hannes StÃ¤rk, Yilun Xu, Tommi Jaakkola, Rafael GÃ³mez-Bombarelli | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 54 | [Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration](https://arxiv.org/abs/2410.13201) | Yun-Yen Chuang, Hung-Min Hsu, Kevin Lin, Chen-Sheng Gu, Ling Zhen Li, Ray-I Chang, Hung-yi Lee | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 55 | [Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning](https://arxiv.org/abs/2410.14157) | Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong | **7.80** | 9 | 8 | 7 | 7 | 7 |
| 56 | [Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://arxiv.org/abs/2410.17891) | Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, Lingpeng Kong | **8.20** | 10 | 8 | 8 | 7 | 7 |
| 57 | [Scaling up Masked Diffusion Models on Text](https://arxiv.org/abs/2410.18514) | Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, Chongxuan Li | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 58 | [Beyond Autoregression: Fast LLMs via Self-Distillation Through Time](https://arxiv.org/abs/2410.21035) | Justin Deschenaux, Caglar Gulcehre | **8.30** | 10 | 8 | 8 | 7 | 7 |
| 59 | [Energy-Based Diffusion Language Models for Text Generation](https://arxiv.org/abs/2410.21357) | Minkai Xu, Tomas Geffner, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, Arash Vahdat | **8.10** | 10 | 8 | 7 | 7 | 7 |
| 60 | [DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models](https://arxiv.org/abs/2411.03250) | Ying Zhou, Xinyao Wang, Yulei Niu, Yaojie Shen, Lexin Tang, Fan Chen, Ben He, Le Sun, Longyin Wen | **7.80** | 9 | 8 | 7 | 7 | 6 |
| 61 | [Conditional [MASK] Discrete Diffusion Language Model](https://arxiv.org/abs/2411.06438) | Hyukhun Koh, Minha Jhang, Dohyung Kim, Sangmook Lee, Kyomin Jung | **7.80** | 9 | 8 | 7 | 7 | 6 |
| 62 | [Multimodal Latent Language Modeling with Next-Token Diffusion](https://arxiv.org/abs/2412.08635) | Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei | **8.00** | 9 | 8 | 8 | 7 | 7 |
| 63 | [Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models](https://arxiv.org/abs/2412.11333) | Xiaochen Zhu, Georgi Karadzhov, Chenxi Whitehouse, Andreas Vlachos | **8.10** | 10 | 8 | 7 | 7 | 6 |
| 64 | [DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak](https://arxiv.org/abs/2412.17522) | Hao Wang, Hao Li, Junda Zhu, Xinyuan Wang, Chengwei Pan, MinLie Huang, Lei Sha | **7.70** | 9 | 8 | 7 | 7 | 6 |
| 65 | [Large Language Models to Diffusion Finetuning](https://arxiv.org/abs/2501.15781) | Edoardo Cetin, Tianyu Zhao, Yujin Tang | **7.80** | 9 | 8 | 7 | 7 | 7 |
| 66 | [Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods](https://arxiv.org/abs/2502.01384) | Oussama Zekri, Nicolas BoullÃ© | **7.90** | 9 | 8 | 7 | 7 | 6 |
| 67 | [DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation](https://arxiv.org/abs/2502.03930) | Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang | **7.80** | 9 | 8 | 7 | 7 | 6 |
| 68 | [Theoretical Benefit and Limitation of Diffusion Language Model](https://arxiv.org/abs/2502.09622) | Guhao Feng, Yihan Geng, Jian Guan, Wei Wu, Liwei Wang, Di He | **8.20** | 10 | 8 | 7 | 8 | 7 |
| 69 | [Non-Markovian Discrete Diffusion with Causal Language Models](https://arxiv.org/abs/2502.09767) | Yangtian Zhang, Sizhuang He, Daniel Levine, Lawrence Zhao, David Zhang, Syed A Rizvi, Emanuele Zappala, Rex Ying, David van Dijk | **8.10** | 10 | 8 | 7 | 7 | 7 |
| 70 | [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992) | Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li | **7.90** | 10 | 8 | 7 | 7 | 7 |
| 71 | [TESS 2: A Large-Scale Generalist Diffusion Language Model](https://arxiv.org/abs/2502.13917) | Jaesung Tae, Hamish Ivison, Sachin Kumar, Arman Cohan | **8.20** | 10 | 8 | 7 | 7 | 7 |
| 72 | [EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models](https://arxiv.org/abs/2502.19765) | Che Hyun Lee, Heeseung Kim, Jiheum Yeom, Sungroh Yoon | **7.80** | 9 | 7 | 8 | 7 | 6 |

## è¯¦ç»†è®ºæ–‡åˆ†æ

ä»¥ä¸‹æ˜¯æ¯ç¯‡é«˜åˆ†è®ºæ–‡çš„è¯¦ç»†åˆ†æï¼ŒåŒ…æ‹¬æ‘˜è¦ã€è¯„åˆ†è¯¦æƒ…ã€ä¼˜ç¼ºç‚¹å’Œé˜…è¯»å»ºè®®ï¼š

### 1. Step-unrolled Denoising Autoencoders for Text Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 7.70/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, Aaron van den Oord
- **URL**: [https://arxiv.org/abs/2112.06749](https://arxiv.org/abs/2112.06749)

#### æ‘˜è¦

In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models. Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a sequence of tokens, starting from random inputs and improving them each t...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: SUNDAE æ˜¯ä¸€ç§æ–°çš„éè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œå¯èƒ½æ¯”ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹æ”¶æ•›æ›´å¿«ï¼Œå¹¶åœ¨æŸäº›ä»»åŠ¡ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚éè‡ªå›å½’ç‰¹æ€§ä½¿å…¶é€‚ç”¨äºå¡«å……æ¨¡æ¿ç­‰ä»»åŠ¡ï¼Œåº”ç”¨å‰æ™¯å¹¿é˜”ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ç¼ºä¹æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ·±å…¥è¯„ä¼° SUNDAE çš„ç†è®ºåŸºç¡€ã€ç®—æ³•å®ç°å’Œå®éªŒéªŒè¯ã€‚æ­¤å¤–ï¼ŒSUNDAE åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æœ€å…ˆè¿›çš„è‡ªå›å½’æ¨¡å‹ç›¸æ¯”å¦‚ä½•ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºSUNDAEç®—æ³•çš„ç»†èŠ‚ã€æ”¹è¿›ç®—å­çš„è®¾è®¡ã€å®éªŒè®¾ç½®å’Œç»“æœåˆ†æéƒ¨åˆ†ã€‚å…³æ³¨SUNDAEåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä»¥åŠä¸ç°æœ‰æ–¹æ³•çš„æ¯”è¾ƒã€‚å¯ä»¥å…³æ³¨åç»­æ˜¯å¦æœ‰ç ”ç©¶åŸºäºSUNDAEè¿›è¡Œæ”¹è¿›æˆ–åº”ç”¨ã€‚

---

### 2. DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models

<div align="center">

**åŠ æƒæ€»åˆ†: 7.60/10** | **ä¸»é¢˜ç›¸å…³æ€§: 8/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Jaemin Cho, Abhay Zala, Mohit Bansal
- **URL**: [https://arxiv.org/abs/2202.04053](https://arxiv.org/abs/2202.04053)

#### æ‘˜è¦

Recently, DALL-E, a multimodal transformer language model, and its variants, including diffusion models, have shown high-quality text-to-image generation capabilities. However, despite the realistic image generation results, there has not been a detailed analysis of how to evaluate such models. In t...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 8/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹è§†è§‰æ¨ç†èƒ½åŠ›çš„æ–°æ•°æ®é›†(PaintSkills)ã€‚ è¯¦ç»†åˆ†æäº†ç°æœ‰æ¨¡å‹ä¸­çš„ç¤¾ä¼šåè§ã€‚ ç ”ç©¶ç»“æœå…·æœ‰å¾ˆå¼ºçš„å®ç”¨ä»·å€¼ï¼Œå¯ä»¥æŒ‡å¯¼æœªæ¥æ¨¡å‹çš„å‘å±•ã€‚
- **ç¼ºç‚¹**: å¯¹é€ æˆæ¨¡å‹æ€§èƒ½ä¸è¶³å’Œç¤¾ä¼šåè§çš„åŸå› çš„åˆ†æä¸å¤Ÿæ·±å…¥ã€‚ è®ºæ–‡å¯èƒ½æ›´ä¾§é‡äºè¯„ä¼°ï¼Œè€Œå¯¹å¦‚ä½•è§£å†³è¿™äº›é—®é¢˜æä¾›çš„è§£å†³æ–¹æ¡ˆç›¸å¯¹è¾ƒå°‘ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»è¯¥è®ºæ–‡ä»¥äº†è§£å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯å…³äºè§†è§‰æ¨ç†èƒ½åŠ›å’Œç¤¾ä¼šåè§æ–¹é¢ã€‚ PaintSkillsæ•°æ®é›†å¯¹äºå¼€å‘å’Œè¯„ä¼°æœªæ¥çš„æ¨¡å‹éå¸¸æœ‰ä»·å€¼ã€‚ å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶è®ºæ–‡çš„å‚è€ƒæ–‡çŒ®ï¼Œæ·±å…¥äº†è§£æ‰©æ•£æ¨¡å‹çš„ç†è®ºåŸºç¡€å’Œç¤¾ä¼šåè§ç¼“è§£æ–¹æ³•ã€‚

---

### 3. Diffusion-LM Improves Controllable Text Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, Tatsunori B. Hashimoto
- **URL**: [https://arxiv.org/abs/2205.14217](https://arxiv.org/abs/2205.14217)

#### æ‘˜è¦

Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 8/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡çš„ä¼˜ç‚¹åœ¨äºå…¶ä¸»é¢˜ä¸æ‰©æ•£è¯­è¨€æ¨¡å‹ç´§å¯†ç›¸å…³ï¼Œå¹¶ä¸”æå‡ºäº†å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºå¯æ§æ–‡æœ¬ç”Ÿæˆçš„æ–°é¢–æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨å¤æ‚æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨ä»·å€¼å’Œæ½œåœ¨å½±å“åŠ›ã€‚
- **ç¼ºç‚¹**: ä»…ä»…åŸºäºæ‘˜è¦å¾ˆéš¾è¯„ä¼°è®ºæ–‡çš„ç¼ºç‚¹ã€‚å¯èƒ½çš„ç¼ºç‚¹åŒ…æ‹¬ï¼šæ‰©æ•£æ¨¡å‹çš„è®¡ç®—æˆæœ¬å¯èƒ½è¾ƒé«˜ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–æ‰èƒ½åœ¨å®é™…åº”ç”¨ä¸­å®ç°é«˜æ•ˆç”Ÿæˆï¼›æ¨¡å‹çš„å¯è§£é‡Šæ€§å¯èƒ½è¾ƒå·®ï¼›å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡å¯èƒ½å­˜åœ¨å±€é™æ€§ã€‚éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶æ‰èƒ½æ˜ç¡®ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨Diffusion-LMçš„ç®—æ³•ç»†èŠ‚ã€å®éªŒè®¾ç½®ã€ç»“æœåˆ†æä»¥åŠä¸ç°æœ‰æ–¹æ³•çš„æ¯”è¾ƒã€‚åŒæ—¶ï¼Œå¯ä»¥å…³æ³¨è®ºæ–‡ä¸­å…³äºæ¨¡å‹å±€é™æ€§å’Œæœªæ¥ç ”ç©¶æ–¹å‘çš„è®¨è®ºï¼Œä»¥ä¾¿æ›´å…¨é¢åœ°äº†è§£è¯¥ç ”ç©¶çš„ä»·å€¼å’Œæ„ä¹‰ã€‚

---

### 4. DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong
- **URL**: [https://arxiv.org/abs/2210.08933](https://arxiv.org/abs/2210.08933)

#### æ‘˜è¦

Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generatio...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡ä¼˜ç‚¹åœ¨äºå…¶ä¸»é¢˜èšç„¦æ€§å¼ºï¼Œåˆ›æ–°æ€§åœ°å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºSeq2Seqæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å–å¾—äº†ä¸é”™çš„å®éªŒç»“æœã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†æ¨¡å‹çš„å¤šæ ·æ€§ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶è¿›è¡Œäº†ç†è®ºåˆ†æã€‚ä»£ç å¼€æºä¿è¯äº†å¯å¤ç°æ€§ã€‚
- **ç¼ºç‚¹**: ç†è®ºåˆ†æçš„æ·±åº¦å¯ä»¥è¿›ä¸€æ­¥åŠ å¼ºï¼Œå¯ä»¥æ›´æ·±å…¥åœ°æ¢è®¨æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚å®éªŒéƒ¨åˆ†å¯ä»¥å¢åŠ æ›´å¤šä¸åŒçš„æ•°æ®é›†å’ŒåŸºçº¿æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡çš„å®éªŒéƒ¨åˆ†å’Œç†è®ºåˆ†æéƒ¨åˆ†ï¼Œé‡ç‚¹å…³æ³¨DiffuSeqçš„è®¾è®¡ä»¥åŠå…¶ä¸ä¼ ç»ŸSeq2Seqæ¨¡å‹çš„åŒºåˆ«ã€‚å¯ä»¥å°è¯•å¤ç°å®éªŒç»“æœï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›æˆ–æ‰©å±•ï¼Œä¾‹å¦‚æ¢ç´¢ä¸åŒçš„å™ªå£° schedule æˆ–æ¨¡å‹æ¶æ„ã€‚

---

### 5. SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control

<div align="center">

**åŠ æƒæ€»åˆ†: 7.90/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov
- **URL**: [https://arxiv.org/abs/2210.17432](https://arxiv.org/abs/2210.17432)

#### æ‘˜è¦

Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM -- a diffusion-based language model with two key design ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: SSD-LM æ¨¡å‹çš„è®¾è®¡å…·æœ‰åˆ›æ–°æ€§ï¼Œç»“åˆäº†åŠè‡ªå›å½’å’Œå•çº¯å½¢æ‰©æ•£çš„ä¼˜åŠ¿ï¼Œå¹¶åœ¨æ–‡æœ¬ç”Ÿæˆå’Œæ¨¡å—åŒ–æ§åˆ¶æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ã€‚ ç›´æ¥åœ¨è¯æ±‡ç©ºé—´ä¸Šæ‰©æ•£ä½¿å¾—èƒ½å¤Ÿæ–¹ä¾¿åœ°è¿›è¡Œæ¨¡å—åŒ–æ§åˆ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„ä¼˜ç‚¹ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹æŠ€æœ¯ç»†èŠ‚çš„è¯¦ç»†æè¿°ï¼Œéš¾ä»¥è¯„ä¼°å…¶ç†è®ºæ·±åº¦å’Œä¸¥è°¨æ€§ã€‚ è¿˜éœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½äº†è§£å®éªŒè®¾ç½®å’Œç»“æœåˆ†æçš„ç»†èŠ‚ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨SSD-LMæ¨¡å‹çš„å…·ä½“å®ç°ç»†èŠ‚ï¼ŒåŒ…æ‹¬æ‰©æ•£è¿‡ç¨‹ã€åŠè‡ªå›å½’ç»“æ„çš„å®ç°ä»¥åŠå•çº¯å½¢æ‰©æ•£çš„å…·ä½“æ–¹æ³•ã€‚ åŒæ—¶ï¼Œå…³æ³¨å®éªŒè®¾ç½®å’Œç»“æœåˆ†æï¼ŒéªŒè¯è®ºæ–‡çš„ç»“è®ºã€‚ å¦‚æœå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹æ„Ÿå…´è¶£ï¼Œè¿™ç¯‡è®ºæ–‡å€¼å¾—æ·±å…¥ç ”ç©¶ã€‚

---

### 6. Self-conditioned Embedding Diffusion for Text Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 8.10/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Robin Strudel, Corentin Tallec, Florent AltchÃ©, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, RÃ©mi Leblond
- **URL**: [https://arxiv.org/abs/2211.04236](https://arxiv.org/abs/2211.04236)

#### æ‘˜è¦

Can continuous diffusion models bring the same performance breakthrough on natural language they did for image generation? To circumvent the discrete nature of text data, we can simply project tokens in a continuous space of embeddings, as is standard in language modeling. We propose Self-conditione...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡é€‰é¢˜å…·æœ‰å‰æ²¿æ€§ï¼Œå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼Œæå‡ºäº†æ–°é¢–çš„Self-conditioned Embedding Diffusionæ–¹æ³•ï¼Œå…·æœ‰ä¸€å®šçš„å®ç”¨ä»·å€¼å’Œç ”ç©¶å½±å“åŠ›ã€‚ä½œè€…å›¢é˜Ÿçš„èƒŒæ™¯ä¹Ÿæ¯”è¾ƒå¼ºï¼Œæ¥è‡ªçŸ¥åçš„ç ”ç©¶æœºæ„ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­å¯¹æŠ€æœ¯ç»†èŠ‚çš„æè¿°ç›¸å¯¹æœ‰é™ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½äº†è§£æ›´è¯¦ç»†çš„æŠ€æœ¯å®ç°å’Œå®éªŒç»“æœã€‚å…³äºæ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œç”Ÿæˆæ–‡æœ¬çš„è´¨é‡æ§åˆ¶æ–¹é¢ï¼Œæ‘˜è¦æ²¡æœ‰æåŠï¼Œéœ€è¦è¿›ä¸€æ­¥çš„è¯„ä¼°ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨Self-conditioned Embedding Diffusionçš„å…·ä½“å®ç°ç»†èŠ‚ï¼Œå®éªŒè®¾ç½®å’Œç»“æœï¼Œä»¥åŠä¸å…¶ä»–æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„æ¯”è¾ƒã€‚å…³æ³¨è®ºæ–‡ä¸­å¯¹æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„æŒ‘æˆ˜çš„è®¨è®ºï¼Œä»¥åŠæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚

---

### 7. DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, Xipeng Qiu
- **URL**: [https://arxiv.org/abs/2211.15029](https://arxiv.org/abs/2211.15029)

#### æ‘˜è¦

We present DiffusionBERT, a new generative masked language model based on discrete diffusion models. Diffusion models and many pre-trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On th...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡å°†æ‰©æ•£æ¨¡å‹ä¸é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹BERTç›¸ç»“åˆï¼Œæå‡ºäº†DiffusionBERTï¼Œå¹¶é€šè¿‡æ–°çš„å™ªå£°è°ƒåº¦ç­–ç•¥å’Œæ—¶é—´æ­¥èåˆæ–¹æ³•è¿›è¡Œæ”¹è¿›ï¼Œåœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è®ºæ–‡çš„å®éªŒè®¾è®¡è¾ƒä¸ºåˆç†ï¼Œç»“æœä¹Ÿå…·æœ‰ä¸€å®šçš„è¯´æœåŠ›ã€‚
- **ç¼ºç‚¹**: è™½ç„¶æœ‰ä¸€å®šåˆ›æ–°ï¼Œä½†æ˜¯æ›´å¤šçš„æ˜¯å¯¹ç°æœ‰æŠ€æœ¯çš„ç»„åˆå’Œæ”¹è¿›ï¼Œç¼ºä¹é©å‘½æ€§çš„ç†è®ºçªç ´ã€‚å¦å¤–ï¼Œè®ºæ–‡å¯èƒ½éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢æ¨¡å‹çš„å¯è§£é‡Šæ€§ä»¥åŠåœ¨æ›´å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡çš„å®éªŒéƒ¨åˆ†ï¼Œé‡ç‚¹å…³æ³¨å™ªå£°è°ƒåº¦ç­–ç•¥å’Œæ—¶é—´æ­¥èåˆæ–¹æ³•çš„å®ç°ç»†èŠ‚ï¼Œä»¥åŠè¿™äº›æ–¹æ³•å¯¹æ€§èƒ½çš„å½±å“ã€‚å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•å°†è¯¥æ–¹æ³•åº”ç”¨åˆ°å…¶ä»–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå¹¶æ¢ç´¢æ›´é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•ã€‚

---

### 8. Continuous diffusion for categorical data

<div align="center">

**åŠ æƒæ€»åˆ†: 8.10/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, RÃ©mi Leblond, Will Grathwohl, Jonas Adler
- **URL**: [https://arxiv.org/abs/2211.15089](https://arxiv.org/abs/2211.15089)

#### æ‘˜è¦

Diffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals (such as images and sound) through iterative refinement. Their success hinges on the fact that the underlying physical phenomena are continuous. For inherently discrete and categorical data such as...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: å°†è¿ç»­æ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹åº”ç”¨äºç¦»æ•£çš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œæå‡ºäº†æ–°çš„CDCDæ¡†æ¶ï¼Œå¯èƒ½åœ¨æ–‡æœ¬ç”Ÿæˆå’Œåºåˆ—é¢„æµ‹æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚
- **ç¼ºç‚¹**: éœ€è¦è¿›ä¸€æ­¥é˜…è¯»å…¨æ–‡æ‰èƒ½åˆ¤æ–­å…¶ç†è®ºæ·±åº¦å’Œå®éªŒéªŒè¯çš„å……åˆ†æ€§ã€‚æ‘˜è¦å¹¶æœªæ˜ç¡®è¯´æ˜CDCDæ¡†æ¶çš„å…·ä½“å®ç°ç»†èŠ‚å’Œå±€é™æ€§ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨CDCDæ¡†æ¶çš„å…·ä½“å®ç°ç»†èŠ‚ï¼Œä»¥åŠå®éªŒè®¾ç½®å’Œç»“æœåˆ†æã€‚ å¯ä»¥å…³æ³¨è¯¥æ–¹æ³•åœ¨ä¸åŒè¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶ä¸å…¶ä»–ç°æœ‰çš„æ‰©æ•£è¯­è¨€æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚ æ­¤å¤–ï¼Œè¿˜éœ€è¦å…³æ³¨è¯¥æ–¹æ³•çš„è®¡ç®—å¤æ‚åº¦ï¼Œä»¥åŠåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å¯æ‰©å±•æ€§ã€‚

---

### 9. Latent Diffusion for Language Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, Kilian Q. Weinberger
- **URL**: [https://arxiv.org/abs/2212.09462](https://arxiv.org/abs/2212.09462)

#### æ‘˜è¦

Diffusion models have achieved great success in modeling continuous data modalities such as images, audio, and video, but have seen limited use in discrete domains such as language. Recent attempts to adapt diffusion to language have presented diffusion as an alternative to existing pretrained langu...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: ç»“åˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæå‡ºåœ¨latent spaceä¸­è¿›è¡Œæ‰©æ•£çš„æ–¹æ³•ï¼Œè§£å†³äº†ç›´æ¥åœ¨ç¦»æ•£ç©ºé—´è¿›è¡Œæ‰©æ•£çš„å›°éš¾ã€‚å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šç§è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•ä¹Ÿå……åˆ†åˆ©ç”¨äº†ç°æœ‰PLMçš„çŸ¥è¯†ï¼Œé¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒdiffusion modelã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­æ²¡æœ‰æåŠæ¨¡å‹è®­ç»ƒçš„å¤æ‚åº¦å’Œèµ„æºéœ€æ±‚ï¼Œéœ€è¦è¿›ä¸€æ­¥è¯„ä¼°ã€‚å¯¹æ‰©æ•£è¿‡ç¨‹çš„æ§åˆ¶èƒ½åŠ›å¯èƒ½æœ‰é™ï¼Œç”Ÿæˆçš„æ–‡æœ¬å¯èƒ½ç¼ºä¹å¤šæ ·æ€§å’Œåˆ›é€ æ€§ã€‚å…·ä½“çš„æ•ˆæœéœ€è¦åœ¨é˜…è¯»å®Œæ•´è®ºæ–‡å’ŒæŸ¥çœ‹å®éªŒç»“æœåæ‰èƒ½åˆ¤æ–­ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è¯¥è®ºæ–‡ï¼Œç‰¹åˆ«å…³æ³¨latent spaceçš„è®¾è®¡ã€æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒç»†èŠ‚ä»¥åŠå®éªŒç»“æœçš„åˆ†æã€‚å¯ä»¥å°†å…¶ä¸å…¶ä»–æ‰©æ•£è¯­è¨€æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œåˆ†æå…¶ä¼˜ç¼ºç‚¹ã€‚å¦‚æœå¯¹DLMæ„Ÿå…´è¶£ï¼Œå¯ä»¥å°è¯•å¤ç°è¯¥è®ºæ–‡çš„ç»“æœï¼Œå¹¶æ¢ç´¢å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

---

### 10. SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang
- **URL**: [https://arxiv.org/abs/2212.10325](https://arxiv.org/abs/2212.10325)

#### æ‘˜è¦

Diffusion model, a new generative modelling paradigm, has achieved great success in image, audio, and video generation. However, considering the discrete categorical nature of text, it is not trivial to extend continuous diffusion models to natural language, and text diffusion models are less studie...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡ä¸»é¢˜æ˜ç¡®ï¼Œèšç„¦æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨åºåˆ—åˆ°åºåˆ—ç”Ÿæˆä»»åŠ¡ä¸Šçš„åº”ç”¨ã€‚æå‡ºSeqDiffuSeqæ¨¡å‹ï¼Œå¹¶ç»“åˆè‡ªé€‚åº”å™ªå£°è°ƒåº¦ç­‰æŠ€æœ¯ï¼Œæœ‰ä¸€å®šçš„åˆ›æ–°æ€§ã€‚å®éªŒç»“æœè¡¨æ˜æ¨¡å‹æ€§èƒ½è‰¯å¥½ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­æ²¡æœ‰æåŠå…·ä½“çš„å®éªŒç»†èŠ‚å’Œæ•°æ®é›†ï¼Œæ— æ³•åˆ¤æ–­å®éªŒçš„å……åˆ†æ€§å’Œå¯é æ€§ã€‚éœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½è¯„ä¼°ç†è®ºåˆ†æçš„æ·±åº¦å’Œä¸¥è°¨æ€§ã€‚å¯¹æ¨¡å‹æ€§èƒ½æå‡çš„å…·ä½“åŸå› åˆ†æå¯èƒ½ä¸å¤Ÿæ·±å…¥ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨è‡ªé€‚åº”å™ªå£°è°ƒåº¦çš„å…·ä½“å®ç°å’Œæœ‰æ•ˆæ€§ï¼Œä»¥åŠå®éªŒéƒ¨åˆ†çš„è¯¦ç»†è®¾ç½®å’Œç»“æœåˆ†æã€‚åŒæ—¶ï¼Œå¯ä»¥å…³æ³¨è¯¥è®ºæ–‡ä¸å…¶ä»–æ–‡æœ¬æ‰©æ•£æ¨¡å‹çš„ç›¸å…³å·¥ä½œå¯¹æ¯”ï¼Œä»¥åŠæœªæ¥ç ”ç©¶æ–¹å‘çš„å±•æœ›ã€‚

---

### 11. Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan, Weizhu Chen
- **URL**: [https://arxiv.org/abs/2212.11685](https://arxiv.org/abs/2212.11685)

#### æ‘˜è¦

In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a rand...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹GENIEï¼Œåˆ©ç”¨è¿ç»­æ®µè½å»å™ªç›®æ ‡å‡½æ•°ï¼Œå¹¶åœ¨å¤šä¸ªæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ•ˆæœã€‚å¼€æºäº†ä»£ç å’Œæ¨¡å‹ï¼Œæ–¹ä¾¿å¤ç°å’Œä½¿ç”¨ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹æ‰©æ•£æ¨¡å‹ç»†èŠ‚å’Œè¿ç»­æ®µè½å»å™ªç›®æ ‡å‡½æ•°çš„è¯¦ç»†æŠ€æœ¯æè¿°ï¼Œå¯èƒ½éœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ·±å…¥äº†è§£ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºè¿ç»­æ®µè½å»å™ªç›®æ ‡å‡½æ•°çš„ç»†èŠ‚ä»¥åŠæ‰©æ•£æ¨¡å‹çš„å…·ä½“å®ç°ã€‚å¯ä»¥å°è¯•å¤ç°è®ºæ–‡çš„ç»“æœï¼Œå¹¶åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œæ¢ç´¢GENIEåœ¨ä¸åŒåœºæ™¯ä¸‹çš„åº”ç”¨ã€‚

---

### 12. A Reparameterized Discrete Diffusion Model for Text Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Lin Zheng, Jianbo Yuan, Lei Yu, Lingpeng Kong
- **URL**: [https://arxiv.org/abs/2302.05737](https://arxiv.org/abs/2302.05737)

#### æ‘˜è¦

This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡èšç„¦äºæ‰©æ•£è¯­è¨€æ¨¡å‹è¿™ä¸€é‡è¦ç ”ç©¶æ–¹å‘ï¼Œæå‡ºäº†é‡å‚æ•°åŒ–çš„æ–°æ¡†æ¶ï¼Œå¹¶å£°ç§°åœ¨è®­ç»ƒå’Œè§£ç æ–¹é¢æœ‰æ‰€æ”¹è¿›ï¼Œå…·æœ‰æ½œåœ¨çš„å®ç”¨ä»·å€¼å’Œç ”ç©¶å½±å“åŠ›ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ç¼ºä¹æ›´å…·ä½“çš„ç»†èŠ‚ï¼Œä¾‹å¦‚å…·ä½“çš„é‡å‚æ•°åŒ–æ–¹æ³•ã€è®­ç»ƒå’Œè§£ç æŠ€æœ¯çš„ç»†èŠ‚ï¼Œä»¥åŠå®éªŒç»“æœçš„é‡åŒ–æŒ‡æ ‡ï¼Œå› æ­¤éš¾ä»¥å®Œå…¨è¯„ä¼°å…¶æŠ€æœ¯æ·±åº¦å’Œå®ç”¨ä»·å€¼ã€‚éœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½è¿›è¡Œæ›´å‡†ç¡®çš„è¯„ä¼°ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨é‡å‚æ•°åŒ–çš„å…·ä½“å®ç°æ–¹å¼ã€æ–°çš„è®­ç»ƒå’Œè§£ç æŠ€æœ¯ç»†èŠ‚ï¼Œä»¥åŠå®éªŒéƒ¨åˆ†çš„æŒ‡æ ‡å’Œå¯¹æ¯”ç»“æœã€‚æ­¤å¤–ï¼Œå¯ä»¥å…³æ³¨è¯¥æ–¹æ³•åœ¨é•¿æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿç­‰å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚

---

### 13. DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises

<div align="center">

**åŠ æƒæ€»åˆ†: 8.10/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Mingxuan Wang
- **URL**: [https://arxiv.org/abs/2302.10025](https://arxiv.org/abs/2302.10025)

#### æ‘˜è¦

While diffusion models have achieved great success in generating continuous signals such as images and audio, it remains elusive for diffusion models in learning discrete sequence data like natural languages. Although recent advances circumvent this challenge of discreteness by embedding discrete to...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡èšç„¦äºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒé—®é¢˜ï¼Œæå‡ºäº†æ–°é¢–çš„å™ªå£°æ“çºµæ–¹æ³•ï¼Œå¹¶å–å¾—äº†åˆæ­¥çš„å®éªŒæˆæœã€‚æ˜ç¡®æŒ‡å‡ºäº†ç°æœ‰æ‰©æ•£è¯­è¨€æ¨¡å‹çš„ä¸è¶³ï¼Œå¹¶å°è¯•è§£å†³è¿™äº›é—®é¢˜ã€‚ æ‘˜è¦æ¸…æ™°åœ°é˜è¿°äº†è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³å’Œè´¡çŒ®ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹DINOISERå…·ä½“å®ç°ç»†èŠ‚çš„æè¿°ï¼Œä¾‹å¦‚å™ªå£°å°ºåº¦è‡ªé€‚åº”çš„å…·ä½“ç®—æ³•å’Œæ¨ç†è¿‡ç¨‹ä¸­çš„å™ªå£°æ”¾å¤§ç­–ç•¥ã€‚å®ç”¨ä»·å€¼å’Œç ”ç©¶å½±å“åŠ›è¿˜éœ€è¿›ä¸€æ­¥éªŒè¯ï¼Œéœ€è¦å…³æ³¨å®éªŒç»“æœçš„æ˜¾è‘—æ€§å’Œæ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨DINOISERçš„å…·ä½“å®ç°ç»†èŠ‚ã€å®éªŒè®¾è®¡å’Œç»“æœåˆ†æã€‚å°¤å…¶è¦æ³¨æ„è®ºæ–‡å¦‚ä½•è‡ªé€‚åº”åœ°ç¡®å®šå™ªå£°å°ºåº¦èŒƒå›´ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨æ”¾å¤§å™ªå£°å°ºåº¦çš„æºæ¡ä»¶æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ å…³æ³¨å®éªŒç»“æœæ˜¯å¦å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œä»¥åŠè¯¥æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†å’Œä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ å¦‚æœå¯¹æ‰©æ•£æ¨¡å‹æ„Ÿå…´è¶£ï¼Œå¯ä»¥æ·±å…¥ç ”ç©¶ç›¸å…³çš„ç†è®ºèƒŒæ™¯å’Œæœ€æ–°çš„ç ”ç©¶è¿›å±•ã€‚

---

### 14. Diffusion Models for Non-autoregressive Text Generation: A Survey

<div align="center">

**åŠ æƒæ€»åˆ†: 7.90/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Yifan Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen
- **URL**: [https://arxiv.org/abs/2303.06574](https://arxiv.org/abs/2303.06574)

#### æ‘˜è¦

Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 6/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: ç³»ç»Ÿæ€§åœ°æ€»ç»“äº†æ‰©æ•£æ¨¡å‹åœ¨éè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œæ¶µç›–äº†æ¨¡å‹çš„å®šä¹‰ã€ä¸»è¦ç±»å‹ã€å…³é”®è®¾è®¡ã€ä¸é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç»“åˆä»¥åŠä¼˜åŒ–æŠ€æœ¯ã€‚æä¾›äº†ç›¸å…³ç ”ç©¶çš„GitHubé“¾æ¥ï¼Œæ–¹ä¾¿è¯»è€…æ·±å…¥äº†è§£ã€‚
- **ç¼ºç‚¹**: ç»¼è¿°æ€§è´¨å†³å®šäº†å…¶åœ¨åˆ›æ–°æ€§å’ŒæŠ€æœ¯æ·±åº¦ä¸Šç›¸å¯¹æœ‰é™ï¼Œå¯èƒ½ç¼ºå°‘å¯¹æŸäº›ç‰¹å®šæ¨¡å‹çš„æ·±å…¥åˆ†æå’Œå®éªŒç»“æœçš„æ¯”è¾ƒã€‚ä¾èµ–äºå·²å‘è¡¨çš„ç ”ç©¶ï¼Œæ— æ³•æä¾›å…¨æ–°çš„å®éªŒæ•°æ®å’Œç»“è®ºã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»è¯¥è®ºæ–‡ä»¥å¿«é€Ÿäº†è§£æ‰©æ•£æ¨¡å‹åœ¨éè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚å¯ä»¥ç»“åˆè®ºæ–‡ä¸­æåˆ°çš„å‚è€ƒæ–‡çŒ®è¿›è¡Œæ·±å…¥é˜…è¯»ï¼Œå¹¶å…³æ³¨GitHubé“¾æ¥ä¸­çš„ä»£ç å®ç°ã€‚ç‰¹åˆ«å…³æ³¨è®ºæ–‡ä¸­å¯¹äºæœªæ¥ç ”ç©¶æ–¹å‘çš„è®¨è®ºï¼Œä»¥ä¾¿æ‰¾åˆ°æœ‰ä»·å€¼çš„ç ”ç©¶è¯¾é¢˜ã€‚

---

### 15. A Cheaper and Better Diffusion Language Model with Soft-Masked Noise

<div align="center">

**åŠ æƒæ€»åˆ†: 8.10/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Jiaao Chen, Aston Zhang, Mu Li, Alex Smola, Diyi Yang
- **URL**: [https://arxiv.org/abs/2304.04746](https://arxiv.org/abs/2304.04746)

#### æ‘˜è¦

Diffusion models that are based on iterative denoising have been recently proposed and leveraged in various generation tasks like image generation. Whereas, as a way inherently built for continuous data, existing diffusion models still have some limitations in modeling discrete data, e.g., languages...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡é’ˆå¯¹DLMåœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„å±€é™æ€§æå‡ºäº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œé‡ç‚¹å…³æ³¨é™ä½è®­ç»ƒæˆæœ¬å’Œæé«˜ç”Ÿæˆè´¨é‡ï¼Œå…·æœ‰ä¸€å®šçš„å®ç”¨ä»·å€¼ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¿¡æ¯æœ‰é™ï¼Œæ— æ³•å…¨é¢è¯„ä¼°è®ºæ–‡çš„æŠ€æœ¯æ·±åº¦ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½äº†è§£å…·ä½“å®ç°ç»†èŠ‚å’Œå®éªŒç»“æœã€‚å¦å¤–ï¼Œæ‘˜è¦ä¸­æ²¡æœ‰æåŠå…·ä½“çš„soft-maskingç­–ç•¥ï¼Œä»¥åŠå…¶èƒŒåçš„ç†è®ºä¾æ®ï¼Œè¿™äº›éƒ½éœ€è¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œè¯¦ç»†çš„è¯´æ˜ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š(1) soft-maskingç­–ç•¥çš„å…·ä½“å®ç°æ–¹å¼å’Œç†è®ºä¾æ®ï¼›(2) å®éªŒè®¾ç½®å’Œç»“æœï¼ŒéªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼›(3) ä¸å…¶ä»–DLMæ–¹æ³•çš„è¯¦ç»†å¯¹æ¯”åˆ†æï¼Œçªå‡ºMasked-Diffuse LMçš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚å¦‚æœå¯¹DLMåœ¨æ–‡æœ¬ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨æ„Ÿå…´è¶£ï¼Œè¿™ç¯‡è®ºæ–‡å€¼å¾—æ·±å…¥ç ”ç©¶ã€‚

---

### 16. GlyphDiffusion: Text Generation as Image Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 7.80/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Junyi Li, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen
- **URL**: [https://arxiv.org/abs/2304.12519](https://arxiv.org/abs/2304.12519)

#### æ‘˜è¦

Diffusion models have become a new generative paradigm for text generation. Considering the discrete categorical nature of text, in this paper, we propose GlyphDiffusion, a novel diffusion approach for text generation via text-guided image generation. Our key idea is to render the target text as a g...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ GlyphDiffusionï¼Œå°†æ–‡æœ¬ç”Ÿæˆè½¬åŒ–ä¸ºå›¾åƒç”Ÿæˆé—®é¢˜ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨æ¡ä»¶æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ã€‚è¿™ä¸ªæ€è·¯æœ‰å¯èƒ½ç®€åŒ–æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶å¸¦æ¥æ–°çš„æ€§èƒ½æå‡ã€‚
- **ç¼ºç‚¹**: ä¾èµ–å›¾åƒæ¸²æŸ“å’Œè§£ç è¿‡ç¨‹å¯èƒ½ä¼šå¼•å…¥é¢å¤–çš„è¯¯å·®ï¼Œå½±å“ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚è¿™ç§æ–¹æ³•çš„æ•ˆç‡å¯èƒ½ä½äºç›´æ¥çš„æ–‡æœ¬æ‰©æ•£æ¨¡å‹ï¼Œå› ä¸ºæ¶‰åŠå›¾åƒçš„ç”Ÿæˆå’Œå¤„ç†ã€‚ Text grounding moduleçš„è®¾è®¡å’Œä¼˜åŒ–éœ€è¦ä»”ç»†è€ƒè™‘ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»è®ºæ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºText Groundingæ¨¡å—çš„ç»†èŠ‚å’Œå®éªŒè®¾ç½®ã€‚ å…³æ³¨å®éªŒç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œä»¥åŠä¸å…¶ä»–DLMæ–¹æ³•çš„è¯¦ç»†å¯¹æ¯”ã€‚ é‡ç‚¹å…³æ³¨æ–¹æ³•æ˜¯å¦èƒ½è§£å†³ç°æœ‰DLMçš„æŸäº›é—®é¢˜ï¼Œæˆ–è€…åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

---

### 17. Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 8.00/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Kun Zhou, Yifan Li, Wayne Xin Zhao, Ji-Rong Wen
- **URL**: [https://arxiv.org/abs/2305.04044](https://arxiv.org/abs/2305.04044)

#### æ‘˜è¦

Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: å°†ç¦»æ•£æ‰©æ•£æ¨¡å‹å¼•å…¥éè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆï¼Œå¹¶ä¸BARTç­‰é¢„è®­ç»ƒæ¨¡å‹ç»“åˆï¼Œæå‡ºäº†è¿­ä»£è‡ªæç¤ºç­–ç•¥ï¼Œå®éªŒç»“æœæ˜¾ç¤ºä¼˜äºç°æœ‰NARæ–¹æ³•ç”šè‡³ARæ–¹æ³•ã€‚è€ƒè™‘äº†æ–‡æœ¬æ•°æ®çš„ç¦»æ•£ç‰¹æ€§ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­æŠ€æœ¯ç»†èŠ‚æè¿°ä¸å¤Ÿè¯¦ç»†ï¼Œç†è®ºåˆ†æçš„æ·±åº¦éœ€è¦åœ¨é˜…è¯»å…¨æ–‡åæ‰èƒ½è¯„ä¼°ã€‚æœªæåŠæ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦ï¼Œéè‡ªå›å½’æ¨¡å‹çš„åŠ é€Ÿæ•ˆæœå¯èƒ½å› ä¸ºè¿­ä»£å»å™ªè¿‡ç¨‹è€Œé™ä½ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨æŠ€æœ¯ç»†èŠ‚çš„å®ç°ï¼Œä»¥åŠå®éªŒéƒ¨åˆ†çš„è®¾ç½®å’Œåˆ†æã€‚å¯ä»¥å…³æ³¨è¯¥æ–¹æ³•åœ¨ä¸åŒç±»å‹æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä»¥åŠè®¡ç®—å¤æ‚åº¦ã€‚å¯ä»¥é‡ç‚¹å…³æ³¨è‡ªæç¤ºç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚

---

### 18. TESS: Text-to-Text Self-Conditioned Simplex Diffusion

<div align="center">

**åŠ æƒæ€»åˆ†: 8.10/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Rabeeh Karimi Mahabadi, Hamish Ivison, Jaesung Tae, James Henderson, Iz Beltagy, Matthew E. Peters, Arman Cohan
- **URL**: [https://arxiv.org/abs/2305.08379](https://arxiv.org/abs/2305.08379)

#### æ‘˜è¦

Diffusion models have emerged as a powerful paradigm for generation, obtaining strong performance in various continuous domains. However, applying continuous diffusion models to natural language remains challenging due to its discrete nature and the need for a large number of diffusion steps to gene...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„éè‡ªå›å½’æ‰©æ•£è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œåœ¨å¤šä¸ªNLPä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œå¹¶ä¸”å…¬å¼€äº†ä»£ç ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…å¤ç°å’Œæ”¹è¿›ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­æ²¡æœ‰è¯¦ç»†è¯´æ˜æŠ€æœ¯å®ç°çš„ç»†èŠ‚å’Œæ•°å­¦æ¨å¯¼ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ·±å…¥äº†è§£ã€‚å¦å¤–ï¼Œè™½ç„¶å‡å°‘äº†æ‰©æ•£æ­¥éª¤ï¼Œä½†æ•´ä½“è®¡ç®—æ•ˆç‡ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹ç›¸æ¯”å¦‚ä½•ï¼Œéœ€è¦åœ¨å®éªŒéƒ¨åˆ†è¿›è¡Œæ›´è¯¦ç»†çš„åˆ†æå’Œæ¯”è¾ƒã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯æ–¹æ³•éƒ¨åˆ†å’Œå®éªŒéƒ¨åˆ†ï¼Œä»¥äº†è§£TESSçš„å…·ä½“å®ç°ç»†èŠ‚å’Œæ€§èƒ½è¡¨ç°ã€‚é‡ç‚¹å…³æ³¨logit simplexç©ºé—´çš„åº”ç”¨ä»¥åŠè‡ªæ¡ä»¶æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚å¯ä»¥å°è¯•å¤ç°è®ºæ–‡ä¸­çš„ç»“æœï¼Œå¹¶åœ¨å…¶ä»–ä»»åŠ¡ä¸Šè¿›è¡ŒéªŒè¯ï¼Œä»¥è¿›ä¸€æ­¥è¯„ä¼°TESSçš„æ³›åŒ–èƒ½åŠ›ã€‚

---

### 19. AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Tong Wu, Zhihao Fan, Xiao Liu, Yeyun Gong, Yelong Shen, Jian Jiao, Hai-Tao Zheng, Juntao Li, Zhongyu Wei, Jian Guo, Nan Duan, Weizhu Chen
- **URL**: [https://arxiv.org/abs/2305.09515](https://arxiv.org/abs/2305.09515)

#### æ‘˜è¦

Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced seq...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„AR-Diffusionæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆä¸­ç¼ºä¹åºåˆ—ä¾èµ–æ€§çš„é—®é¢˜ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´denoising stepsï¼Œå®ç°äº†è‡ªå›å½’ç‰¹æ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼ŒåŒæ—¶æé«˜äº†ç”Ÿæˆé€Ÿåº¦ã€‚ä»£ç å¼€æºä¾¿äºå¤ç°å’Œæ‰©å±•ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­å¯¹æŠ€æœ¯ç»†èŠ‚çš„æè¿°ä¸å¤Ÿå……åˆ†ï¼Œç†è®ºåˆ†æçš„æ·±åº¦éœ€è¦åœ¨é˜…è¯»å…¨æ–‡åæ‰èƒ½åˆ¤æ–­ã€‚æ‘˜è¦ä¸­å£°æ˜çš„åŠ é€Ÿå€æ•° (100x ~ 600x) è¿‡äºå¤¸å¼ ï¼Œéœ€è¦è°¨æ…å¯¹å¾…ï¼Œå¯èƒ½éœ€è¦åœ¨ç‰¹å®šæ¡ä»¶ä¸‹æ‰èƒ½å®ç°ã€‚å¯¹ä¸åŒä»»åŠ¡çš„é€‚ç”¨æ€§å¯èƒ½å­˜åœ¨å·®å¼‚ï¼Œéœ€è¦åœ¨æ›´å¤šæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨AR-Diffusionçš„å…·ä½“å®ç°ç»†èŠ‚ï¼Œä»¥åŠå®éªŒéƒ¨åˆ†çš„è®¾ç½®å’Œç»“æœåˆ†æã€‚éœ€è¦å…³æ³¨åœ¨ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†ä¸ŠAR-Diffusionçš„æ€§èƒ½è¡¨ç°ï¼Œä»¥åŠåŠ é€Ÿæ•ˆæœçš„å¯é æ€§ã€‚å¯ä»¥å°è¯•å¤ç°è®ºæ–‡ç»“æœï¼Œå¹¶è¿›ä¸€æ­¥æ¢ç´¢AR-Diffusionåœ¨å…¶ä»–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šçš„åº”ç”¨ã€‚

---

### 20. Diffusion Language Models Generation Can Be Halted Early

<div align="center">

**åŠ æƒæ€»åˆ†: 7.90/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Sofia Maria Lo Cicero Vaina, Nikita Balagansky, Daniil Gavrilov
- **URL**: [https://arxiv.org/abs/2305.10818](https://arxiv.org/abs/2305.10818)

#### æ‘˜è¦

Diffusion Language models (DLMs) are a promising avenue for text generation due to their practical properties on tractable controllable generation. They also have the advantage of not having to predict text autoregressively. However, despite these notable features, DLMs have not yet reached the perf...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 6/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡èšç„¦äºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„ä¸€ä¸ªé‡è¦é—®é¢˜ï¼šç”Ÿæˆé€Ÿåº¦æ…¢ã€‚æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡æå‰åœæ­¢ç”Ÿæˆæ¥åŠ é€ŸDLMçš„ç”Ÿæˆï¼Œå®éªŒç»“æœè¡¨æ˜æœ‰æ•ˆï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨ä»·å€¼ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹æ–¹æ³•çš„å…·ä½“æè¿°ï¼Œæ— æ³•åˆ¤æ–­å…¶åŸç†å’Œå®ç°ç»†èŠ‚ã€‚éœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½è¿›è¡Œæ›´æ·±å…¥çš„è¯„ä¼°ã€‚å¦å¤–ï¼Œç›®å‰åªèƒ½çœ‹åˆ°å®éªŒåœ¨Plaid, SSD, å’Œ CDCD DLMs ä¸Šè¿›è¡Œï¼Œæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½æœªçŸ¥ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨è¯¥æ–¹æ³•å¦‚ä½•ä¼°è®¡DLMsç”Ÿæˆæ–‡æœ¬çš„å®Œæ•´æ€§ï¼Œä»¥åŠå®éªŒè®¾ç½®å’Œç»“æœçš„è¯¦ç»†åˆ†æã€‚è¿›ä¸€æ­¥ç ”ç©¶å¯ä»¥æ¢ç´¢è¯¥æ–¹æ³•åœ¨ä¸åŒDLMæ¶æ„ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠä¸å…¶ä»–åŠ é€Ÿæ–¹æ³•çš„ç»“åˆã€‚

---

### 21. David helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion LMs

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, Marjan Ghazvininejad
- **URL**: [https://arxiv.org/abs/2305.14771](https://arxiv.org/abs/2305.14771)

#### æ‘˜è¦

Diffusion-based language models are emerging as a promising alternative to autoregressive LMs: they approach the competence of autoregressive LMs while offering nuanced controllability at inference time. While autoregressive LMs have benefited immensely from scaling and instruction-based learning, e...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¨ç†æ—¶æ¨¡å‹é›†æˆæ¡†æ¶ï¼ˆSSD-2ï¼‰ï¼Œå…è®¸å°å‹ä¸“ç”¨æ¨¡å‹ä¸å¤§å‹é€šç”¨æ¨¡å‹ååŒå·¥ä½œï¼Œå…·æœ‰å®šåˆ¶åŒ–å’Œéƒ¨ç½²çš„ä¼˜åŠ¿ã€‚ç ”ç©¶è¿˜å…³æ³¨äº†æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è§„æ¨¡æ‰©å±•å’Œæ•ˆç‡æå‡ï¼Œè¿™å¯¹äºDLMçš„å®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¿¡æ¯æœ‰é™ï¼Œæ— æ³•è¯„ä¼°æŠ€æœ¯å®ç°çš„ç»†èŠ‚ï¼Œå¦‚å…·ä½“çš„è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–æ–¹æ³•ã€æ¨¡å‹æ¶æ„çš„å…·ä½“æ”¹è¿›ï¼Œä»¥åŠå®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡ã€‚è®ºæ–‡çš„å®é™…æ•ˆæœå’Œæ³›åŒ–èƒ½åŠ›éœ€è¦åœ¨é˜…è¯»å…¨æ–‡åæ‰èƒ½ç¡®å®šã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ä»¥æ·±å…¥äº†è§£SSD-2çš„å…·ä½“å®ç°ç»†èŠ‚ã€å®éªŒç»“æœå’Œæ€§èƒ½è¡¨ç°ã€‚å…³æ³¨æ¨¡å‹é›†æˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€è®­ç»ƒå’Œæ¨ç†æ•ˆç‡çš„æ”¹è¿›ï¼Œä»¥åŠåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå¯ä»¥å…³æ³¨è¯¥æ–¹æ³•ä¸å…¶ä»–æ¨¡å‹é›†æˆç­–ç•¥çš„æ¯”è¾ƒã€‚

---

### 22. Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 7.60/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Tianyu Yang, Thy Thy Tran, Iryna Gurevych
- **URL**: [https://arxiv.org/abs/2305.15025](https://arxiv.org/abs/2305.15025)

#### æ‘˜è¦

Current variational dialog models have employed pre-trained language models (PLMs) to parameterize the likelihood and posterior distributions. However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated respons...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡å°†æ‰©æ•£æ¨¡å‹æˆåŠŸåº”ç”¨äºå¯¹è¯ç”Ÿæˆä»»åŠ¡ï¼Œè§£å†³äº†ä¼ ç»ŸCVAEçš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†memory dropoutæœºåˆ¶ã€‚å¼€æºä»£ç æ–¹ä¾¿äº†ç ”ç©¶çš„å¤ç°å’Œåº”ç”¨ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­å¯¹å®éªŒç»†èŠ‚çš„æè¿°è¾ƒå°‘ï¼Œä¾‹å¦‚å…·ä½“çš„è¯„ä»·æŒ‡æ ‡å’Œå®éªŒè®¾ç½®ã€‚ç¼ºä¹ä¸å…¶ä»–ç›¸å…³å·¥ä½œçš„æ·±å…¥æ¯”è¾ƒï¼Œä»¥åŠå¯¹æ‰©æ•£æ¨¡å‹é€‰æ‹©çš„ç†è®ºä¾æ®çš„è®¨è®ºã€‚
- **é˜…è¯»å»ºè®®**: å¯ä»¥é˜…è¯»è¯¥è®ºæ–‡äº†è§£å¦‚ä½•å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºå¯¹è¯ç”Ÿæˆä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨è§£å†³CVAEçš„åéªŒå´©å¡Œé—®é¢˜ä¸Šã€‚å¦‚æœå¯¹ä»£ç å®ç°æ„Ÿå…´è¶£ï¼Œå¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¼€æºä»£ç ã€‚åœ¨é˜…è¯»æ—¶ï¼Œéœ€è¦å…³æ³¨å®éªŒéƒ¨åˆ†çš„è¯¦ç»†æè¿°ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£è¯¥æ–¹æ³•çš„æ€§èƒ½ã€‚

---

### 23. Likelihood-Based Diffusion Language Models

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Ishaan Gulrajani, Tatsunori B. Hashimoto
- **URL**: [https://arxiv.org/abs/2305.18619](https://arxiv.org/abs/2305.18619)

#### æ‘˜è¦

Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡ç›´æ¥è§£å†³äº†æ‰©æ•£è¯­è¨€æ¨¡å‹é¢†åŸŸçš„ä¸€ä¸ªé‡è¦é—®é¢˜ï¼Œå³å¦‚ä½•æé«˜æ‰©æ•£æ¨¡å‹çš„ä¼¼ç„¶æ€§ä»¥ä¸è‡ªå›å½’æ¨¡å‹ç«äº‰ã€‚ å®ƒé€šè¿‡æ–¹æ³•æ”¹è¿›ã€è§„æ¨¡æ‰©å±•å’Œè®¡ç®—ä¼˜åŒ–æˆåŠŸè®­ç»ƒäº†ä¸€ä¸ªæ€§èƒ½ä¼˜äºGPT-2çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å‘å¸ƒäº†è¯¥æ¨¡å‹ã€‚è¿™ä¸ºåç»­ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹ç®—æ³•æ”¹è¿›çš„å…·ä½“æè¿°å’ŒæŠ€æœ¯ç»†èŠ‚ï¼Œå¯èƒ½éœ€è¦åœ¨æ­£æ–‡ä¸­è¿›ä¸€æ­¥éªŒè¯ã€‚ å¦å¤–ï¼Œæ‘˜è¦ä¸»è¦å…³æ³¨ä¼¼ç„¶æ€§æŒ‡æ ‡ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è€ƒå¯Ÿå…¶ä»–ç”Ÿæˆè´¨é‡æŒ‡æ ‡ï¼Œæ¯”å¦‚å¤šæ ·æ€§ã€ç›¸å¹²æ€§ç­‰ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡æ­£æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºç®—æ³•æ”¹è¿›å’Œç¼©æ”¾è§„å¾‹çš„éƒ¨åˆ†ã€‚å…³æ³¨å®éªŒè®¾ç½®å’Œç»“æœåˆ†æï¼Œäº†è§£Plaid 1Bæ¨¡å‹çš„å…·ä½“æ€§èƒ½æŒ‡æ ‡å’Œå±€é™æ€§ã€‚å¯¹äºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„ç ”ç©¶è€…ï¼Œè¿™æ˜¯ä¸€ç¯‡å€¼å¾—é˜…è¯»çš„è®ºæ–‡ã€‚

---

### 24. Fine-grained Text Style Transfer with Diffusion-Based Language Models

<div align="center">

**åŠ æƒæ€»åˆ†: 8.10/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Yiwei Lyu, Tiange Luo, Jiacheng Shi, Todd C. Hollon, Honglak Lee
- **URL**: [https://arxiv.org/abs/2305.19512](https://arxiv.org/abs/2305.19512)

#### æ‘˜è¦

Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowl...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡èšç„¦äºä¸€ä¸ªæœ‰æ„ä¹‰çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåœ¨ä½èµ„æºè®¾ç½®ä¸‹ä¼˜äºç°æœ‰æ–¹æ³•çš„æ–°æ–¹æ³•ã€‚ å®ƒä¸æ‰©æ•£è¯­è¨€æ¨¡å‹é¢†åŸŸé«˜åº¦ç›¸å…³ï¼Œå¹¶ä¸”åœ¨é£æ ¼è¿ç§»ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦æä¾›çš„ä¿¡æ¯æœ‰é™ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ›´å…¨é¢åœ°è¯„ä¼°å…¶æŠ€æœ¯ç»†èŠ‚ã€å®éªŒè®¾ç½®çš„ä¸¥è°¨æ€§ä»¥åŠç»“æœçš„å¯é æ€§ã€‚æ‘˜è¦ä¸­å¹¶æœªè¯¦ç»†æè¿°æ‰€ä½¿ç”¨çš„æ–¹æ³•ï¼Œè¿™å°†é™åˆ¶æˆ‘ä»¬å¯¹å…¶å®é™…åˆ›æ–°æ°´å¹³çš„è¯„ä¼°ã€‚æ¨¡å‹ç»“æ„å’Œè®­ç»ƒç»†èŠ‚ç­‰æŠ€æœ¯ç»†èŠ‚åœ¨æ‘˜è¦ä¸­æ²¡æœ‰æåŠï¼Œè¿™ä½¿å¾—ç¡®å®šç ”ç©¶çš„æ·±åº¦å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯æ–¹æ³•éƒ¨åˆ†å’Œå®éªŒéƒ¨åˆ†ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹çš„å…·ä½“å®ç°ã€å®éªŒè®¾ç½®çš„åˆç†æ€§ä»¥åŠç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚æ­¤å¤–ï¼Œéœ€è¦å…³æ³¨è®ºæ–‡å¯¹äºæ¨¡å‹åœ¨ä½èµ„æºæƒ…å†µä¸‹è¡¨ç°è‰¯å¥½çš„åŸå› çš„åˆ†æã€‚

---

### 25. DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 7.60/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Guanqun Bi, Lei Shen, Yanan Cao, Meng Chen, Yuqiang Xie, Zheng Lin, Xiaodong He
- **URL**: [https://arxiv.org/abs/2306.01657](https://arxiv.org/abs/2306.01657)

#### æ‘˜è¦

Empathy is a crucial factor in open-domain conversations, which naturally shows one's caring and understanding to others. Though several methods have been proposed to generate empathetic responses, existing works often lead to monotonous empathy that refers to generic and safe expressions. In this p...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: 1. æ˜ç¡®èšç„¦äºæ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿå¯¹è¯ç”Ÿæˆä¸Šçš„åº”ç”¨ã€‚
2. æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šç²’åº¦æ§åˆ¶æƒ…æ„Ÿå¯¹è¯ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹æ¡†æ¶ã€‚
3. å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¯æ§æ€§ã€ä¿¡æ¯æ€§å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚
- **ç¼ºç‚¹**: 1. æ‘˜è¦ä¸­æ²¡æœ‰æåŠå…·ä½“çš„æ¨¡å‹ç»†èŠ‚å’Œå®ç°æ–¹æ³•ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½äº†è§£ã€‚
2. å…·ä½“æŠ€æœ¯æ·±åº¦å’Œæ•°å­¦æ¨å¯¼çš„ä¸¥è°¨æ€§éœ€è¦è¿›ä¸€æ­¥é˜…è¯»è®ºæ–‡è¿›è¡Œè¯„ä¼°ã€‚
- **é˜…è¯»å»ºè®®**: å¦‚æœå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿå¯¹è¯ç”Ÿæˆä»»åŠ¡ä¸Šçš„åº”ç”¨æ„Ÿå…´è¶£ï¼Œå¯ä»¥æ·±å…¥é˜…è¯»è¯¥è®ºæ–‡ã€‚é‡ç‚¹å…³æ³¨å¤šç²’åº¦æ§åˆ¶ä¿¡å·çš„è®¾è®¡ã€maskingç­–ç•¥çš„å®ç°ç»†èŠ‚ä»¥åŠå®éªŒç»“æœçš„åˆ†æã€‚åç»­ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´å¤šæœ‰æ•ˆçš„æ§åˆ¶ä¿¡å·å’Œæ›´é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹æ¶æ„ã€‚

---

### 26. PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model

<div align="center">

**åŠ æƒæ€»åˆ†: 7.90/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Josh Susskind, Navdeep Jaitly
- **URL**: [https://arxiv.org/abs/2306.02531](https://arxiv.org/abs/2306.02531)

#### æ‘˜è¦

Autoregressive models for text sometimes generate repetitive and low-quality output because errors accumulate during the steps of generation. This issue is often attributed to exposure bias - the difference between how a model is trained, and how it is used during inference. Denoising diffusion mode...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: ç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œè§£å†³äº†é•¿æ–‡æœ¬ç”Ÿæˆä¸­è´¨é‡å’Œæ•ˆç‡çš„é—®é¢˜ã€‚æå‡ºäº†PLANNERæ¨¡å‹ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨ä¸åŒç”Ÿæˆä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­æ²¡æœ‰è¯¦ç»†çš„æŠ€æœ¯ç»†èŠ‚å’Œå®éªŒè®¾ç½®ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°å…¶æŠ€æœ¯æ·±åº¦å’Œå®éªŒçš„ä¸¥è°¨æ€§ã€‚éœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½äº†è§£æ¨¡å‹çš„å…·ä½“å®ç°å’Œæ€§èƒ½ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨PLANNERæ¨¡å‹çš„å…·ä½“å®ç°ç»†èŠ‚ã€latent diffusionå’Œcoarse-to-fineç”Ÿæˆæ–¹å¼çš„å…·ä½“åšæ³•ã€å®éªŒè®¾ç½®å’Œç»“æœåˆ†æã€‚å¯¹æ¯”PLANNERæ¨¡å‹ä¸å…¶ä»–æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ï¼Œè¯„ä¼°å…¶ä¼˜åŠ¿å’Œä¸è¶³ã€‚å¦‚æœå¯¹æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„åº”ç”¨æ„Ÿå…´è¶£ï¼Œè¿™æ˜¯ä¸€ç¯‡å€¼å¾—é˜…è¯»çš„è®ºæ–‡ã€‚

---

### 27. StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models

<div align="center">

**åŠ æƒæ€»åˆ†: 7.80/10** | **ä¸»é¢˜ç›¸å…³æ€§: 8/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Yinghao Aaron Li, Cong Han, Vinay S. Raghavan, Gavin Mischler, Nima Mesgarani
- **URL**: [https://arxiv.org/abs/2306.07691](https://arxiv.org/abs/2306.07691)

#### æ‘˜è¦

In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through dif...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 8/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œå¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—è®­ç»ƒï¼Œåœ¨TTSé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå®ç°äº†äººç±»æ°´å¹³çš„è¯­éŸ³åˆæˆã€‚æå‡ºçš„StyleTTS 2æ–¹æ³•å’Œå¯å¾®åˆ†çš„æ—¶é•¿å»ºæ¨¡å…·æœ‰è¾ƒé«˜çš„åˆ›æ–°æ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦æ²¡æœ‰è¯¦ç»†è¯´æ˜å…·ä½“çš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç»†èŠ‚ï¼Œæ— æ³•åˆ¤æ–­å…¶æŠ€æœ¯å®ç°çš„å…·ä½“ç»†èŠ‚ã€‚ä¸»é¢˜ç›¸å…³æ€§ä¸Šï¼Œä¸»è¦è§£å†³çš„æ˜¯è¯­éŸ³é—®é¢˜ï¼Œå¯¹çº¯ç²¹çš„è¯­è¨€æ¨¡å‹çš„å½±å“å¯èƒ½æœ‰é™ã€‚ä¾èµ–å¤§å‹é¢„è®­ç»ƒSLMï¼Œè®¡ç®—æˆæœ¬å¯èƒ½è¾ƒé«˜ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨StyleTTS 2çš„æ¨¡å‹æ¶æ„ã€è®­ç»ƒæ–¹æ³•ã€å®éªŒè®¾ç½®å’Œç»“æœåˆ†æã€‚ç‰¹åˆ«æ˜¯éœ€è¦äº†è§£æ‰©æ•£æ¨¡å‹å¦‚ä½•ç”¨äºé£æ ¼å»ºæ¨¡ï¼Œä»¥åŠå¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹åœ¨å¯¹æŠ—è®­ç»ƒä¸­çš„ä½œç”¨ã€‚åŒæ—¶ï¼Œå¯ä»¥å…³æ³¨è¯¥æ¨¡å‹çš„è®¡ç®—æˆæœ¬å’Œæ³›åŒ–èƒ½åŠ›ã€‚

---

### 28. PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 7.80/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Zhiyuan Hu, Chumin Liu, Yue Feng, Anh Tuan Luu, Bryan Hooi
- **URL**: [https://arxiv.org/abs/2306.08456](https://arxiv.org/abs/2306.08456)

#### æ‘˜è¦

Controllable text generation is a challenging and meaningful field in natural language generation (NLG). Especially, poetry generation is a typical one with well-defined and strict conditions for text generation which is an ideal playground for the assessment of current methodologies. While prior wo...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡å°†Diffusionæ¨¡å‹åº”ç”¨äºè¯—æ­Œç”Ÿæˆï¼Œå¹¶æå‡ºäº†è”åˆæ§åˆ¶è¯­ä¹‰å’ŒéŸµå¾‹çš„æ–¹æ³•ï¼Œæ˜¯DLMåœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆé¢†åŸŸçš„ä¸€ä¸ªæœ‰è¶£åº”ç”¨ã€‚é€šè¿‡å¼•å…¥åº¦é‡æ§åˆ¶å™¨ï¼Œå®ç°äº†å¯¹è¯—æ­Œæ ¼å¼å’ŒéŸµå¾‹çš„çµæ´»æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­å¯¹æŠ€æœ¯ç»†èŠ‚çš„æè¿°ä¸å¤Ÿè¯¦ç»†ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½è¯„ä¼°ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯çš„æ·±åº¦ã€‚å®ç”¨ä»·å€¼ä¸»è¦ä½“ç°åœ¨è¯—æ­Œç”Ÿæˆé¢†åŸŸï¼Œåº”ç”¨åœºæ™¯ç›¸å¯¹æœ‰é™ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨åº¦é‡æ§åˆ¶å™¨çš„è®¾è®¡ã€Diffusionæ¨¡å‹çš„å…·ä½“å®ç°ä»¥åŠå®éªŒç»“æœçš„è¯¦ç»†åˆ†æã€‚å¯¹äºç ”ç©¶æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨å¯æ§æ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„åº”ç”¨ï¼Œä»¥åŠå¯¹è¯—æ­Œç”Ÿæˆæ„Ÿå…´è¶£çš„è¯»è€…ï¼Œè¯¥è®ºæ–‡å…·æœ‰ä¸€å®šçš„å‚è€ƒä»·å€¼ã€‚

---

### 29. DiffuDetox: A Mixed Diffusion Model for Text Detoxification

<div align="center">

**åŠ æƒæ€»åˆ†: 7.70/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Griffin Floto, Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Zhenwei Tang, Ali Pesaranghader, Manasa Bharadwaj, Scott Sanner
- **URL**: [https://arxiv.org/abs/2306.08505](https://arxiv.org/abs/2306.08505)

#### æ‘˜è¦

Text detoxification is a conditional text generation task aiming to remove offensive content from toxic text. It is highly useful for online forums and social media, where offensive content is frequently encountered. Intuitively, there are diverse ways to detoxify sentences while preserving their me...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡é’ˆå¯¹æ–‡æœ¬è§£æ¯’è¿™ä¸€å®é™…é—®é¢˜ï¼Œæå‡ºäº†åŸºäºæ··åˆæ‰©æ•£æ¨¡å‹çš„åˆ›æ–°æ–¹æ³•DiffuDetoxã€‚é€šè¿‡æ¡ä»¶æ¨¡å‹ä¿è¯ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§ï¼Œé€šè¿‡æ— æ¡ä»¶æ¨¡å‹å¢å¼ºæ–‡æœ¬çš„æµç•…æ€§ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•æœ‰æ•ˆï¼Œå…·æœ‰è¾ƒé«˜çš„å®ç”¨ä»·å€¼ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å…³äºæŠ€æœ¯ç»†èŠ‚çš„æ·±å…¥æè¿°ï¼Œæ¯”å¦‚æ— æ¡ä»¶æ¨¡å‹çš„å…·ä½“è®¾è®¡ã€æŸå¤±å‡½æ•°ã€è¶…å‚æ•°è®¾ç½®ç­‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶å½±å“åŠ›å¯èƒ½å—åˆ°æ¨¡å‹å¤æ‚åº¦å’Œæ³›åŒ–èƒ½åŠ›çš„é™åˆ¶ã€‚æ‘˜è¦ä¸­æ— æ³•åˆ¤æ–­æ•°å­¦æ¨å¯¼çš„æ·±åº¦ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡çš„å®éªŒéƒ¨åˆ†ï¼Œé‡ç‚¹å…³æ³¨DiffuDetoxçš„å…·ä½“å®ç°ç»†èŠ‚å’Œå‚æ•°è®¾ç½®ã€‚å¯ä»¥é‡ç‚¹å…³æ³¨æ— æ¡ä»¶æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•ä»¥åŠæ¡ä»¶å’Œæ— æ¡ä»¶æ¨¡å‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚å¦‚æœå¯¹æ–‡æœ¬è§£æ¯’æ„Ÿå…´è¶£ï¼Œå¯ä»¥å°è¯•å¤ç°è¯¥è®ºæ–‡çš„ç»“æœï¼Œå¹¶è¿›ä¸€æ­¥ç ”ç©¶å…¶åœ¨å…¶ä»–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

---

### 30. XDLM: Cross-lingual Diffusion Language Model for Machine Translation

<div align="center">

**åŠ æƒæ€»åˆ†: 7.50/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Linyao Chen, Aosong Feng, Boming Yang, Zihui Li
- **URL**: [https://arxiv.org/abs/2307.13560](https://arxiv.org/abs/2307.13560)

#### æ‘˜è¦

Recently, diffusion models have excelled in image generation tasks and have also been applied to neural language processing (NLP) for controllable text generation. However, the application of diffusion models in a cross-lingual setting is less unexplored. Additionally, while pretraining with diffusi...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 6/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºè·¨è¯­è¨€æœºå™¨ç¿»è¯‘ï¼Œæå‡ºäº†æ–°çš„è®­ç»ƒç›®æ ‡ï¼Œå¹¶å£°ç§°å–å¾—äº†æ¯”Transformeræ›´å¥½çš„ç»“æœï¼Œå…·æœ‰ä¸€å®šçš„åˆ›æ–°æ€§å’Œå®ç”¨ä»·å€¼ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºå°‘å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ï¼Œä¾‹å¦‚TLDMçš„ç»†èŠ‚ä»¥åŠæ¨¡å‹æ¶æ„ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ›´å‡†ç¡®è¯„ä¼°ã€‚Diffusionæ¨¡å‹æœ¬èº«çš„æ¨ç†é€Ÿåº¦ä¹Ÿæ˜¯ä¸€ä¸ªæ½œåœ¨çš„ç¼ºç‚¹ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨TLDMçš„å…·ä½“å®ç°æ–¹å¼ã€æ¨¡å‹çš„æ¶æ„ç»†èŠ‚ä»¥åŠå®éªŒç»“æœçš„å¯¹æ¯”ã€‚åŒæ—¶ï¼Œéœ€è¦å…³æ³¨è®ºæ–‡æ˜¯å¦è®¨è®ºäº†diffusionæ¨¡å‹æ¨ç†é€Ÿåº¦çš„é—®é¢˜ï¼Œä»¥åŠå¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

---

### 31. Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu
- **URL**: [https://arxiv.org/abs/2308.12219](https://arxiv.org/abs/2308.12219)

#### æ‘˜è¦

The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autor...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæ¢ç´¢äº†å¦‚ä½•å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºé€šç”¨è¯­è¨€ä»»åŠ¡ï¼Œå¹¶è¯æ˜äº†é€šè¿‡ç¼©æ”¾å’ŒæŒ‡ä»¤å¾®è°ƒå¯ä»¥æå‡å…¶æ€§èƒ½ã€‚ æŒ‡ä»¤å¾®è°ƒä½¿æ¨¡å‹å…·å¤‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬èƒ½åŠ›ï¼Œå¢åŠ äº†æ¨¡å‹çš„çµæ´»æ€§å’Œå®ç”¨æ€§ã€‚ è¿™é¡¹ç ”ç©¶ä¸ºæ‰©æ•£æ¨¡å‹åœ¨NLPé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹å…·ä½“æŠ€æœ¯ç»†èŠ‚çš„è¯¦ç»†æè¿°ï¼Œä¾‹å¦‚æ‰©æ•£é€‚åº”çš„å…·ä½“æ–¹æ³•ã€æŒ‡ä»¤å¾®è°ƒçš„ç­–ç•¥ä»¥åŠå®éªŒè®¾ç½®ç­‰ã€‚ è¿™ä½¿å¾—éš¾ä»¥è¯„ä¼°è¯¥æ–¹æ³•çš„å®é™…å¯è¡Œæ€§å’Œæ•ˆæœã€‚ æ‘˜è¦ä¹Ÿæ²¡æœ‰æåŠè®¡ç®—èµ„æºå’Œè®­ç»ƒæ—¶é—´ï¼Œè¿™å¯¹è¯„ä¼°å®ç”¨ä»·å€¼å¾ˆé‡è¦ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ä»¥äº†è§£æ‰©æ•£é€‚åº”å’ŒæŒ‡ä»¤å¾®è°ƒçš„å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œé‡ç‚¹å…³æ³¨å®éªŒéƒ¨åˆ†ä»¥è¯„ä¼°è¯¥æ–¹æ³•çš„å®é™…æ•ˆæœå’Œæ€§èƒ½ã€‚ å…³æ³¨æ¨¡å‹å¤§å°ã€è®­ç»ƒæˆæœ¬ç­‰å®é™…éƒ¨ç½²é—®é¢˜ã€‚

---

### 32. ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer

<div align="center">

**åŠ æƒæ€»åˆ†: 7.90/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Zachary Horvitz, Ajay Patel, Chris Callison-Burch, Zhou Yu, Kathleen McKeown
- **URL**: [https://arxiv.org/abs/2308.15459](https://arxiv.org/abs/2308.15459)

#### æ‘˜è¦

Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„é£æ ¼è¿ç§»æ¡†æ¶ï¼Œèƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒçš„ç›®æ ‡é£æ ¼ï¼Œå¹¶ä¸”å‚æ•°æ•ˆç‡è¾ƒé«˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé£æ ¼è¿ç§»ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦éƒ¨åˆ†æ²¡æœ‰è¯¦ç»†æè¿°æŠ€æœ¯ç»†èŠ‚å’Œç†è®ºåˆ†æï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ›´å¥½åœ°è¯„ä¼°å…¶æŠ€æœ¯æ·±åº¦ã€‚å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤šçš„å·¥ç¨‹å®è·µæ‰èƒ½è½åœ°ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œå°¤å…¶æ˜¯æŠ€æœ¯ç»†èŠ‚å’Œå®éªŒéƒ¨åˆ†ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°å…¶åˆ›æ–°æ€§å’ŒæŠ€æœ¯æ·±åº¦ã€‚å¦‚æœå¯¹æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨æ„Ÿå…´è¶£ï¼Œè¿™ç¯‡è®ºæ–‡å€¼å¾—æ·±å…¥ç ”ç©¶ã€‚

---

### 33. Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution

<div align="center">

**åŠ æƒæ€»åˆ†: 8.10/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Aaron Lou, Chenlin Meng, Stefano Ermon
- **URL**: [https://arxiv.org/abs/2310.16834](https://arxiv.org/abs/2310.16834)

#### æ‘˜è¦

Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete st...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨ç¦»æ•£æ–‡æœ¬æ•°æ®ä¸Šçš„åº”ç”¨ç“¶é¢ˆï¼Œæå‡ºäº†åˆ›æ–°çš„æŸå¤±å‡½æ•°Score Entropyï¼Œå¹¶åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚ç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å’Œå¯æ§æ€§æ–¹é¢ï¼Œå±•ç°å‡ºä¼˜äºä¼ ç»Ÿè‡ªå›å½’æ¨¡å‹çš„æ½œåŠ›ã€‚
- **ç¼ºç‚¹**: ä»…ä»æ‘˜è¦æ¥çœ‹ï¼Œè®ºæ–‡çš„æŠ€æœ¯æ·±åº¦å’Œç†è®ºåˆ†æçš„ä¸¥è°¨æ€§å°šä¸æ˜ç¡®ã€‚å¦å¤–ï¼Œè™½ç„¶æåˆ°äº†é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†å…·ä½“çš„å®éªŒæ•°æ®å’Œæ¯”è¾ƒæƒ…å†µè¿˜éœ€è¦è¿›ä¸€æ­¥è€ƒå¯Ÿã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå³åœ¨ä¸åŒæ•°æ®é›†å’Œä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä¹Ÿéœ€è¦éªŒè¯ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨Score Entropyçš„ç†è®ºæ¨å¯¼ã€å®éªŒè®¾ç½®å’Œç»“æœåˆ†æï¼Œä»¥åŠä¸ç°æœ‰æ–¹æ³•çš„è¯¦ç»†æ¯”è¾ƒã€‚ å¯ä»¥å…³æ³¨è®ºæ–‡çš„åç»­å·¥ä½œï¼Œçœ‹è¯¥æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†å’Œä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä»¥åŠåœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚

---

### 34. CodeFusion: A Pre-trained Diffusion Model for Code Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 8.10/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Mukul Singh, JosÃ© Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen
- **URL**: [https://arxiv.org/abs/2310.17680](https://arxiv.org/abs/2310.17680)

#### æ‘˜è¦

Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºä»£ç ç”Ÿæˆï¼Œè§£å†³è‡ªå›å½’æ¨¡å‹çš„å±€é™æ€§ï¼›åœ¨Bashã€Pythonå’ŒExcelå…¬å¼ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„ç»“æœï¼›å¼€æºï¼Œæ˜“äºå¤ç°å’Œæ‰©å±•ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ç¼ºä¹å¯¹æ‰©æ•£æ¨¡å‹å…·ä½“å®ç°ç»†èŠ‚çš„æè¿°ï¼›éœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ›´å…¨é¢åœ°è¯„ä¼°è®ºæ–‡çš„æŠ€æœ¯æ·±åº¦å’Œç†è®ºè´¡çŒ®ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºæ¨¡å‹æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œå®éªŒè®¾ç½®çš„ç»†èŠ‚ã€‚å…³æ³¨CodeFusionåœ¨ä¸åŒç±»å‹ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä»¥åŠå…¶ä¸å…¶ä»–æ¨¡å‹çš„æ¯”è¾ƒç»“æœã€‚åŒæ—¶ï¼Œå¯ä»¥å°è¯•å¤ç°è®ºæ–‡ç»“æœï¼Œå¹¶æ¢ç´¢CodeFusionåœ¨å®é™…é¡¹ç›®ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

---

### 35. Transfer Learning for Text Diffusion Models

<div align="center">

**åŠ æƒæ€»åˆ†: 7.80/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Kehang Han, Kathleen Kenealy, Aditya Barua, Noah Fiedel, Noah Constant
- **URL**: [https://arxiv.org/abs/2401.17181](https://arxiv.org/abs/2401.17181)

#### æ‘˜è¦

In this report, we explore the potential for text diffusion to replace autoregressive (AR) decoding for the training and deployment of large language models (LLMs). We are particularly interested to see whether pretrained AR models can be transformed into text diffusion models through a lightweight ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 6/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡ä¸»é¢˜æ˜ç¡®ï¼Œç´§å¯†å›´ç»•æ‰©æ•£è¯­è¨€æ¨¡å‹å±•å¼€ã€‚æå‡ºäº†AR2Diffè¿™ä¸€æ–°é¢–çš„è¿ç§»å­¦ä¹ æ€è·¯ï¼Œå¹¶åœ¨ä»£ç åˆæˆå’ŒQAç­‰ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜æ‰©æ•£æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥è¶…è¶Šè‡ªå›å½’æ¨¡å‹ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æœ‰ä»·å€¼çš„çº¿ç´¢ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹AR2Diffæ–¹æ³•çš„å…·ä½“ç»†èŠ‚æè¿°ï¼Œå¯¹æŠ€æœ¯æ·±åº¦çš„è¯„ä¼°å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚å¯¹diffusionæ¨¡å‹è®­ç»ƒå’Œæ”¶æ•›ç¨³å®šæ€§çš„è®¨è®ºå¯èƒ½ä¸å¤Ÿå……åˆ†ã€‚å½±å“åŠ›å¯èƒ½å—åˆ°åˆ›æ–°æ€§å’ŒæŠ€æœ¯æ·±åº¦çš„é™åˆ¶ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ›´å‡†ç¡®è¯„ä¼°ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®å®Œæ•´é˜…è¯»è¯¥è®ºæ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºAR2Diffçš„å…·ä½“å®ç°ç»†èŠ‚å’Œå®éªŒè®¾ç½®éƒ¨åˆ†ï¼Œä»¥æ›´å…¨é¢åœ°äº†è§£è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå±€é™æ€§ã€‚åŒæ—¶ï¼Œå…³æ³¨è®ºæ–‡ä¸­å…³äºæ‰©æ•£æ¨¡å‹è®­ç»ƒå’Œæ”¶æ•›ç¨³å®šæ€§çš„è®¨è®ºï¼Œä»¥åŠæœªæ¥ç ”ç©¶æ–¹å‘çš„å±•æœ›ã€‚å¦‚æœå¯¹æ‰©æ•£æ¨¡å‹åœ¨LLMä¸­çš„åº”ç”¨æ„Ÿå…´è¶£ï¼Œè¿™æ˜¯ä¸€ç¯‡å€¼å¾—é˜…è¯»çš„è®ºæ–‡ã€‚

---

### 36. Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, Lingpeng Kong
- **URL**: [https://arxiv.org/abs/2402.07754](https://arxiv.org/abs/2402.07754)

#### æ‘˜è¦

Recently, diffusion models have garnered significant interest in the field of text processing due to their many potential advantages compared to conventional autoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a novel approach that integrates diffusion models with Chain-of-Th...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: å°†æ‰©æ•£æ¨¡å‹ä¸é“¾å¼æ€è€ƒæ¨ç†ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ€è·¯ï¼Œæé«˜äº†DLMçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¶…è¿‡äº†æ›´å¤§çš„è‡ªå›å½’æ¨¡å‹ï¼Œå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚
- **ç¼ºç‚¹**: ä»…é€šè¿‡æ‘˜è¦æ— æ³•è¯„ä¼°æŠ€æœ¯çš„ç»†èŠ‚å’Œå®éªŒçš„å…¨é¢æ€§ã€‚è®ºæ–‡å¯èƒ½éœ€è¦è¿›ä¸€æ­¥å®Œå–„ï¼Œä¾‹å¦‚ï¼Œæä¾›æ›´æ·±å…¥çš„ç†è®ºåˆ†æå’Œæ›´å¤šçš„å®éªŒç»“æœï¼Œæ¥å……åˆ†è¯æ˜æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚åŒæ—¶ï¼ŒDLMç›¸å¯¹è‡ªå›å½’æ¨¡å‹ï¼Œinferenceçš„æ•ˆç‡å¯èƒ½ä¸å¦‚è‡ªå›å½’æ¨¡å‹ï¼Œè¿™éƒ¨åˆ†éœ€è¦åœ¨è®ºæ–‡ä¸­è¿›è¡Œæ›´è¯¦ç»†çš„åˆ†æå’Œè®¨è®ºã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®è¯¦ç»†é˜…è¯»è¯¥è®ºæ–‡ï¼Œç‰¹åˆ«å…³æ³¨å…¶æ‰©æ•£æ¨¡å‹ä¸é“¾å¼æ€è€ƒæ¨ç†çš„å…·ä½“å®ç°æ–¹å¼ã€å®éªŒè®¾è®¡å’Œç»“æœåˆ†æã€‚å¦‚æœå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹æ„Ÿå…´è¶£ï¼Œè¿™æ˜¯ä¸€ç¯‡å€¼å¾—æ·±å…¥ç ”ç©¶çš„è®ºæ–‡ã€‚å…³æ³¨å…¶inferenceæ•ˆç‡ï¼Œå¹¶æ€è€ƒå¦‚ä½•è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚

---

### 37. Text-Guided Molecule Generation with Diffusion Language Model

<div align="center">

**åŠ æƒæ€»åˆ†: 7.70/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Haisong Gong, Qiang Liu, Shu Wu, Liang Wang
- **URL**: [https://arxiv.org/abs/2402.13040](https://arxiv.org/abs/2402.13040)

#### æ‘˜è¦

Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with Diffusion Languag...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: å°†æ‰©æ•£è¯­è¨€æ¨¡å‹åº”ç”¨äºæ–‡æœ¬å¼•å¯¼çš„åˆ†å­ç”Ÿæˆä»»åŠ¡ï¼Œæå‡ºä¸¤é˜¶æ®µç”Ÿæˆè¿‡ç¨‹ï¼Œåœ¨åˆ†å­ç”Ÿæˆé¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œå¹¶å¼€æºäº†ä»£ç ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹æ‰©æ•£æ¨¡å‹å…·ä½“ç»†èŠ‚çš„æ·±å…¥æè¿°ï¼Œå¦‚æŸå¤±å‡½æ•°ã€é‡‡æ ·ç­–ç•¥ç­‰ã€‚ç ”ç©¶å½±å“åŠ›è¿˜éœ€è¦åç»­å®éªŒå’Œæ›´å¤šå·¥ä½œçš„éªŒè¯ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨ä¸¤é˜¶æ®µæ‰©æ•£è¿‡ç¨‹çš„ç»†èŠ‚ï¼Œå°¤å…¶æ˜¯å™ªå£°ä¼˜åŒ–å’Œæœ‰æ•ˆæ€§æ ¡æ­£çš„å…·ä½“å®ç°æ–¹å¼ã€‚åŒæ—¶ï¼Œå…³æ³¨å®éªŒè®¾ç½®å’Œç»“æœåˆ†æï¼Œåˆ¤æ–­æ¨¡å‹æ•ˆæœçš„çœŸå®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

---

### 38. Text Diffusion with Reinforced Conditioning

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang
- **URL**: [https://arxiv.org/abs/2402.14843](https://arxiv.org/abs/2402.14843)

#### æ‘˜è¦

Diffusion models have demonstrated exceptional capability in generating high-quality images, videos, and audio. Due to their adaptiveness in iterative refinement, they provide a strong potential for achieving better non-autoregressive sequence generation. However, existing text diffusion models stil...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡é’ˆå¯¹æ–‡æœ¬æ‰©æ•£æ¨¡å‹çš„ä¸¤ä¸ªå…³é”®é—®é¢˜æå‡ºäº†åˆ›æ–°æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ æ¨¡å‹çš„è®¾è®¡è€ƒè™‘äº†è®­ç»ƒå’Œé‡‡æ ·ä¹‹é—´çš„å·®å¼‚ï¼Œä½¿å¾—æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šèƒ½å¤Ÿæ›´å¥½åœ°å‘æŒ¥ä½œç”¨ã€‚
- **ç¼ºç‚¹**: ä»æ‘˜è¦æ¥çœ‹ï¼Œéœ€è¦æŸ¥çœ‹è®ºæ–‡æ­£æ–‡æ‰èƒ½ç¡®å®šå®éªŒéƒ¨åˆ†çš„å®Œæ•´æ€§ã€æ•°æ®é‡ä»¥åŠä¸å…¶ä»–åŸºçº¿æ¨¡å‹å¯¹æ¯”çš„å……åˆ†æ€§ã€‚æ­¤å¤–ï¼Œå¼ºåŒ–æ¡ä»¶å’Œæ—¶é—´æ„ŸçŸ¥æ–¹å·®ç¼©æ”¾çš„å…·ä½“å®ç°ç»†èŠ‚ä¹Ÿéœ€è¦åœ¨æ­£æ–‡ä¸­è¯¦ç»†è¯´æ˜ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨å®éªŒéƒ¨åˆ†çš„ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯ä¸ç°æœ‰æ–¹æ³•çš„æ¯”è¾ƒã€‚å¯ä»¥å…³æ³¨ä½œè€…æ˜¯å¦å…¬å¼€ä»£ç å’Œæ•°æ®é›†ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥éªŒè¯æ¨¡å‹çš„æ€§èƒ½ã€‚

---

### 39. TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings

<div align="center">

**åŠ æƒæ€»åˆ†: 7.90/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Alexander Shabalin, Viacheslav Meshchaninov, Egor Chimbulatov, Vladislav Lapikov, Roman Kim, Grigory Bartosh, Dmitry Molchanov, Sergey Markov, Dmitry Vetrov
- **URL**: [https://arxiv.org/abs/2402.19097](https://arxiv.org/abs/2402.19097)

#### æ‘˜è¦

This paper presents the Text Encoding Diffusion Model (TEncDM), a novel approach to diffusion modeling that operates in the space of pre-trained language model encodings. In contrast to traditionally used embeddings, encodings integrate contextual information. In our approach, we also employ a trans...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºè¯­è¨€æ¨¡å‹ç¼–ç ç©ºé—´ï¼Œå¹¶ç»“åˆtransformerè§£ç å™¨ï¼Œåœ¨æ¡ä»¶æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚ å®éªŒè®¾è®¡æ¯”è¾ƒå…¨é¢ï¼Œå¼€æºä»£ç æ–¹ä¾¿å¤ç°ã€‚
- **ç¼ºç‚¹**: ç†è®ºåˆ†æå¯èƒ½ä¸å¤Ÿæ·±å…¥ï¼Œä¾‹å¦‚å¯¹å™ªå£°è°ƒåº¦å™¨çš„é€‰æ‹©åŸå› ç¼ºå°‘æ›´æ·±å…¥çš„è§£é‡Šã€‚è™½ç„¶æœ‰encoderå’Œdecoderçš„ablation study, ä½†æ˜¯å¯¹decoderçš„è®¾è®¡å’Œencoderçš„é€‰å–èƒŒåçš„åŸå› é˜è¿°ä¸è¶³ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®æ·±å…¥é˜…è¯»è¯¥è®ºæ–‡ï¼Œå°¤å…¶å…³æ³¨TEncDMæ¨¡å‹çš„è®¾è®¡ç»†èŠ‚å’Œå®éªŒç»“æœã€‚ å¦‚æœå¯¹æ‰©æ•£æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹æ„Ÿå…´è¶£ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚åŒæ—¶ï¼Œå¯ä»¥å…³æ³¨è¯¥è®ºæ–‡çš„åç»­å¼•ç”¨æƒ…å†µï¼Œäº†è§£å…¶åœ¨é¢†åŸŸå†…çš„å½±å“ã€‚

---

### 40. Differentially Private Synthetic Data via Foundation Model APIs 2: Text

<div align="center">

**åŠ æƒæ€»åˆ†: 7.60/10** | **ä¸»é¢˜ç›¸å…³æ€§: 8/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Chulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, Da Yu, Huseyin A Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li, Sergey Yekhanin
- **URL**: [https://arxiv.org/abs/2403.01749](https://arxiv.org/abs/2403.01749)

#### æ‘˜è¦

Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 8/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡æå‡ºäº†Aug-PEç®—æ³•ï¼Œé€šè¿‡APIè®¿é—®LLMç”Ÿæˆå·®åˆ†éšç§ä¿æŠ¤çš„åˆæˆæ–‡æœ¬æ•°æ®ï¼Œé¿å…äº†æ˜‚è´µçš„DP finetuningï¼Œå…·æœ‰å®ç”¨ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚ä»£ç å’Œæ•°æ®å…¬å¼€ï¼Œä¾¿äºå¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºå°‘å¯¹Aug-PEç®—æ³•å…·ä½“ç»†èŠ‚çš„æè¿°ã€‚æŠ€æœ¯æ·±åº¦æ–¹é¢ï¼Œæ‘˜è¦ä¸­æ²¡æœ‰é€éœ²æ•°å­¦åŸç†å’Œå…¬å¼æ¨å¯¼ï¼Œå¯èƒ½éœ€è¦åœ¨æ­£æ–‡ä¸­å¯»æ‰¾ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºAug-PEç®—æ³•çš„ç»†èŠ‚å’Œå®éªŒéƒ¨åˆ†ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°å…¶æŠ€æœ¯æ·±åº¦å’Œæœ‰æ•ˆæ€§ã€‚å¯ä»¥å…³æ³¨ä½œè€…æä¾›çš„ä»£ç å’Œæ•°æ®ï¼Œå°è¯•å¤ç°å®éªŒç»“æœã€‚

---

### 41. Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows

<div align="center">

**åŠ æƒæ€»åˆ†: 7.80/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Shujian Zhang, Lemeng Wu, Chengyue Gong, Xingchao Liu
- **URL**: [https://arxiv.org/abs/2403.16995](https://arxiv.org/abs/2403.16995)

#### æ‘˜è¦

Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: æå‡ºäº†Language Rectified Flowï¼Œç»“åˆäº†æ¦‚ç‡æµå’Œæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ŒåŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ï¼Œå¹¶åœ¨æ§åˆ¶æ–‡æœ¬ç”Ÿæˆå’Œæ–‡æœ¬ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚è§£å†³æ‰©æ•£æ¨¡å‹æ¨ç†é€Ÿåº¦æ…¢çš„ç—›ç‚¹ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­æ²¡æœ‰è¯¦ç»†ä»‹ç»Language Rectified Flowçš„å…·ä½“å®ç°ç»†èŠ‚å’Œç†è®ºæ¨å¯¼ï¼Œå¯¹æŠ€æœ¯çš„æ·±åº¦ç†è§£éœ€è¦é˜…è¯»å…¨æ–‡ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºLanguage Rectified Flowçš„å®ç°ç»†èŠ‚å’ŒODEçš„å­¦ä¹ è¿‡ç¨‹ã€‚å…³æ³¨å®éªŒéƒ¨åˆ†çš„ç»†èŠ‚ï¼Œäº†è§£è¯¥æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚å¯ä»¥å…³æ³¨è¯¥è®ºæ–‡åç»­çš„å¼•ç”¨å’Œåº”ç”¨æƒ…å†µï¼Œè¯„ä¼°å…¶é•¿æœŸå½±å“åŠ›ã€‚

---

### 42. DiffusionDialog: A Diffusion Model for Diverse Dialog Generation with Latent Space

<div align="center">

**åŠ æƒæ€»åˆ†: 7.60/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Jianxiang Xiang, Zhenhua Liu, Haodong Liu, Yin Bai, Jia Cheng, Wenliang Chen
- **URL**: [https://arxiv.org/abs/2404.06760](https://arxiv.org/abs/2404.06760)

#### æ‘˜è¦

In real-life conversations, the content is diverse, and there exists the one-to-many problem that requires diverse generation. Previous studies attempted to introduce discrete or Gaussian-based continuous latent variables to address the one-to-many problem, but the diversity is limited. Recently, di...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: å°†æ‰©æ•£æ¨¡å‹å¼•å…¥å¯¹è¯ç”Ÿæˆé¢†åŸŸï¼Œå°è¯•è§£å†³å¯¹è¯ç”Ÿæˆå¤šæ ·æ€§çš„é—®é¢˜ï¼Œå¹¶å…³æ³¨inference efficiencyã€‚ç»“åˆEncoderå’ŒDiffusion Modelæ„å»ºlatent spaceï¼Œæ˜¯æ¯”è¾ƒæ–°é¢–çš„æ€è·¯ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¿¡æ¯æœ‰é™ï¼Œæ— æ³•åˆ¤æ–­æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœå’Œæ³›åŒ–èƒ½åŠ›ã€‚ éœ€è¦æ·±å…¥é˜…è¯»å…¨æ–‡ï¼Œè¯„ä¼°å®éªŒè®¾è®¡çš„åˆç†æ€§ã€ç»“æœçš„å¯é æ€§ä»¥åŠä¸å…¶ä»–æ–¹æ³•çš„å¯¹æ¯”æƒ…å†µã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹ç»“æ„çš„ç»†èŠ‚ã€å®éªŒè®¾ç½®å’Œç»“æœåˆ†æã€‚ å°¤å…¶éœ€è¦å…³æ³¨æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œä»¥åŠä¸ä¼ ç»Ÿå¯¹è¯ç”Ÿæˆæ¨¡å‹å’ŒåŸºäºGANçš„æ¨¡å‹çš„å¯¹æ¯”ç»“æœã€‚ å¦‚æœç»“æœå¯é ï¼Œä¸”inference efficiencyç¡®å®æœ‰ä¼˜åŠ¿ï¼Œåˆ™å€¼å¾—æ·±å…¥ç ”ç©¶ã€‚

---

### 43. Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data

<div align="center">

**åŠ æƒæ€»åˆ†: 8.30/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, Chongxuan Li
- **URL**: [https://arxiv.org/abs/2406.03736](https://arxiv.org/abs/2406.03736)

#### æ‘˜è¦

Discrete diffusion models with absorbing processes have shown promise in language modeling. The key quantities to be estimated are the ratios between the marginal probabilities of two transitive states at all timesteps, called the concrete score. In this paper, we reveal that the concrete score in a...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 8/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡ä¸»é¢˜æ˜ç¡®ï¼Œèšç„¦æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼›åˆ›æ–°æ€§å¼ºï¼Œæå‡ºäº†RADDæ¨¡å‹å’Œç»Ÿä¸€è§†è§’ï¼›å®ç”¨ä»·å€¼è¾ƒé«˜ï¼Œå¯åŠ é€Ÿé‡‡æ ·ï¼›æŠ€æœ¯æ·±åº¦è¾ƒå¥½ï¼Œæœ‰ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼›å¼€æºä»£ç æ–¹ä¾¿å¤ç°ã€‚
- **ç¼ºç‚¹**: è™½ç„¶å–å¾—SOTAï¼Œä½†å®éªŒè§„æ¨¡åŸºäºGPT-2ï¼Œéœ€è¦åœ¨æ›´å¤§è§„æ¨¡æ•°æ®é›†å’Œæ¨¡å‹ä¸Šè¿›ä¸€æ­¥éªŒè¯æ•ˆæœã€‚å¯¹RADDçš„ç†è®ºåˆ†æå¯ä»¥æ›´æ·±å…¥ï¼Œä¾‹å¦‚æ”¶æ•›æ€§è¯æ˜ç­‰ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»è€…é‡ç‚¹å…³æ³¨è®ºæ–‡æå‡ºçš„RADDæ¨¡å‹ï¼Œä»¥åŠå…¶å¯¹absorbing diffusionçš„åˆ†æå’Œç»Ÿä¸€è§†è§’ã€‚å¯ä»¥å°è¯•å¤ç°è®ºæ–‡ç»“æœï¼Œå¹¶æ¢ç´¢RADDåœ¨æ›´å¤§è§„æ¨¡æ•°æ®é›†å’Œæ¨¡å‹ä¸Šçš„è¡¨ç°ã€‚åŒæ—¶ï¼Œå¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶RADDçš„ç†è®ºæ€§è´¨ï¼Œå¦‚æ”¶æ•›æ€§ç­‰ã€‚

---

### 44. Simple and Effective Masked Diffusion Language Models

<div align="center">

**åŠ æƒæ€»åˆ†: 7.90/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T Chiu, Alexander Rush, Volodymyr Kuleshov
- **URL**: [https://arxiv.org/abs/2406.07524](https://arxiv.org/abs/2406.07524)

#### æ‘˜è¦

While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. In this work, we show that simple masked discrete diffusion is more performant than previously thought. We apply an...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡çš„ä¼˜ç‚¹åœ¨äºä¸»é¢˜æ˜ç¡®ï¼Œé’ˆå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº†æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•å’Œç®€åŒ–çš„ç›®æ ‡å‡½æ•°ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ åŒæ—¶ï¼Œè®ºæ–‡æä¾›äº†ä»£ç å’Œæ•™ç¨‹ï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…å¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚
- **ç¼ºç‚¹**: è®ºæ–‡çš„ç¼ºç‚¹å¯èƒ½åœ¨äºæŠ€æœ¯æ·±åº¦æ–¹é¢ï¼Œè™½ç„¶å¯¹masked diffusion modelsè¿›è¡Œäº†æ·±å…¥è®¨è®ºï¼Œä½†å¯èƒ½ç¼ºä¹æ›´æ·±å±‚æ¬¡çš„ç†è®ºè¯æ˜æˆ–æ›´å¹¿æ³›çš„å®éªŒéªŒè¯ã€‚æ­¤å¤–ï¼Œè™½ç„¶ç»“æœæ¥è¿‘ARæ¨¡å‹ï¼Œä½†æ˜¯æ²¡æœ‰å®Œå…¨è¶…è¶Šï¼Œåç»­éœ€è¦æ›´å¤šçªç ´ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®å¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹æ„Ÿå…´è¶£çš„ç ”ç©¶è€…é˜…è¯»è¯¥è®ºæ–‡ã€‚ç‰¹åˆ«å…³æ³¨è®ºæ–‡ä¸­æå‡ºçš„è®­ç»ƒæ–¹æ³•å’Œç®€åŒ–çš„ç›®æ ‡å‡½æ•°ï¼Œå¹¶å°è¯•å¤ç°å’Œè¿›ä¸€æ­¥æ”¹è¿›ã€‚åŒæ—¶ï¼Œå¯ä»¥ç»“åˆå…¶ä»–ç›¸å…³ç ”ç©¶ï¼Œæ¢ç´¢æ›´æœ‰æ•ˆçš„æ‰©æ•£è¯­è¨€æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ã€‚

---

### 45. Promises, Outlooks and Challenges of Diffusion Language Modeling

<div align="center">

**åŠ æƒæ€»åˆ†: 7.90/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Justin Deschenaux, Caglar Gulcehre
- **URL**: [https://arxiv.org/abs/2406.11473](https://arxiv.org/abs/2406.11473)

#### æ‘˜è¦

The modern autoregressive Large Language Models (LLMs) have achieved outstanding performance on NLP benchmarks, and they are deployed in the real world. However, they still suffer from limitations of the autoregressive training paradigm. For example, autoregressive token generation is notably slow a...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡èšç„¦äºæ‰©æ•£è¯­è¨€æ¨¡å‹è¿™ä¸€æ–°å…´é¢†åŸŸï¼Œå¯¹SEDDæ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°å’Œåˆ†æï¼ŒåŒ…æ‹¬å…¶ä¼˜åŠ¿ã€æŒ‘æˆ˜å’Œå±€é™æ€§ã€‚è®ºæ–‡é€šè¿‡å®éªŒè¯æ˜SEDDåœ¨æ¨ç†é€Ÿåº¦æ–¹é¢ä¼˜äºè‡ªå›å½’æ¨¡å‹ï¼Œå¹¶æŒ‡å‡ºäº†å…¶åœ¨çŸ­promptæ¡ä»¶ç”Ÿæˆæ–¹é¢çš„ä¸è¶³ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚
- **ç¼ºç‚¹**: è®ºæ–‡ä¸»è¦æ˜¯å¯¹ç°æœ‰SEDDæ¨¡å‹çš„è¯„ä¼°å’Œåˆ†æï¼Œåˆ›æ–°æ€§ç›¸å¯¹æœ‰é™ã€‚è™½ç„¶è¿›è¡Œäº†è¾ƒä¸ºæ·±å…¥çš„æŠ€æœ¯åˆ†æï¼Œä½†ç¼ºä¹å¯¹SEDDæ¨¡å‹æœ¬èº«æ›´æ·±å±‚æ¬¡çš„ç†è®ºæ¢è®¨å’Œæ”¹è¿›ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®å¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹çš„ç†è®ºåŸºç¡€è¿›è¡Œæ›´æ·±å…¥çš„ç ”ç©¶ï¼Œå¹¶å°è¯•æå‡ºæ–°çš„æ‰©æ•£è¯­è¨€æ¨¡å‹æ¶æ„æˆ–æ”¹è¿›ç°æœ‰æ¨¡å‹ï¼Œä¾‹å¦‚è§£å†³SEDDåœ¨çŸ­promptæ¡ä»¶ç”Ÿæˆæ–¹é¢çš„ä¸è¶³ã€‚å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨å…¶ä»–NLPä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä¾‹å¦‚æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰ã€‚

---

### 46. DiffuseDef: Improved Robustness to Adversarial Attacks

<div align="center">

**åŠ æƒæ€»åˆ†: 7.50/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Zhenhao Li, Marek Rei, Lucia Specia
- **URL**: [https://arxiv.org/abs/2407.00248](https://arxiv.org/abs/2407.00248)

#### æ‘˜è¦

Pretrained language models have significantly advanced performance across various natural language processing tasks. However, adversarial attacks continue to pose a critical challenge to system built using these models, as they can be exploited with carefully crafted adversarial texts. Inspired by t...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºå¯¹æŠ—æ”»å‡»é˜²å¾¡è¿™ä¸€é‡è¦é—®é¢˜ï¼Œæå‡ºäº†DiffuseDefæ–¹æ³•ï¼Œå¹¶ç»“åˆäº†å¯¹æŠ—è®­ç»ƒã€å»å™ªå’Œé›†æˆæŠ€æœ¯ã€‚è¿™ç§æ–¹æ³•å…·æœ‰ä¸€å®šçš„åˆ›æ–°æ€§ï¼Œå¹¶ä¸”å¦‚æœæœ‰æ•ˆï¼Œåˆ™å…·æœ‰è¾ƒé«˜çš„å®ç”¨ä»·å€¼ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹æ‰©æ•£æ¨¡å‹å…·ä½“å®ç°ç»†èŠ‚çš„æè¿°ã€‚ä¸ºäº†æ›´å…¨é¢åœ°è¯„ä¼°è®ºæ–‡çš„è´¨é‡ï¼Œéœ€è¦è¿›ä¸€æ­¥äº†è§£å®éªŒè®¾ç½®å’Œç»“æœï¼Œä»¥åˆ¤æ–­æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨æ‰©æ•£æ¨¡å‹çš„å…·ä½“å®ç°ç»†èŠ‚ï¼Œå¯¹æŠ—è®­ç»ƒå’Œé›†æˆç­–ç•¥ï¼Œä»¥åŠå®éªŒéªŒè¯éƒ¨åˆ†ï¼Œä»¥å…¨é¢è¯„ä¼°è¯¥è®ºæ–‡çš„è´¨é‡ã€‚å¯ä»¥å…³æ³¨å®éªŒç»“æœæ˜¯å¦è¶³å¤Ÿå…·æœ‰è¯´æœåŠ›ï¼Œä»¥åŠä¸å…¶ä»–ç°æœ‰æ–¹æ³•çš„æ¯”è¾ƒæƒ…å†µã€‚

---

### 47. Discrete Diffusion Language Model for Long Text Summarization

<div align="center">

**åŠ æƒæ€»åˆ†: 7.90/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Do Huu Dat, Do Duc Anh, Anh Tuan Luu, Wray Buntine
- **URL**: [https://arxiv.org/abs/2407.10998](https://arxiv.org/abs/2407.10998)

#### æ‘˜è¦

While diffusion models excel at conditional generating high-quality images, prior works in discrete diffusion models were not evaluated on conditional long-text generation. In this work, we address the limitations of prior discrete diffusion models for conditional long-text generation, particularly ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡é’ˆå¯¹é•¿æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ï¼Œæå‡ºäº†æ–°çš„è¯­ä¹‰æ„ŸçŸ¥å™ªå£°è¿‡ç¨‹å’ŒCrossMambaæ¨¡å‹ï¼Œè§£å†³äº†ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†SOTAæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒå¿«çš„æ¨ç†é€Ÿåº¦ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºå°‘æ›´å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚å’Œå®éªŒåˆ†æï¼Œæ¯”å¦‚CrossMambaçš„å…·ä½“å®ç°å’Œä¼˜åŠ¿ï¼Œä»¥åŠè¯­ä¹‰æ„ŸçŸ¥å™ªå£°è¿‡ç¨‹çš„åŸç†ç­‰ï¼Œå¯¹ç®—æ³•å’Œæ¨¡å‹çš„æè¿°è¾ƒä¸ºç¬¼ç»Ÿã€‚è®ºæ–‡çš„æŠ€æœ¯æ·±åº¦å’Œä¸¥è°¨æ€§éœ€è¦åœ¨æ­£æ–‡ä¸­è¿›ä¸€æ­¥éªŒè¯ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨è¯­ä¹‰æ„ŸçŸ¥å™ªå£°è¿‡ç¨‹å’ŒCrossMambaæ¨¡å‹çš„å…·ä½“å®ç°ï¼Œä»¥åŠå®éªŒéƒ¨åˆ†å¯¹æ¨¡å‹æ€§èƒ½çš„è¯¦ç»†åˆ†æã€‚ å¯ä»¥å…³æ³¨åç»­å·¥ä½œï¼Œçœ‹æ˜¯å¦èƒ½å¤Ÿè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ•ˆæœå’Œæ•ˆç‡ï¼Œå¹¶ä¸”åº”ç”¨åˆ°æ›´å¤šçš„é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ã€‚

---

### 48. Diffusion Guided Language Modeling

<div align="center">

**åŠ æƒæ€»åˆ†: 8.40/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Justin Lovelace, Varsha Kishore, Yiwei Chen, Kilian Q. Weinberger
- **URL**: [https://arxiv.org/abs/2408.04220](https://arxiv.org/abs/2408.04220)

#### æ‘˜è¦

Current language models demonstrate remarkable proficiency in text generation. However, for many applications it is desirable to control attributes, such as sentiment, or toxicity, of the generated language -- ideally tailored towards each specific use case and target audience. For auto-regressive l...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡çš„ä¼˜ç‚¹åœ¨äºå°†æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹çš„ä¼˜åŠ¿ç»“åˆèµ·æ¥ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–ä¸”å®ç”¨çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé™ä½äº†æ–‡æœ¬å±æ€§æ§åˆ¶çš„éš¾åº¦ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚
- **ç¼ºç‚¹**: ç¼ºç‚¹åœ¨äºæ‘˜è¦ä¸­ç¼ºå°‘å…³äºæŠ€æœ¯ç»†èŠ‚çš„æè¿°ï¼Œä¾‹å¦‚å¦‚ä½•ç”Ÿæˆlatent proposalï¼Œä»¥åŠå¦‚ä½•æœ‰æ•ˆå¼•å¯¼è‡ªå›å½’æ¨¡å‹ã€‚éœ€è¦é˜…è¯»æ­£æ–‡æ‰èƒ½æ›´å…¨é¢åœ°è¯„ä¼°å…¶æŠ€æœ¯æ·±åº¦å’Œå®éªŒéªŒè¯çš„å……åˆ†æ€§ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡æ­£æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºæ¨¡å‹æ¶æ„ã€è®­ç»ƒç»†èŠ‚å’Œå®éªŒè®¾ç½®çš„éƒ¨åˆ†ï¼Œä»¥æ›´å…¨é¢åœ°äº†è§£è¯¥æ–¹æ³•çš„æŠ€æœ¯ç»†èŠ‚å’Œæ€§èƒ½è¡¨ç°ã€‚å…³æ³¨è®ºæ–‡ä¸­å®éªŒéƒ¨åˆ†çš„æ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½å¤Ÿè¦†ç›–åˆ°ä½ çš„åº”ç”¨åœºæ™¯ã€‚

---

### 49. Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Jacob K Christopher, Brian R Bartoldson, Tal Ben-Nun, Michael Cardei, Bhavya Kailkhura, Ferdinando Fioretto
- **URL**: [https://arxiv.org/abs/2408.05636](https://arxiv.org/abs/2408.05636)

#### æ‘˜è¦

Speculative decoding has emerged as a widely adopted method to accelerate large language model inference without sacrificing the quality of the model outputs. While this technique has facilitated notable speed improvements by enabling parallel sequence verification, its efficiency remains inherently...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: 1. ä¸»é¢˜é«˜åº¦ç›¸å…³ï¼Œç´§å¯†å›´ç»•æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒåº”ç”¨å±•å¼€ã€‚
2. åˆ›æ–°æ€§è¾ƒå¼ºï¼Œå°†æ‰©æ•£æ¨¡å‹å¼•å…¥Speculative Decodingï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŠ é€Ÿæ¡†æ¶ã€‚
3. å®ç”¨ä»·å€¼è¾ƒé«˜ï¼Œèƒ½å¤Ÿæ˜¾è‘—åŠ é€ŸLLMçš„æ¨ç†é€Ÿåº¦ã€‚
4. å®éªŒç»“æœæ˜¾ç¤ºäº†æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚
- **ç¼ºç‚¹**: 1. æŠ€æœ¯æ·±åº¦æ–¹é¢ï¼Œå¦‚æœèƒ½æ›´è¯¦ç»†åœ°é˜è¿°æ‰©æ•£æ¨¡å‹çš„é€‰æ‹©ã€è®­ç»ƒä»¥åŠä¸Speculative Decodingç»“åˆçš„å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¼šæ›´æœ‰è¯´æœåŠ›ã€‚
2. å¯èƒ½éœ€è¦æ›´å¤šçš„ ablation study æ¥åˆ†æå„ä¸ªç»„ä»¶å¯¹æ€§èƒ½çš„å½±å“ã€‚
3. éœ€è¦åœ¨æ›´å¤šä¸åŒçš„è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡ŒéªŒè¯ï¼Œä»¥è¯æ˜å…¶é€šç”¨æ€§ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è¯¥è®ºæ–‡ï¼Œäº†è§£Speculative Decodingå’Œæ‰©æ•£æ¨¡å‹çš„åŸºæœ¬åŸç†ã€‚é‡ç‚¹å…³æ³¨è®ºæ–‡ä¸­æ‰©æ•£æ¨¡å‹ä¸Speculative Decodingç»“åˆçš„å…·ä½“å®ç°æ–¹å¼ã€‚åŒæ—¶ï¼Œå…³æ³¨å®éªŒç»“æœå’Œåˆ†æï¼Œäº†è§£è¯¥æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚å¦‚æœå¯¹è¯¥æ–¹å‘æ„Ÿå…´è¶£ï¼Œå¯ä»¥å°è¯•å¤ç°è®ºæ–‡ä¸­çš„å®éªŒï¼Œå¹¶æ¢ç´¢è¿›ä¸€æ­¥æ”¹è¿›çš„å¯èƒ½æ€§ï¼Œä¾‹å¦‚ï¼Œæ¢ç´¢ä¸åŒçš„æ‰©æ•£æ¨¡å‹æ¶æ„ï¼Œæˆ–è€…æ”¹è¿›Speculative Decodingçš„éªŒè¯ç­–ç•¥ã€‚

---

### 50. Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang
- **URL**: [https://arxiv.org/abs/2409.02908](https://arxiv.org/abs/2409.02908)

#### æ‘˜è¦

Masked diffusion models (MDMs) have emerged as a popular research topic for generative modeling of discrete data, thanks to their superior performance over other discrete diffusion models, and are rivaling the auto-regressive models (ARMs) for language modeling tasks. The recent effort in simplifyin...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 8/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡æ·±å…¥åˆ†æäº†Masked Diffusion Modelsçš„è®­ç»ƒå’Œé‡‡æ ·æœºåˆ¶ï¼ŒæŒ‡å‡ºäº†å…¶ä¸time variableæ— å…³çš„ç‰¹æ€§ï¼Œå¹¶æå‡ºäº†First-Hitting Samplerä»¥åŠ é€Ÿé‡‡æ ·ã€‚åŒæ—¶ï¼Œæ­ç¤ºäº†categorical samplingä¸­çš„numericalé—®é¢˜ï¼Œå¹¶æ¢è®¨äº†å…¶å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­æåˆ°é‡‡ç”¨First-Hitting Sampler (FHS) å¯ä»¥æ˜¾è‘—åŠ é€Ÿcategorical samplingå¹¶æé«˜ç”Ÿæˆè´¨é‡ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œ FHS å¯èƒ½éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè°ƒæ•´æ‰èƒ½è¾¾åˆ°æœ€ä½³æ•ˆæœï¼Œè€Œè¿™ç§è°ƒæ•´ä¼šå¢åŠ è®¡ç®—æˆæœ¬ï¼ŒæŠµæ¶ˆåŠ é€Ÿçš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè™½ç„¶è®ºæ–‡æŒ‡å‡ºäº†numericalé—®é¢˜ï¼Œä½†å¯èƒ½éœ€è¦æ›´æ·±å…¥çš„åˆ†æå’Œæ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ³•æ¥å½»åº•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»è¯¥è®ºæ–‡çš„ç ”ç©¶è€…é‡ç‚¹å…³æ³¨è®ºæ–‡ä¸­å¯¹MDMæ—¶é—´æ— å…³æ€§çš„åˆ†æã€FHSç®—æ³•çš„ç»†èŠ‚ä»¥åŠnumerical issueçš„è®¨è®ºã€‚ å¯ä»¥å°è¯•å¤ç°è®ºæ–‡ä¸­çš„å®éªŒç»“æœï¼Œå¹¶è¿›ä¸€æ­¥ç ”ç©¶FHSç®—æ³•åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„é€‚ç”¨æ€§å’Œä¼˜åŒ–æ–¹æ³•ã€‚ é’ˆå¯¹numerical issueï¼Œå¯ä»¥æ¢ç´¢æ›´é«˜ç²¾åº¦çš„æµ®ç‚¹æ•°è¿ç®—æˆ–æ›´é²æ£’çš„é‡‡æ ·ç®—æ³•ã€‚

---

### 51. An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification

<div align="center">

**åŠ æƒæ€»åˆ†: 7.70/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Zhuowei Chen, Lianxi Wang, Yuben Wu, Xinfeng Liao, Yujia Tian, Junyang Zhong
- **URL**: [https://arxiv.org/abs/2409.03203](https://arxiv.org/abs/2409.03203)

#### æ‘˜è¦

Sentiment classification (SC) often suffers from low-resource challenges such as domain-specific contexts, imbalanced label distributions, and few-shot scenarios. The potential of the diffusion language model (LM) for textual data augmentation (DA) remains unexplored, moreover, textual DA methods st...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡é’ˆå¯¹ä½èµ„æºæƒ…æ„Ÿåˆ†ç±»é—®é¢˜ï¼Œæå‡ºäº†ä½¿ç”¨ Diffusion LM è¿›è¡Œæ•°æ®å¢å¼ºçš„æ–°é¢–æ–¹æ³•ï¼Œå³DiffusionCLSï¼Œé€šè¿‡é‡æ„å¼ºæƒ…æ„Ÿtokenæ¥ç”Ÿæˆä¼ªæ ·æœ¬ï¼Œå¹³è¡¡äº†æ•°æ®å¢å¼ºçš„ diversity å’Œ consistencyï¼Œ åŒæ—¶ä½¿ç”¨äº† Noise-Resistant Training objective æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚ å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šç§ä½èµ„æºåœºæ™¯ä¸‹æœ‰æ•ˆã€‚
- **ç¼ºç‚¹**: ä»…é€šè¿‡æ‘˜è¦æ— æ³•è¯„ä¼°ç†è®ºåˆ†æçš„æ·±åº¦å’Œå®éªŒçš„å…¨é¢æ€§ã€‚å¦‚æœèƒ½æä¾›æ›´è¯¦ç»†çš„æŠ€æœ¯ç»†èŠ‚å’Œæ›´å…¨é¢çš„å®éªŒè®¾ç½®ä¿¡æ¯ï¼Œå°†æœ‰åŠ©äºè¿›ä¸€æ­¥è¯„ä¼°è¯¥è®ºæ–‡çš„è´¨é‡ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äº DiffusionCLS çš„å…·ä½“å®ç°ï¼ŒNoise-Resistant Training objective çš„è®¾è®¡ï¼Œä»¥åŠå®éªŒè®¾ç½®å’Œç»“æœåˆ†æéƒ¨åˆ†ã€‚å…³æ³¨è¯¥æ–¹æ³•åœ¨å…¶ä»–ä½èµ„æº NLP ä»»åŠ¡ä¸Šçš„æ½œåœ¨åº”ç”¨ã€‚

---

### 52. Towards Diverse and Efficient Audio Captioning via Diffusion Models

<div align="center">

**åŠ æƒæ€»åˆ†: 7.60/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Manjie Xu, Chenxing Li, Xinyi Tu, Yong Ren, Ruibo Fu, Wei Liang, Dong Yu
- **URL**: [https://arxiv.org/abs/2409.09401](https://arxiv.org/abs/2409.09401)

#### æ‘˜è¦

We introduce Diffusion-based Audio Captioning (DAC), a non-autoregressive diffusion model tailored for diverse and efficient audio captioning. Although existing captioning models relying on language backbones have achieved remarkable success in various captioning tasks, their insufficient performanc...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: å°†Diffusionæ¨¡å‹åº”ç”¨äºéŸ³é¢‘å­—å¹•ç”Ÿæˆï¼Œåœ¨ç”Ÿæˆé€Ÿåº¦å’Œå¤šæ ·æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶å¯èƒ½æ¨åŠ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡çš„å‘å±•ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹å…·ä½“æŠ€æœ¯ç»†èŠ‚çš„æè¿°ï¼Œä¾‹å¦‚æ¨¡å‹ç»“æ„ã€è®­ç»ƒæ–¹æ³•ç­‰ã€‚éœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½è¯„ä¼°å…¶æŠ€æœ¯æ·±åº¦å’Œå®éªŒéªŒè¯çš„å……åˆ†æ€§ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹ç»“æ„ã€è®­ç»ƒæ–¹æ³•ã€å®éªŒè®¾ç½®å’Œç»“æœåˆ†æï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°è®ºæ–‡çš„è´¡çŒ®å’Œä»·å€¼ã€‚å¯ä»¥å…³æ³¨è®ºæ–‡ä¸­æ˜¯å¦è¯¦ç»†æè¿°äº†å¦‚ä½•è§£å†³Diffusionæ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆä¸­å¯èƒ½é‡åˆ°çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚ä¸€è‡´æ€§å’Œæµç•…æ€§ã€‚åŒæ—¶ï¼Œå…³æ³¨å®éªŒç»“æœæ˜¯å¦è¶³å¤Ÿå……åˆ†ï¼Œæ˜¯å¦ä¸å…¶ä»–æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„å¯¹æ¯”ï¼Œä»¥åŠæ˜¯å¦å­˜åœ¨æ¶ˆèå®éªŒæ¥éªŒè¯ä¸åŒç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚

---

### 53. Think While You Generate: Discrete Diffusion with Planned Denoising

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Sulin Liu, Juno Nam, Andrew Campbell, Hannes StÃ¤rk, Yilun Xu, Tommi Jaakkola, Rafael GÃ³mez-Bombarelli
- **URL**: [https://arxiv.org/abs/2410.06264](https://arxiv.org/abs/2410.06264)

#### æ‘˜è¦

Discrete diffusion has achieved state-of-the-art performance, outperforming or approaching autoregressive models on standard benchmarks. In this work, we introduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework that separates the generation process into two models: a planner and...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ‰©æ•£è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è§„åˆ’å™¨æ¥ä¼˜åŒ–å»å™ªè¿‡ç¨‹ï¼Œæé«˜äº†ç”Ÿæˆæ•ˆç‡å’Œæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDDPDåœ¨è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œå¹¶ç¼©å°äº†æ‰©æ•£æ¨¡å‹ä¸è‡ªå›å½’æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚æä¾›äº†å¯å¤ç°çš„ä»£ç ã€‚
- **ç¼ºç‚¹**: ä»æ‘˜è¦æ¥çœ‹ï¼Œéœ€è¦æ·±å…¥ç ”ç©¶æ­£æ–‡æ‰èƒ½è¯„ä¼°ç†è®ºåˆ†æçš„ä¸¥è°¨æ€§å’Œå®éªŒçš„å…¨é¢æ€§ã€‚å¦‚æœå®éªŒåªæ˜¯é›†ä¸­åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šï¼Œå¯èƒ½ä¼šé™åˆ¶ç»“è®ºçš„æ³›åŒ–æ€§ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºè§„åˆ’å™¨è®¾è®¡çš„ç»†èŠ‚å’Œå®éªŒéƒ¨åˆ†çš„åˆ†æï¼Œä»¥æ›´å…¨é¢åœ°äº†è§£è¯¥æ–¹æ³•çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚å¯ä»¥å…³æ³¨ä½œè€…åœ¨GitHubä¸Šæä¾›çš„ä»£ç ï¼Œè¿›è¡Œå¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚

---

### 54. Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Yun-Yen Chuang, Hung-Min Hsu, Kevin Lin, Chen-Sheng Gu, Ling Zhen Li, Ray-I Chang, Hung-yi Lee
- **URL**: [https://arxiv.org/abs/2410.13201](https://arxiv.org/abs/2410.13201)

#### æ‘˜è¦

The diffusion model, a new generative modeling paradigm, has achieved significant success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed S2S Diffusion. Existing S2S-Diffusion models predominantly rely on f...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ scheduler-exploiter æ¡†æ¶ Meta-DiffuBï¼Œç”¨äºåºåˆ—åˆ°åºåˆ—æ–‡æœ¬ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡ Meta-Exploration è®­ç»ƒ scheduler æ¨¡å‹ï¼Œä¸ºæ¯ä¸ªå¥å­è°ƒåº¦ä¸Šä¸‹æ–‡ç›¸å…³çš„å™ªå£°ï¼Œå¹¶åœ¨å››ä¸ª benchmark æ•°æ®é›†ä¸Šå–å¾—äº† SOTA ç»“æœã€‚è®ºæ–‡è¿˜æä¾›äº†å¯è§†åŒ–åˆ†æï¼Œæœ‰åŠ©äºç†è§£å™ªå£°è°ƒåº¦å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹ Meta-Exploration ç®—æ³•çš„å…·ä½“æè¿°ï¼Œéœ€è¦é˜…è¯»è®ºæ–‡æ‰èƒ½äº†è§£ç»†èŠ‚ã€‚å®éªŒç»“æœçš„å¯é‡å¤æ€§ä»¥åŠåœ¨æ›´å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›éœ€è¦è¿›ä¸€æ­¥éªŒè¯ã€‚å¦å¤–ï¼Œplug-and-play çš„åº”ç”¨æ•ˆæœä¹Ÿéœ€è¦åœ¨æ›´å¤šåœºæ™¯ä¸‹è¿›è¡Œæµ‹è¯•ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨ Meta-Exploration ç®—æ³•çš„ç»†èŠ‚å’Œå®éªŒç»“æœçš„å¯é æ€§ã€‚å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶ scheduler æ¨¡å‹åœ¨å…¶ä»– S2S-Diffusion æ¨¡å‹ä¸­çš„é€‚ç”¨æ€§ï¼Œä»¥åŠåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å¯¹äºå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹ä»¥åŠåºåˆ—åˆ°åºåˆ—æ–‡æœ¬ç”Ÿæˆæ„Ÿå…´è¶£çš„è¯»è€…ï¼Œè¿™æ˜¯ä¸€ç¯‡å€¼å¾—é˜…è¯»çš„è®ºæ–‡ã€‚

---

### 55. Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning

<div align="center">

**åŠ æƒæ€»åˆ†: 7.80/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong
- **URL**: [https://arxiv.org/abs/2410.14157](https://arxiv.org/abs/2410.14157)

#### æ‘˜è¦

Autoregressive language models, despite their impressive capabilities, struggle with complex reasoning and long-term planning tasks. We introduce discrete diffusion models as a novel solution to these challenges. Through the lens of subgoal imbalance, we demonstrate how diffusion models effectively ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡çš„ä¼˜ç‚¹åœ¨äºå®ƒæ¢ç´¢äº†æ‰©æ•£æ¨¡å‹åœ¨å¤æ‚æ¨ç†å’Œè§„åˆ’ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œå¹¶æå‡ºäº†MGDMæ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¼˜äºè‡ªå›å½’æ¨¡å‹ã€‚è®ºæ–‡æ¸…æ™°åœ°é˜è¿°äº†æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å›°éš¾å­ç›®æ ‡æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¹¶æä¾›äº†å¼€æºä»£ç ã€‚
- **ç¼ºç‚¹**: è®ºæ–‡çš„ç¼ºç‚¹åœ¨äºç†è®ºåˆ†æå¯ä»¥æ›´åŠ æ·±å…¥ã€‚è™½ç„¶æä¾›äº†å®éªŒç»“æœï¼Œä½†éœ€è¦æ›´å¤šçš„ç†è®ºæ”¯æŒï¼Œä¾‹å¦‚å¯¹diffusionè¿‡ç¨‹çš„æ”¶æ•›æ€§å’Œå¤æ‚åº¦çš„åˆ†æã€‚æ­¤å¤–ï¼Œç›®å‰çš„å®éªŒä»»åŠ¡ç›¸å¯¹æ¯”è¾ƒçª„ï¼Œå¯ä»¥è€ƒè™‘æ‰©å±•åˆ°æ›´å¤æ‚çš„è‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»è¯¥è®ºæ–‡ä»¥äº†è§£æ‰©æ•£æ¨¡å‹åœ¨å¤æ‚æ¨ç†å’Œè§„åˆ’ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚å°¤å…¶å…³æ³¨MGDMæ–¹æ³•å’Œå®éªŒç»“æœï¼Œå¯ä»¥ä½œä¸ºåç»­ç ”ç©¶çš„èµ·ç‚¹ã€‚åœ¨é˜…è¯»æ—¶ï¼Œå¯ä»¥æ€è€ƒå¦‚ä½•å°†è¯¥æ–¹æ³•åº”ç”¨åˆ°å…¶ä»–è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­ï¼Œå¹¶è¿›è¡Œæ›´æ·±å…¥çš„ç†è®ºåˆ†æã€‚æ­¤å¤–ï¼Œå¯ä»¥å…³æ³¨è®ºæ–‡æä¾›çš„å¼€æºä»£ç ï¼Œä»¥ä¾¿å¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚

---

### 56. Scaling Diffusion Language Models via Adaptation from Autoregressive Models

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, Lingpeng Kong
- **URL**: [https://arxiv.org/abs/2410.17891](https://arxiv.org/abs/2410.17891)

#### æ‘˜è¦

Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: 1. ä¸»é¢˜èšç„¦ï¼šå®Œå…¨ä¸“æ³¨äºæ‰©æ•£è¯­è¨€æ¨¡å‹ã€‚
2. åˆ›æ–°æ€§å¼ºï¼šæå‡ºäº†ä¸€ç§å°†é¢„è®­ç»ƒARæ¨¡å‹é€‚é…åˆ°DLMçš„æœ‰æ•ˆæ–¹æ³•ã€‚
3. å®ç”¨ä»·å€¼é«˜ï¼šèƒ½å¤Ÿåˆ©ç”¨ç°æœ‰çš„ARæ¨¡å‹ï¼Œå¹¶ä¸”æ¨¡å‹å…·æœ‰è¾ƒå¥½çš„ç”Ÿæˆèƒ½åŠ›å’Œå¡«å……èƒ½åŠ›ã€‚
4. ä»£ç å¼€æºï¼šå¼€æºä»£ç ä½¿å¾—å…¶ä»–ç ”ç©¶äººå‘˜å¯ä»¥å¤ç°å’Œæ‰©å±•è¿™é¡¹å·¥ä½œã€‚
5. å®éªŒå……åˆ†ï¼šåœ¨å¤šä¸ªbenchmarkä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
- **ç¼ºç‚¹**: 1. æŠ€æœ¯æ·±åº¦å¯ä»¥æ›´æ·±å…¥ï¼Œä¾‹å¦‚å¯ä»¥æ›´æ·±å…¥åœ°åˆ†æARæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„å·®å¼‚å’Œè”ç³»ã€‚
2. å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ä¸åŒè§„æ¨¡çš„ARæ¨¡å‹é€‚é…åˆ°DLMçš„æ•ˆæœï¼Œä»¥åŠæœ€ä½³çš„é€‚é…ç­–ç•¥ã€‚
3. ç¼ºä¹ä¸å…¶ä»–DLMçš„æ›´å…¨é¢çš„å¯¹æ¯”åˆ†æï¼Œä¾‹å¦‚è®­ç»ƒæ•ˆç‡ï¼Œå†…å­˜æ¶ˆè€—ç­‰ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®å¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹é¢†åŸŸæ„Ÿå…´è¶£çš„è¯»è€…ä»”ç»†é˜…è¯»è¿™ç¯‡è®ºæ–‡ã€‚è¯¥è®ºæ–‡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒDLMçš„æ–¹æ³•ï¼Œå¹¶å…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼ã€‚å¯ä»¥é‡ç‚¹å…³æ³¨è®ºæ–‡ä¸­å…³äºARæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´è”ç³»çš„è®¨è®ºï¼Œä»¥åŠå®éªŒéƒ¨åˆ†çš„ç»†èŠ‚ã€‚å¦å¤–ï¼Œå¯ä»¥å°è¯•å¤ç°è®ºæ–‡ä¸­çš„ç»“æœï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ‰©å±•ç ”ç©¶ã€‚

---

### 57. Scaling up Masked Diffusion Models on Text

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, Chongxuan Li
- **URL**: [https://arxiv.org/abs/2410.18514](https://arxiv.org/abs/2410.18514)

#### æ‘˜è¦

Masked diffusion models (MDMs) have shown promise in language modeling, yet their scalability and effectiveness in core language tasks, such as text generation and language understanding, remain underexplored. This paper establishes the first scaling law for MDMs, demonstrating a scaling rate compar...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡çš„ä¼˜ç‚¹åœ¨äºå…¶ä¸»é¢˜é«˜åº¦ç›¸å…³ï¼Œåˆ›æ–°æ€§åœ°æ¢ç´¢äº†MDMsçš„scaling lawï¼Œå¹¶æå‡ºäº†å®ç”¨çš„æ— ç›‘ç£classifier-free guidanceã€‚å®éªŒç»“æœè¡¨æ˜MDMsåœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¶Šäº†ARMsï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åŒå‘æ¨ç†å’Œæ•°æ®æ—¶åºå˜åŒ–æ–¹é¢ã€‚è®ºæ–‡è¿˜å¼€æºäº†ä»£ç ï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…å¤ç°å’Œæ”¹è¿›ã€‚
- **ç¼ºç‚¹**: è®ºæ–‡çš„ç¼ºç‚¹å¯èƒ½åœ¨äºç†è®ºåˆ†ææ–¹é¢ï¼Œå°¤å…¶æ˜¯å¯¹äºscaling lawçš„ç†è®ºè§£é‡Šå¯ä»¥æ›´åŠ æ·±å…¥ã€‚å¦å¤–ï¼Œå®éªŒéƒ¨åˆ†å¯ä»¥è€ƒè™‘æ›´å¹¿æ³›çš„ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œä»¥å¢å¼ºç»“è®ºçš„æ™®é€‚æ€§ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®å¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹æ„Ÿå…´è¶£çš„è¯»è€…é˜…è¯»è¯¥è®ºæ–‡ã€‚ç‰¹åˆ«æ˜¯å…³æ³¨MDMsçš„scaling lawã€æ— ç›‘ç£classifier-free guidanceå’Œå®éªŒç»“æœã€‚å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶è®ºæ–‡ä¸­æåˆ°çš„MDMsçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œæ¢ç´¢MDMsåœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ï¼Œä»¥åŠæ”¹è¿›MDMsçš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚

---

### 58. Beyond Autoregression: Fast LLMs via Self-Distillation Through Time

<div align="center">

**åŠ æƒæ€»åˆ†: 8.30/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Justin Deschenaux, Caglar Gulcehre
- **URL**: [https://arxiv.org/abs/2410.21035](https://arxiv.org/abs/2410.21035)

#### æ‘˜è¦

Autoregressive (AR) Large Language Models (LLMs) have demonstrated significant success across numerous tasks. However, the AR modeling paradigm presents certain limitations; for instance, contemporary autoregressive LLMs are trained to generate one token at a time, which can result in noticeable lat...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºæå‡ºäº†ä¸€ä¸ªåŠ é€Ÿdiffusion language modelç”Ÿæˆè¿‡ç¨‹çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡self-distillationå‡å°‘inferenceæ­¥éª¤ï¼Œå¹¶å®éªŒè¯æ˜å…¶æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é€Ÿåº¦ä¸Šè¶…è¶Šäº†ä¼ ç»ŸARæ¨¡å‹ã€‚è¯¥æ–¹æ³•å…·æœ‰å®é™…åº”ç”¨æ½œåŠ›ï¼Œèƒ½å¤Ÿæé«˜diffusion modelsåœ¨LLMé¢†åŸŸçš„ç«äº‰åŠ›ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å…³äºdistillationæ–¹æ³•çš„å…·ä½“ç»†èŠ‚ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½äº†è§£ã€‚æ­¤å¤–ï¼Œè™½ç„¶åœ¨é€Ÿåº¦ä¸Šè¶…è¶Šäº†ARæ¨¡å‹ï¼Œä½†è¿˜éœ€è¦è¿›ä¸€æ­¥è€ƒå¯Ÿåœ¨ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚860Må‚æ•°æ¨¡å‹çš„ç»“æœä»…è¯´æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œè¿˜éœ€è¦æ›´å¤šå®éªŒæ•°æ®æ¥è¯æ˜ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨self-distillationæ–¹æ³•çš„å…·ä½“å®ç°ç»†èŠ‚å’Œå®éªŒè®¾ç½®ã€‚è€ƒå¯Ÿè¯¥æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå¯ä»¥å…³æ³¨è¯¥æ–¹æ³•åœ¨æ›´å¤§è§„æ¨¡æ¨¡å‹ä¸Šçš„åº”ç”¨æ½œåŠ›ã€‚

---

### 59. Energy-Based Diffusion Language Models for Text Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 8.10/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Minkai Xu, Tomas Geffner, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, Arash Vahdat
- **URL**: [https://arxiv.org/abs/2410.21357](https://arxiv.org/abs/2410.21357)

#### æ‘˜è¦

Despite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently emerged as a promising alternative. Unfortunately, th...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: æå‡ºäº†æ–°çš„èƒ½é‡æ¨¡å‹çš„æ‰©æ•£è¯­è¨€æ¨¡å‹(EDLM)ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶æ¥è¿‘è‡ªå›å½’æ¨¡å‹æ€§èƒ½ã€‚ æä¾›äº†SamplingåŠ é€Ÿï¼Œå…·æœ‰ä¸€å®šçš„å®ç”¨ä»·å€¼ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­å…³äºæŠ€æœ¯ç»†èŠ‚çš„æè¿°ä¸å¤Ÿè¯¦ç»†ã€‚ ä¾‹å¦‚EBMçš„å…·ä½“å½¢å¼ï¼Œå¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä»¥åŠé‡è¦æ€§é‡‡æ ·çš„å…·ä½“å®ç°ç­‰ï¼Œ éœ€è¦é€šè¿‡é˜…è¯»å…¨æ–‡æ‰èƒ½ç†è§£ã€‚æ‘˜è¦æ²¡æœ‰ä½“ç°å‡ºæ‰€æœ‰å®éªŒç»†èŠ‚ï¼Œä¾‹å¦‚æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡çš„å…·ä½“è®¾ç½®ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ä»¥äº†è§£æ›´è¯¦ç»†çš„æŠ€æœ¯ç»†èŠ‚å’Œå®éªŒç»“æœã€‚å°¤å…¶å…³æ³¨èƒ½é‡æ¨¡å‹çš„å…·ä½“å®ç°å’Œå¹¶è¡Œé‡è¦æ€§é‡‡æ ·çš„ç®—æ³•ç»†èŠ‚ã€‚ å¯ä»¥å°è¯•å¤ç°è®ºæ–‡ä¸­çš„å®éªŒç»“æœï¼ŒéªŒè¯EDLMçš„æœ‰æ•ˆæ€§ã€‚

---

### 60. DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models

<div align="center">

**åŠ æƒæ€»åˆ†: 7.80/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Ying Zhou, Xinyao Wang, Yulei Niu, Yaojie Shen, Lexin Tang, Fan Chen, Ben He, Le Sun, Longyin Wen
- **URL**: [https://arxiv.org/abs/2411.03250](https://arxiv.org/abs/2411.03250)

#### æ‘˜è¦

Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis. However, synthetic data generation via prompting LLMs remains challenging due to LLMs' limit...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡çš„æ ¸å¿ƒä¼˜ç‚¹åœ¨äºå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºè¯­è¨€æ•°æ®çš„åˆæˆï¼Œå¹¶é’ˆå¯¹LLMç”Ÿæˆç»“æ„åŒ–æ•°æ®çš„é—®é¢˜æå‡ºäº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚DiffLMæ¡†æ¶å…·æœ‰è‰¯å¥½çš„å¯æ§æ€§å’Œå®ç”¨ä»·å€¼ï¼Œèƒ½å¤Ÿæå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚
- **ç¼ºç‚¹**: ä»æ‘˜è¦æ¥çœ‹ï¼ŒæŠ€æœ¯æ·±åº¦éƒ¨åˆ†æè¿°è¾ƒä¸ºç®€ç•¥ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ›´å‡†ç¡®çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæ‘˜è¦æœªæåŠæ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ï¼Œä»¥åŠæ¨¡å‹å‚æ•°è§„æ¨¡ç­‰é—®é¢˜ï¼Œè¿™äº›æ˜¯å®é™…åº”ç”¨ä¸­éœ€è¦è€ƒè™‘çš„é‡è¦å› ç´ ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®è¯¦ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºVAEå’Œæ‰©æ•£æ¨¡å‹çš„å…·ä½“å®ç°ã€latent feature injectionæ¨¡å—çš„è®¾è®¡ä»¥åŠå®éªŒç»“æœçš„åˆ†æã€‚é‡ç‚¹å…³æ³¨æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ä»¥åŠæ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚å¦‚æœä»£ç å¼€æºï¼Œå¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶ä»£ç å®ç°ç»†èŠ‚ï¼Œå¹¶å°è¯•å°†å…¶åº”ç”¨äºå…¶ä»–ç›¸å…³ä»»åŠ¡ã€‚

---

### 61. Conditional [MASK] Discrete Diffusion Language Model

<div align="center">

**åŠ æƒæ€»åˆ†: 7.80/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Hyukhun Koh, Minha Jhang, Dohyung Kim, Sangmook Lee, Kyomin Jung
- **URL**: [https://arxiv.org/abs/2411.06438](https://arxiv.org/abs/2411.06438)

#### æ‘˜è¦

Although auto-regressive models excel in natural language processing, they often struggle to generate diverse text and provide limited controllability. Non-auto-regressive methods could be an alternative but often produce degenerate outputs and exhibit shortcomings in conditional generation. To addr...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡å…³æ³¨äº†æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨éè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆä¸­çš„è´¨é‡å’Œå¤šæ ·æ€§é—®é¢˜ï¼Œæå‡ºäº†Diffusion-EAGSæ¡†æ¶å¹¶ç»“åˆäº†masked language modelsã€‚æå‡ºçš„entropy-adaptive Gibbs samplingå’Œentropy-based noise schedulingçœ‹èµ·æ¥å¾ˆæœ‰æ½œåŠ›ã€‚è¯¥ç ”ç©¶æœ‰è§£å†³å®é™…é—®é¢˜çš„æ½œåŠ›ã€‚
- **ç¼ºç‚¹**: ä»…ä»…ä»æ‘˜è¦æ¥çœ‹ï¼Œéš¾ä»¥è¯„ä¼°æŠ€æœ¯ç»†èŠ‚å’Œå®éªŒçš„ä¸¥è°¨æ€§ã€‚éœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½åˆ¤æ–­æå‡ºçš„æ–¹æ³•çš„å®é™…æ•ˆæœå’Œé€‚ç”¨èŒƒå›´ã€‚æ‘˜è¦ä¸­æœªæåŠæ•°æ®é›†ï¼Œä¹Ÿæœªèƒ½åˆ¤æ–­æ¨¡å‹è®­ç»ƒçš„å¤æ‚åº¦ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»è¯¥è®ºæ–‡ä»¥äº†è§£Diffusion-EAGSæ¡†æ¶çš„ç»†èŠ‚ã€entropy-adaptive Gibbs samplingå’Œentropy-based noise schedulingçš„å®ç°æ–¹å¼ï¼Œä»¥åŠå®éªŒè®¾ç½®å’Œç»“æœçš„è¯¦ç»†åˆ†æã€‚ç‰¹åˆ«å…³æ³¨å®éªŒç»“æœæ˜¯å¦æ”¯æŒè®ºæ–‡å£°ç§°çš„åœ¨è´¨é‡-å¤šæ ·æ€§trade-offä¸Šçš„ä¼˜åŠ¿ã€‚å¯ä»¥å°è¯•å¤ç°æˆ–åŸºäºå…¶æ¡†æ¶è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚

---

### 62. Multimodal Latent Language Modeling with Next-Token Diffusion

<div align="center">

**åŠ æƒæ€»åˆ†: 8.00/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei
- **URL**: [https://arxiv.org/abs/2412.08635](https://arxiv.org/abs/2412.08635)

#### æ‘˜è¦

Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„LatentLMæ¡†æ¶ï¼Œç»“åˆäº†VAEå’Œnext-token diffusionï¼Œç”¨äºå¤šæ¨¡æ€æ•°æ®çš„ç”Ÿæˆå’Œç†è§£ã€‚å®ƒåœ¨å›¾åƒç”Ÿæˆã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆç­‰å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è®ºæ–‡è§£å†³äº†æ‰©æ•£æ¨¡å‹åº”ç”¨äºç¦»æ•£æ•°æ®ç”Ÿæˆæ—¶çš„ä¸€äº›æŒ‘æˆ˜ï¼Œä¾‹å¦‚æ–¹å·®å´©æºƒã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹æŠ€æœ¯ç»†èŠ‚çš„æ·±å…¥æ¢è®¨ï¼Œä¾‹å¦‚diffusionè¿‡ç¨‹çš„å…·ä½“å®ç°ã€æŸå¤±å‡½æ•°çš„è®¾è®¡ä»¥åŠÏƒ-VAEçš„æ•°å­¦æ¨å¯¼ã€‚è¯„ä¼°å…¶çœŸæ­£æ·±åº¦éœ€è¦é˜…è¯»å…¨æ–‡ã€‚æ­¤å¤–ï¼Œè®ºæ–‡éœ€è¦æä¾›æ›´è¯¦ç»†çš„å®éªŒè®¾ç½®å’Œç»“æœåˆ†æï¼Œä»¥å¢å¼ºå…¶å¯ä¿¡åº¦ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºÏƒ-VAEçš„å®ç°ç»†èŠ‚å’Œå®éªŒè®¾ç½®éƒ¨åˆ†ã€‚ å¯ä»¥å…³æ³¨LatentLMåœ¨ä¸åŒæ¨¡æ€æ•°æ®ä¸Šçš„ç”Ÿæˆæ•ˆæœï¼Œä»¥åŠå…¶åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å¯æ‰©å±•æ€§ã€‚å¦‚æœå¯¹å¤šæ¨¡æ€å»ºæ¨¡å’Œæ‰©æ•£æ¨¡å‹æ„Ÿå…´è¶£ï¼Œè¿™æ˜¯ä¸€ç¯‡å€¼å¾—é˜…è¯»çš„è®ºæ–‡ã€‚åŒæ—¶ï¼Œä¹Ÿè¦å…³æ³¨è¯¥æ–¹æ³•åœ¨èµ„æºå—é™æƒ…å†µä¸‹çš„è¡¨ç°ä»¥åŠæ½œåœ¨çš„å±€é™æ€§ã€‚

---

### 63. Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models

<div align="center">

**åŠ æƒæ€»åˆ†: 8.10/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Xiaochen Zhu, Georgi Karadzhov, Chenxi Whitehouse, Andreas Vlachos
- **URL**: [https://arxiv.org/abs/2412.11333](https://arxiv.org/abs/2412.11333)

#### æ‘˜è¦

Diffusion models have shown promise in text generation but often struggle with generating long, coherent, and contextually accurate text. Token-level diffusion overlooks word-order dependencies and enforces short output windows, while passage-level diffusion struggles with learning robust representa...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡é’ˆå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­çš„é—®é¢˜ï¼Œæå‡ºäº†Segment-Level Diffusionæ¡†æ¶ï¼Œé€šè¿‡åˆ†å‰²ã€è¡¨ç¤ºå­¦ä¹ å’Œæ½œåœ¨ç©ºé—´æŒ‡å¯¼ï¼Œæœ‰æ•ˆåœ°æå‡äº†ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ã€‚åŒæ—¶ï¼Œæ¶ˆèå®éªŒä¹ŸéªŒè¯äº†å„ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚
- **ç¼ºç‚¹**: è®ºæ–‡çš„æŠ€æœ¯æ·±åº¦å¯ä»¥è¿›ä¸€æ­¥åŠ å¼ºï¼Œä¾‹å¦‚æ›´æ·±å…¥åœ°åˆ†æåˆ†å‰²ç­–ç•¥çš„ç†è®ºä¾æ®ï¼Œä»¥åŠæ½œåœ¨ç©ºé—´æŒ‡å¯¼çš„å…·ä½“å®ç°ç»†èŠ‚å’Œæ•ˆæœã€‚æ­¤å¤–ï¼Œè™½ç„¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œä½†æ•°æ®é›†çš„ç±»å‹ç›¸å¯¹é›†ä¸­ï¼Œå¦‚æœèƒ½æ‰©å±•åˆ°æ›´å¤šç±»å‹çš„é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œå°†ä¼šæ›´å…·è¯´æœåŠ›ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡çš„å®éªŒéƒ¨åˆ†ï¼Œå…³æ³¨Segment-Level Diffusionæ¡†æ¶çš„å…·ä½“å®ç°ç»†èŠ‚å’Œå‚æ•°è®¾ç½®ã€‚å¯ä»¥å°è¯•å¤ç°è®ºæ–‡çš„å®éªŒç»“æœï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œä¾‹å¦‚æ¢ç´¢ä¸åŒçš„åˆ†å‰²ç­–ç•¥å’Œæ½œåœ¨ç©ºé—´æŒ‡å¯¼æ–¹æ³•ã€‚åŒæ—¶ï¼Œå¯ä»¥å…³æ³¨è¯¥è®ºæ–‡åç»­çš„ç ”ç©¶è¿›å±•ï¼Œä»¥åŠå…¶ä»–ç ”ç©¶è€…åœ¨è¯¥æ–¹å‘ä¸Šçš„å·¥ä½œã€‚

---

### 64. DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak

<div align="center">

**åŠ æƒæ€»åˆ†: 7.70/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Hao Wang, Hao Li, Junda Zhu, Xinyuan Wang, Chengwei Pan, MinLie Huang, Lei Sha
- **URL**: [https://arxiv.org/abs/2412.17522](https://arxiv.org/abs/2412.17522)

#### æ‘˜è¦

Large Language Models (LLMs) are susceptible to generating harmful content when prompted with carefully crafted inputs, a vulnerability known as LLM jailbreaking. As LLMs become more powerful, studying jailbreak methods is critical to enhancing security and aligning models with human values. Traditi...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: 1. åˆ©ç”¨ Diffusion æ¨¡å‹è¿›è¡Œ LLM Jailbreak æ”»å‡»ï¼Œæ€è·¯æ–°é¢–ã€‚
2. ä½¿ç”¨ seq2seq ç»“æ„ï¼Œå…‹æœäº†è‡ªå›å½’æ¨¡å‹çš„å±€é™æ€§ã€‚
3. æå‡ºäº† Gumbel-Softmax æŠ€å·§ï¼Œä½¿é‡‡æ ·è¿‡ç¨‹å¯å¾®ã€‚
4. åœ¨ç›¸å…³æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
- **ç¼ºç‚¹**: 1. æ‘˜è¦ä¿¡æ¯æœ‰é™ï¼ŒAttack Lossçš„è®¾è®¡ç»†èŠ‚ä¸å¤Ÿæ¸…æ™°ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½ç†è§£ã€‚
2. å¯¹äºæ‰©æ•£æ¨¡å‹åœ¨ prompt ç”Ÿæˆä¸­çš„ä¼˜åŠ¿ï¼Œå¯ä»¥æ›´æ·±å…¥åœ°åˆ†æã€‚
3. å…³äºç”Ÿæˆçš„ prompt çš„å¯è¯»æ€§å’Œè¯­ä¹‰ä¿ç•™ç¨‹åº¦ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ¢è®¨ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®è¯¦ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨ Attack Loss çš„è®¾è®¡ã€å®éªŒè®¾ç½®å’Œç»“æœåˆ†æã€‚å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•å°†è¯¥æ–¹æ³•åº”ç”¨äºå…¶ä»– LLM å®‰å…¨ç›¸å…³ä»»åŠ¡ï¼Œä¾‹å¦‚å¯¹æŠ—æ ·æœ¬ç”Ÿæˆå’Œé˜²å¾¡æœºåˆ¶è®¾è®¡ã€‚

---

### 65. Large Language Models to Diffusion Finetuning

<div align="center">

**åŠ æƒæ€»åˆ†: 7.80/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Edoardo Cetin, Tianyu Zhao, Yujin Tang
- **URL**: [https://arxiv.org/abs/2501.15781](https://arxiv.org/abs/2501.15781)

#### æ‘˜è¦

We propose a new finetuning method to provide pre-trained large language models (LMs) the ability to scale test-time compute through the diffusion framework. By increasing the number of diffusion steps, we show our finetuned models achieve monotonically increasing accuracy, directly translating to i...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: æå‡ºäº†ä¸€ç§å°†æ‰©æ•£æ¨¡å‹èå…¥å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒçš„æ–°é¢–æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æµ‹è¯•æ—¶è®¡ç®—æ‰©å±•æ€§å’Œæ€§èƒ½ã€‚è¯¥æ–¹æ³•å…·æœ‰æ½œåœ¨çš„å®ç”¨ä»·å€¼ï¼Œå¹¶ä¸”ä¸ç°æœ‰å¾®è°ƒæ–¹æ³•å…¼å®¹ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ç›¸å¯¹ç®€çŸ­ï¼Œç¼ºä¹å¯¹æŠ€æœ¯ç»†èŠ‚å’Œå®éªŒè®¾ç½®çš„è¯¦ç»†æè¿°ã€‚éœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ›´å…¨é¢åœ°è¯„ä¼°è¯¥è®ºæ–‡çš„è´¨é‡ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®è¯¦ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯æ–¹æ³•ã€å®éªŒè®¾ç½®å’Œç»“æœéƒ¨åˆ†ï¼Œä»¥éªŒè¯æ‘˜è¦ä¸­çš„å£°æ˜ï¼Œå¹¶å…¨é¢è¯„ä¼°è¯¥è®ºæ–‡çš„åˆ›æ–°æ€§å’Œå®ç”¨ä»·å€¼ã€‚ç‰¹åˆ«å…³æ³¨å®éªŒç»“æœæ˜¯å¦æ”¯æŒé€šè¿‡å¢åŠ æ‰©æ•£æ­¥éª¤æ¥å®ç°æ€§èƒ½æå‡ã€‚

---

### 66. Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods

<div align="center">

**åŠ æƒæ€»åˆ†: 7.90/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Oussama Zekri, Nicolas BoullÃ©
- **URL**: [https://arxiv.org/abs/2502.01384](https://arxiv.org/abs/2502.01384)

#### æ‘˜è¦

Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: é’ˆå¯¹ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„ç­–ç•¥æ¢¯åº¦å¾®è°ƒæå‡ºäº†æ–°çš„ç®—æ³•ï¼Œå…·æœ‰ä¸€å®šçš„ç†è®ºä¾æ®å’Œå®éªŒéªŒè¯ï¼Œè§£å†³äº†DLMä¸RLHFç»“åˆçš„å®é™…é—®é¢˜ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦æä¾›çš„ä¿¡æ¯æœ‰é™ï¼Œæ— æ³•æ·±å…¥è¯„ä¼°ç®—æ³•çš„å…·ä½“ç»†èŠ‚å’Œå®éªŒè®¾è®¡çš„ä¸¥è°¨æ€§ã€‚éœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ›´å‡†ç¡®åœ°åˆ¤æ–­ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®è¯¦ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨SEPOç®—æ³•çš„å…·ä½“å®ç°ã€ç†è®ºæ¨å¯¼å’Œå®éªŒç»“æœã€‚ å…³æ³¨è¯¥ç®—æ³•ä¸å…¶ä»–ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„æ¯”è¾ƒï¼Œä»¥åŠåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚ç‰¹åˆ«æ˜¯è¦ä»”ç»†åˆ†æè®ºæ–‡æ˜¯å¦ç»™å‡ºäº†è¶³å¤Ÿçš„å®éªŒæ•°æ®æ¥æ”¯æŒå…¶ç»“è®ºï¼Œå¹¶æ˜ç¡®è¯¥æ–¹æ³•çš„å±€é™æ€§ï¼Œä»¥åŠæœªæ¥æ”¹è¿›çš„æ–¹å‘ã€‚

---

### 67. DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation

<div align="center">

**åŠ æƒæ€»åˆ†: 7.80/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang
- **URL**: [https://arxiv.org/abs/2502.03930](https://arxiv.org/abs/2502.03930)

#### æ‘˜è¦

Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Dif...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: æå‡ºäº†æ–°é¢–çš„DiTARæ¡†æ¶ï¼Œç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£Transformerï¼Œåœ¨è¯­éŸ³ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ã€‚ Patch-basedçš„è‡ªå›å½’æ¡†æ¶å’Œæ¸©åº¦æ§åˆ¶æ¨ç†æ–¹æ³•éƒ½æ¯”è¾ƒæ–°é¢–ã€‚ å®éªŒåˆ†æä¹Ÿæ¯”è¾ƒå……åˆ†.
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­æ²¡æœ‰è¯¦ç»†æè¿°DiTARæ¡†æ¶çš„å…·ä½“å®ç°ç»†èŠ‚ï¼Œä¹Ÿç¼ºä¹å¯¹å®éªŒç»“æœçš„æ·±å…¥åˆ†æã€‚ ç†è®ºåˆ†ææ·±åº¦æœªçŸ¥ã€‚ æ‘˜è¦æ²¡æœ‰æåŠå­˜åœ¨çš„å±€é™æ€§ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è¯¥è®ºæ–‡ï¼Œç‰¹åˆ«æ˜¯å…³æ³¨DiTARæ¡†æ¶çš„å…·ä½“å®ç°ã€å®éªŒè®¾ç½®å’Œç»“æœåˆ†æã€‚å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶patch generationç­–ç•¥ï¼Œå™ªå£°å¼•å…¥çš„æ—¶é—´ç‚¹æ¸©åº¦è®¾ç½®ï¼Œä»¥åŠscaling analysisçš„å…·ä½“ç»†èŠ‚ã€‚ ç»“åˆå…¶ä»–çš„æ‰©æ•£æ¨¡å‹è®ºæ–‡ä¸€èµ·é˜…è¯»ï¼Œ å¯ä»¥æ›´å¥½åœ°ç†è§£è¿™é¡¹å·¥ä½œçš„åˆ›æ–°ç‚¹å’Œè´¡çŒ®ã€‚

---

### 68. Theoretical Benefit and Limitation of Diffusion Language Model

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Guhao Feng, Yihan Geng, Jian Guan, Wei Wu, Liwei Wang, Di He
- **URL**: [https://arxiv.org/abs/2502.09622](https://arxiv.org/abs/2502.09622)

#### æ‘˜è¦

Diffusion language models have emerged as a promising approach for text generation. One would naturally expect this method to be an efficient replacement for autoregressive models since multiple tokens can be sampled in parallel during each diffusion step. However, its efficiency-accuracy trade-off ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 8/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡é¦–æ¬¡å¯¹Masked Diffusion Modelçš„æ€§èƒ½è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œæ­ç¤ºäº†å…¶åœ¨ä¸åŒè¯„ä¼°æŒ‡æ ‡ä¸‹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚ç ”ç©¶ç»“æœå…·æœ‰è¾ƒé«˜çš„å®ç”¨ä»·å€¼ï¼Œèƒ½å¤ŸæŒ‡å¯¼DLMåœ¨å®é™…åº”ç”¨ä¸­çš„é€‰æ‹©å’Œä¼˜åŒ–ã€‚åŒæ—¶ï¼Œè®ºæ–‡çš„æŠ€æœ¯æ·±åº¦å’Œç ”ç©¶å½±å“åŠ›ä¹Ÿå€¼å¾—è‚¯å®šã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­ç¼ºä¹å¯¹å…·ä½“ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ç»†èŠ‚çš„æè¿°ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ã€‚å¦å¤–ï¼Œè¯¥ç ”ç©¶ä¸»è¦é’ˆå¯¹MDMï¼Œå¯èƒ½æ— æ³•ç›´æ¥æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œæ·±å…¥äº†è§£è®ºæ–‡çš„ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ç»†èŠ‚ã€‚å…³æ³¨è®ºæ–‡æå‡ºçš„ç†è®ºç»“è®ºæ˜¯å¦å…·æœ‰æ™®é€‚æ€§ï¼Œä»¥åŠæ˜¯å¦èƒ½å¤Ÿæ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ã€‚å¯ä»¥è€ƒè™‘å°†è¯¥è®ºæ–‡ä½œä¸ºæ‰©æ•£è¯­è¨€æ¨¡å‹ç ”ç©¶çš„èµ·ç‚¹ï¼Œè¿›ä¸€æ­¥æ¢ç´¢å…¶åœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

---

### 69. Non-Markovian Discrete Diffusion with Causal Language Models

<div align="center">

**åŠ æƒæ€»åˆ†: 8.10/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Yangtian Zhang, Sizhuang He, Daniel Levine, Lawrence Zhao, David Zhang, Syed A Rizvi, Emanuele Zappala, Rex Ying, David van Dijk
- **URL**: [https://arxiv.org/abs/2502.09767](https://arxiv.org/abs/2502.09767)

#### æ‘˜è¦

Discrete diffusion models have emerged as a flexible and controllable paradigm for structured sequence modeling, yet they still lag behind causal language models in expressiveness. To bridge the gap between two paradigms, we introduce CaDDi, a causal discrete diffusion model that unifies sequential ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è¯¥è®ºæ–‡æå‡ºäº†æ–°é¢–çš„éé©¬å°”å¯å¤«ç¦»æ•£æ‰©æ•£æ¨¡å‹CaDDiï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•´åˆæ—¶é—´è½¨è¿¹ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ— ç¼åœ°åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿™åœ¨ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹é¢†åŸŸæ˜¯ä¸€é¡¹é‡è¦çš„è¿›å±•ã€‚è®ºæ–‡å£°ç§°åœ¨è‡ªç„¶è¯­è¨€å’Œç”Ÿç‰©åºåˆ—ä»»åŠ¡ä¸Šå‡æœ‰ä¸é”™çš„è¡¨ç°ï¼Œå…·æœ‰ä¸€å®šçš„å®ç”¨ä»·å€¼ã€‚
- **ç¼ºç‚¹**: ä»…ä»æ‘˜è¦æ¥çœ‹ï¼Œç¼ºä¹å¯¹ç†è®ºåˆ†æå’Œæ•°å­¦æ¨å¯¼æ·±åº¦çš„è¯„ä¼°ã€‚è™½ç„¶å£°ç§°åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶ŠSOTAï¼Œä½†æ˜¯ç¼ºä¹è¯¦ç»†çš„å®éªŒè®¾ç½®å’Œç»“æœï¼Œéš¾ä»¥åˆ¤æ–­ç»“æœçš„å¯é æ€§ã€‚å¯¹é¢„è®­ç»ƒLLMçš„åˆ©ç”¨æ–¹å¼ä»¥åŠå…·ä½“å®éªŒç»†èŠ‚éœ€è¦åœ¨å…¨æ–‡ä¸­è¿›ä¸€æ­¥è€ƒå¯Ÿã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»å…¨æ–‡ï¼Œé‡ç‚¹å…³æ³¨CaDDiæ¨¡å‹çš„å…·ä½“å®ç°ç»†èŠ‚ï¼Œå°¤å…¶æ˜¯å¦‚ä½•å°†æ—¶é—´è½¨è¿¹èå…¥æ‰©æ•£è¿‡ç¨‹ä»¥åŠå¦‚ä½•æ— ç¼åœ°åˆ©ç”¨é¢„è®­ç»ƒLLMã€‚è®¤çœŸåˆ†æå®éªŒè®¾ç½®å’Œç»“æœï¼Œè¯„ä¼°CaDDiåœ¨è‡ªç„¶è¯­è¨€å’Œç”Ÿç‰©åºåˆ—ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡æ˜¯å¦å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚åŒæ—¶ï¼Œå¯ä»¥å…³æ³¨è¯¥æ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢çš„è¡¨ç°ï¼Œä»¥åŠå…¶å¯æ‰©å±•æ€§ã€‚

---

### 70. Large Language Diffusion Models

<div align="center">

**åŠ æƒæ€»åˆ†: 7.90/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li
- **URL**: [https://arxiv.org/abs/2502.09992](https://arxiv.org/abs/2502.09992)

#### æ‘˜è¦

Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward da...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡é¦–æ¬¡å°è¯•äº†ä»å¤´å¼€å§‹è®­ç»ƒå¤§è§„æ¨¡çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œå¹¶å–å¾—äº†ä¸€å®šçš„æˆåŠŸã€‚ ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå¹¶ä¸”å¯ä»¥è§£å†³ ARM æ¨¡å‹çš„ä¸€äº›é—®é¢˜ï¼ˆreversal curseï¼‰ã€‚
- **ç¼ºç‚¹**: è™½ç„¶æ€§èƒ½ä¸ LLaMA3 8B ç­‰æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œä½†è®ºæ–‡å¯èƒ½ç¼ºå°‘å¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ›´æ·±å±‚æ¬¡çš„ç†è®ºåˆ†æï¼Œä¾‹å¦‚ä¸ ARM ç›¸æ¯”çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡éœ€è¦æ›´æ·±å…¥åœ°åˆ†ææ¨¡å‹çš„å¯æ‰©å±•æ€§ï¼ŒåŒ…æ‹¬è®¡ç®—æˆæœ¬ä»¥åŠæ•°æ®å’Œæ¨¡å‹è§„æ¨¡å¯¹æ€§èƒ½çš„å½±å“ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®å¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹çš„ç†è®ºåŸºç¡€å’Œç‰¹æ€§è¿›è¡Œæ›´æ·±å…¥çš„ç ”ç©¶ï¼Œå¹¶ä¸å…¶ä»–ç±»å‹çš„è¯­è¨€æ¨¡å‹è¿›è¡Œæ›´å…¨é¢çš„æ¯”è¾ƒã€‚ä»£ç å·²ç»å¼€æºï¼Œå¯ä»¥å…³æ³¨é¡¹ç›®è¿›å±•ï¼Œäº†è§£æ›´å¤šç»†èŠ‚å’Œå¤ç°ç»“æœã€‚ å¦‚æœå¯¹æ‰©æ•£æ¨¡å‹åœ¨è¯­è¨€å»ºæ¨¡ä¸­çš„åº”ç”¨æ„Ÿå…´è¶£ï¼Œå¯ä»¥é˜…è¯»è¯¥è®ºæ–‡ã€‚

---

### 71. TESS 2: A Large-Scale Generalist Diffusion Language Model

<div align="center">

**åŠ æƒæ€»åˆ†: 8.20/10** | **ä¸»é¢˜ç›¸å…³æ€§: 10/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Jaesung Tae, Hamish Ivison, Sachin Kumar, Arman Cohan
- **URL**: [https://arxiv.org/abs/2502.13917](https://arxiv.org/abs/2502.13917)

#### æ‘˜è¦

We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining wi...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 10/10 |
| åˆ›æ–°æ€§ | 8/10 |
| å®ç”¨ä»·å€¼ | 7/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 7/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…·æœ‰ç«äº‰åŠ›çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå¼•å…¥äº†æ–°çš„æŠ€æœ¯ã€‚å¼€æºä»£ç å’Œæ¨¡å‹ä¹Ÿæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…å¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦æ²¡æœ‰æä¾›è¶³å¤Ÿçš„æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦é˜…è¯»å…¨æ–‡æ‰èƒ½æ›´å‡†ç¡®åœ°è¯„ä¼°æŠ€æœ¯çš„æ·±åº¦å’Œæœ‰æ•ˆæ€§ã€‚æ¨¡å‹çš„æ€§èƒ½æå‡æ˜¯å¦æ˜¾è‘—ä¹Ÿéœ€è¦åœ¨é˜…è¯»å…¨æ–‡åè¿›è¡Œè¯„ä¼°ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®ä»”ç»†é˜…è¯»è®ºæ–‡å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯å…³äºadaptation trainingå’Œreward guidanceçš„ç»†èŠ‚ï¼Œä»¥åŠå®éªŒç»“æœå’Œåˆ†æã€‚é‡ç‚¹å…³æ³¨TESS 2åœ¨å“ªäº›ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå“ªäº›ä»»åŠ¡ä¸Šè¿˜æœ‰æå‡ç©ºé—´ã€‚å¯ä»¥å°è¯•å¤ç°è®ºæ–‡ç»“æœï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚

---

### 72. EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models

<div align="center">

**åŠ æƒæ€»åˆ†: 7.80/10** | **ä¸»é¢˜ç›¸å…³æ€§: 9/10**

</div>

#### è®ºæ–‡ä¿¡æ¯

- **ä½œè€…**: Che Hyun Lee, Heeseung Kim, Jiheum Yeom, Sungroh Yoon
- **URL**: [https://arxiv.org/abs/2502.19765](https://arxiv.org/abs/2502.19765)

#### æ‘˜è¦

We propose EdiText, a controllable text editing method that modify the reference text to desired attributes at various scales. We integrate an SDEdit-based editing technique that allows for broad adjustments in the degree of text editing. Additionally, we introduce a novel fine-level editing method ...

#### è¯„åˆ†è¯¦æƒ…

| è¯„åˆ†ç»´åº¦ | åˆ†æ•° |
|---------|------|
| ä¸»é¢˜ç›¸å…³æ€§ | 9/10 |
| åˆ›æ–°æ€§ | 7/10 |
| å®ç”¨ä»·å€¼ | 8/10 |
| æŠ€æœ¯æ·±åº¦ | 7/10 |
| ç ”ç©¶å½±å“åŠ› | 6/10 |

#### è¯„ä»·

- **ä¼˜ç‚¹**: è®ºæ–‡ä¸»é¢˜ä¸æ‰©æ•£è¯­è¨€æ¨¡å‹é«˜åº¦ç›¸å…³ï¼Œæå‡ºäº†ä¸€ä¸ªcoarse-to-fineçš„æ–‡æœ¬ç¼–è¾‘æ¡†æ¶ï¼Œå¹¶å…·æœ‰å®é™…åº”ç”¨æ½œåŠ›ï¼Œä¾‹å¦‚ toxicity control å’Œ sentiment controlã€‚ç»“åˆäº†SDEditå’Œself-conditioningæŠ€æœ¯ï¼Œå…·æœ‰ä¸€å®šçš„åˆ›æ–°æ€§ã€‚
- **ç¼ºç‚¹**: æ‘˜è¦ä¸­æœªä½“ç°å®éªŒç»†èŠ‚ä»¥åŠæ•ˆæœçš„å¯¹æ¯”ï¼Œå¾ˆéš¾åˆ¤æ–­æ˜¯å¦æ¯”å…¶ä»–æ–‡æœ¬ç¼–è¾‘æ–¹æ³•æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å½±å“åŠ›çš„åˆ¤æ–­æ›´å¤šä¾èµ–äºå®éªŒç»“æœçš„ä¼˜ç§€ç¨‹åº¦ã€‚
- **é˜…è¯»å»ºè®®**: å»ºè®®é˜…è¯»å…¨æ–‡ï¼Œç‰¹åˆ«æ˜¯å®éªŒéƒ¨åˆ†ï¼Œè¯¦ç»†äº†è§£EdiTextåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼Œä»¥åŠä¸ç°æœ‰æ–‡æœ¬ç¼–è¾‘æ–¹æ³•çš„å¯¹æ¯”ã€‚å…³æ³¨å…¶åœ¨ç»†ç²’åº¦æ§åˆ¶æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä»¥åŠå¯èƒ½å­˜åœ¨çš„å±€é™æ€§ã€‚å¯¹äºæ‰©æ•£æ¨¡å‹å’Œæ–‡æœ¬ç¼–è¾‘æ„Ÿå…´è¶£çš„ç ”ç©¶è€…ï¼Œå¯ä»¥æ·±å…¥ç ”ç©¶å…¶self-conditioningçš„å®ç°ç»†èŠ‚ã€‚

---


## å…³äºæœ¬é¡¹ç›®

æœ¬é¡¹ç›®ä½¿ç”¨Google Gemini APIå¯¹ä»arXivçˆ¬å–çš„å…³äºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„è®ºæ–‡è¿›è¡Œè‡ªåŠ¨è¯„ä¼°ï¼Œå¹¶æ•´ç†å‡ºé«˜åˆ†è®ºæ–‡åˆ—è¡¨ï¼Œæ—¨åœ¨å¸®åŠ©ç ”ç©¶äººå‘˜å¿«é€Ÿæ‰¾åˆ°è¯¥é¢†åŸŸçš„é«˜è´¨é‡è®ºæ–‡ã€‚

### æ•°æ®æ¥æº

è®ºæ–‡æ•°æ®æ¥è‡ªarXivï¼Œä½¿ç”¨å…³é”®è¯"diffusion language model"è¿›è¡Œæœç´¢å’Œçˆ¬å–ã€‚

### è¯„ä¼°æ–¹æ³•

æ¯ç¯‡è®ºæ–‡ç”±AIæ¨¡å‹æ ¹æ®äº”ä¸ªç»´åº¦è¿›è¡Œè¯„åˆ†ï¼Œå¹¶ç»™å‡ºç»¼åˆè¯„ä»·ã€‚è¯„åˆ†æ ‡å‡†å¦‚ä¸‹ï¼š

1. **ä¸»é¢˜ç›¸å…³æ€§**ï¼ˆ1-10åˆ†ï¼Œæƒé‡40%ï¼‰ï¼šè¯¥è®ºæ–‡ä¸æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸»é¢˜çš„ç›¸å…³ç¨‹åº¦ï¼Œç‰¹åˆ«å…³æ³¨Diffusionåœ¨è¯­è¨€æ¨¡å‹ä¸Šçš„åº”ç”¨ï¼Œè€Œéå…¶ä»–é¢†åŸŸ
2. **åˆ›æ–°æ€§**ï¼ˆ1-10åˆ†ï¼Œæƒé‡20%ï¼‰ï¼šæå‡ºäº†å¤šå°‘æ–°é¢–çš„æ€æƒ³ã€æ–¹æ³•æˆ–è§è§£
3. **å®ç”¨ä»·å€¼**ï¼ˆ1-10åˆ†ï¼Œæƒé‡20%ï¼‰ï¼šç ”ç©¶æˆæœçš„å®é™…åº”ç”¨æ½œåŠ›
4. **æŠ€æœ¯æ·±åº¦**ï¼ˆ1-10åˆ†ï¼Œæƒé‡10%ï¼‰ï¼šæŠ€æœ¯åˆ†æçš„æ·±åº¦å’Œä¸¥è°¨æ€§
5. **ç ”ç©¶å½±å“åŠ›**ï¼ˆ1-10åˆ†ï¼Œæƒé‡10%ï¼‰ï¼šå¯¹é¢†åŸŸçš„æ½œåœ¨å½±å“

åŠ æƒæ€»åˆ†æ˜¯åŸºäºä»¥ä¸Šäº”ä¸ªç»´åº¦æŒ‰æƒé‡è®¡ç®—çš„ç»¼åˆè¯„ä»·ã€‚æœ¬åˆ—è¡¨ä»…åŒ…å«ä¸»é¢˜ç›¸å…³æ€§â‰¥8åˆ†ä¸”åŠ æƒæ€»åˆ†â‰¥7.5åˆ†çš„è®ºæ–‡ã€‚

### è®¸å¯è¯

MIT License

